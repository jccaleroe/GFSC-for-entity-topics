{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "prefixes = ['per', 'verb', 'org', 'loc', 'adj']\n",
    "\n",
    "def load_nodes(name):\n",
    "    n_trees = 0\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/topicTree.nodes.json', 'r') as read_file:\n",
    "            j_nodes = json.load(read_file)\n",
    "        q = deque()\n",
    "        for node in j_nodes:\n",
    "            node_parent[prefix + node['id']] = ['ROOT']\n",
    "            q.append(node)\n",
    "            node_tree[prefix + node['id']] = n_trees\n",
    "            n_trees += 1\n",
    "        while len(q) > 0:\n",
    "            node = q.popleft()\n",
    "            node_id = prefix + node['id']\n",
    "            nodes[node_id] = len(nodes)\n",
    "            node_words[node_id] = node['text'].split(' ')\n",
    "            level = node['data']['level']\n",
    "            node_level[node_id] = level\n",
    "            if level not in nodes_per_level:\n",
    "                nodes_per_level[level] = {}\n",
    "                id2node[level] = {}\n",
    "            nodes_per_level[level][node_id] = len(nodes_per_level[level])\n",
    "            id2node[level][nodes_per_level[level][node_id]] = nodes[node_id]\n",
    "            node_children[node_id] = []\n",
    "            for child in node['children']:\n",
    "                child_id = prefix + child['id']\n",
    "                node_tree[child_id] = node_tree[node_id]\n",
    "                node_parent[child_id] = [node_id]\n",
    "                node_children[node_id].append(child_id)\n",
    "                q.append(child)\n",
    "    print(n_trees, ' trees found')\n",
    "    print(len(nodes), ' topics found')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_assignments(name):\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myAssignment.topics.json', 'r') as read_file:\n",
    "            j_nodes = json.load(read_file)\n",
    "        for node in j_nodes:\n",
    "            topic = node['topic']\n",
    "            assignments[prefix + topic] = {}\n",
    "            for doc in node['doc']:\n",
    "                assignments[prefix + topic][int(doc[0])] = doc[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data_files(name):\n",
    "    cnt = 0\n",
    "    with open(name + 'myData.files.txt', 'r') as read_file:\n",
    "        tmp = read_file.read().split('\\n')\n",
    "    for i in tmp:\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        data_files[cnt] = i\n",
    "        cnt += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_words(name):\n",
    "    repeated = {}\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myData.dict.csv', 'r') as read_file:\n",
    "            for tfidf in csv.reader(read_file):\n",
    "                if tfidf[3] != 'tfidf':\n",
    "                    if tfidf[0] not in words_dic:\n",
    "                        words_dic[tfidf[0]] = len(words_dic)\n",
    "                        tf_idf[tfidf[0]] = float(tfidf[3])\n",
    "                    else:\n",
    "                        if tfidf[0] not in repeated:\n",
    "                            repeated[tfidf[0]] = 2\n",
    "                        else:\n",
    "                            repeated[tfidf[0]] += 1\n",
    "                        tf_idf[tfidf[0]] += float(tfidf[3])\n",
    "    for word in repeated:\n",
    "        tf_idf[word] /= repeated[word]\n",
    "    print(len(repeated), 'repeated words')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_sparse(name):\n",
    "    with open(name + '/myData.sparse.txt', 'r') as read_file:\n",
    "        doc_words = read_file.read().split('\\n')\n",
    "    for i in data_files:\n",
    "        sparse[i] = []\n",
    "    for i in doc_words:\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        tmp = i.split(\", \")\n",
    "        tmp[0] = int(tmp[0])\n",
    "        sparse[tmp[0]].append(tmp[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def read_java_bayes(name):\n",
    "    for node in nodes:\n",
    "        words_prop[node] = {}\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myModel.bif', 'r') as f:\n",
    "            for i in f:\n",
    "                if i.count('|') == 0:\n",
    "                    continue\n",
    "                if i.startswith('probability ( '):\n",
    "                    a = i[i.index('\"') + 1: i.index('|') - 2]\n",
    "                    b = prefix + i[i.index('|') + 3: i.index(')') - 2]\n",
    "                    prob = float(f.readline().split(' ')[1])\n",
    "                    if a.startswith('Z'):\n",
    "                        if b not in node_prob:\n",
    "                            node_prob[b] = {}\n",
    "                        a = prefix + a\n",
    "                        node_prob[b][a] = prob\n",
    "                    else:\n",
    "                        if a in words_dic:\n",
    "                            words_prop[b][a] = prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def prob_dfs(node):\n",
    "    for child in node_children[node]:\n",
    "        words = prob_dfs(child)\n",
    "        for word in words:\n",
    "            words_prop[node][word] = node_prob[node][child] * words_prop[child][word]\n",
    "    return words_prop[node]\n",
    "\n",
    "def get_words_prop(name):\n",
    "    read_java_bayes(name)\n",
    "    for node in nodes:\n",
    "        if len(node_parent[node]) == 1 and node_parent[node][0] == 'ROOT':\n",
    "            prob_dfs(node)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def save_np(folder, matrix, view_id):\n",
    "    m_min = matrix.min()\n",
    "    m_max = matrix.max()\n",
    "    matrix -= m_min\n",
    "    matrix /= (m_max - m_min)\n",
    "    matrix *= 2\n",
    "    matrix -= 1\n",
    "    with open(folder + view_id + \".npy\", 'wb') as f:\n",
    "        np.save(f, matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_views(name):\n",
    "    import os\n",
    "    folder = 'views/' + name + '/level_'\n",
    "    for level in nodes_per_level:\n",
    "        path = folder + str(level) + '/'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        nodes_number = len(nodes_per_level[level])\n",
    "        print(nodes_number, 'nodes in level', level)\n",
    "        view_files = np.zeros((nodes_number, len(data_files)))\n",
    "        view_bayes = np.zeros((nodes_number, len(words_dic)))\n",
    "        view_tfidf = np.zeros((nodes_number, len(words_dic)))\n",
    "        for node, node_id in nodes_per_level[level].items():\n",
    "            for file in assignments[node]:\n",
    "                view_files[node_id][file] = assignments[node][file]\n",
    "            for word in words_prop[node]:\n",
    "                view_bayes[node_id][words_dic[word]] = words_prop[node][word]\n",
    "            for doc in assignments[node]:\n",
    "                for word in sparse[doc]:\n",
    "                    if word in tf_idf:\n",
    "                        view_tfidf[node_id][words_dic[word]] = tf_idf[word] * assignments[node][doc]\n",
    "        save_np(path, view_files, 'files')\n",
    "        save_np(path, view_bayes, 'bayes')\n",
    "        save_np(path, view_tfidf, 'tfidf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def read_results(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return np.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_node(u_name, cluster, c_id, level):\n",
    "    nodes[u_name] = len(nodes)\n",
    "    node_group[u_name] = c_id\n",
    "    node_children[u_name] = []\n",
    "    node_parent[u_name] = ['ROOT']\n",
    "    for node_id in cluster:\n",
    "        node_parent[id2node[level][node_id]] = [u_name]\n",
    "    node_words[u_name] = set()\n",
    "    i = 0\n",
    "    while True:\n",
    "        no_more = True\n",
    "        for node_id in cluster:\n",
    "            node = id2node[level][node_id]\n",
    "            if len(node_words[node]) > i:\n",
    "                no_more = False\n",
    "                if node_words[node][i] not in node_words[u_name]:                    \n",
    "                    node_words[u_name].append(node_words[node][i])\n",
    "                if len(node_words[u_name]) >= 7:\n",
    "                    break\n",
    "        if no_more:\n",
    "            break\n",
    "        i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_clusters(labels, level, is_root):\n",
    "    clusters = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in clusters:\n",
    "            clusters[labels[i]].append(i)\n",
    "        else:\n",
    "            clusters[labels[i]] = [i]\n",
    "    cnt = 0\n",
    "    for c_id in clusters:\n",
    "        if is_root and len(clusters[c_id]) > 1:\n",
    "            create_node('U' + str(cnt), clusters[c_id], c_id, level)\n",
    "            cnt += 1\n",
    "        for i in range(len(clusters[c_id])):\n",
    "            node = id2node[level][clusters[c_id][i]]\n",
    "            node_group[node] = c_id\n",
    "            for j in range(i + 1, len(clusters[c_id])):\n",
    "                node2 = id2node[level][clusters[c_id][j]]\n",
    "                if node_tree[node] == node_tree[node2]:\n",
    "                    continue\n",
    "                node_children[node_parent[node]].append(node2)\n",
    "                node_children[node_parent[node2]].append(node)\n",
    "    if is_root:\n",
    "        print(len(clusters), ' trees after fusion')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def export_nodes_json(name):\n",
    "    graph = []\n",
    "    for node in nodes:\n",
    "        entry = {'id': node, 'text': ' '.join(node_words[node]), 'children': []}\n",
    "        graph.append(entry)\n",
    "    with open(name, 'w') as f:\n",
    "        json.dump(graph, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def create_topics_d3(name, expanded):\n",
    "    graph = {\"nodes\": {}, \"links\": []}\n",
    "    for node in nodes:\n",
    "        words = node if node.startswith('U') else node[:node.index('Z')]\n",
    "        words = words + ': ' + ' '.join(node_words[node]).replace('zzz', '_')\n",
    "        is_root = False\n",
    "        if len(node_parent[node]) == 1 and node_parent[node][0] == 'ROOT':\n",
    "            is_root = True\n",
    "        d = {\"id\":node, \"name\":words, \"group\":int(node_group[node]),\n",
    "             \"isRoot\":expanded or is_root, \"children\":node_children[node], \"expanded\":expanded}\n",
    "        graph[\"nodes\"][node] = d\n",
    "        if expanded:\n",
    "            for child in node_children:\n",
    "                 graph[\"links\"].append((dict([('id', node+'-'+child), (\"source\", node), (\"target\", child)])))\n",
    "    with open(\"graphs/d3/\" + name + '.json', \"w\") as fp:\n",
    "        json.dump(graph, fp, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_topics_gephi(name):\n",
    "    colors = {}\n",
    "    r, g, b, rr, gr, br, cnt = 80, 70, 60, 80, 70, 60, 0\n",
    "    if len(node_group) == 0:\n",
    "        for i in nodes:\n",
    "            node_group[i] = 0\n",
    "        colors[0] = '000000'\n",
    "    else:\n",
    "        for i in node_group.values():\n",
    "            if i not in colors:\n",
    "                color = '{:02x}'.format(r) if cnt < 4 else '00'\n",
    "                color += '{:02x}'.format(g) if cnt % 2 == 0 else '00'\n",
    "                color += '{:02x}'.format(b) if 1 < cnt < 6 else '00'\n",
    "                cnt += 1\n",
    "                if cnt == 7:\n",
    "                    r = (r + rr) % 256\n",
    "                    g = (g + gr) % 256\n",
    "                    b = (b + br) % 256\n",
    "                    cnt = 0\n",
    "                colors[i] = color\n",
    "\n",
    "    graph = ['graph [ directed 1']\n",
    "    for node in nodes:\n",
    "        words = node if node.startswith('U') else node[:node.index('Z')]\n",
    "        words = words + ': ' + ' '.join(node_words[node]).replace('zzz', '_')\n",
    "        graph.append('node [')\n",
    "        graph.append('id ' + str(nodes[node]))\n",
    "        graph.append('label \"' + words + '\"')\n",
    "        graph.append('graphics [fill \"#' + colors[node_group[node]] + '\"]]')\n",
    "    for node in node_children:\n",
    "        for child in node_children[node]:\n",
    "            graph.append('edge [')\n",
    "            graph.append('source ' + str(nodes[node]))\n",
    "            graph.append('target ' + str(nodes[child]) + ' ]')\n",
    "    graph.append(']')\n",
    "    with open('graphs/gephi/' + name + '.gml', 'w') as f:\n",
    "        f.write('\\n'.join(graph))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76  trees found\n",
      "1236  topics found\n",
      "241 repeated words\n"
     ]
    }
   ],
   "source": [
    "nodes, words_dic, node_words, node_tree = {}, {}, {}, {}\n",
    "node_parent, node_level, node_children, nodes_per_level = {}, {}, {}, {}\n",
    "id2node, node_group, data_files, assignments, tf_idf = {}, {}, {}, {}, {}\n",
    "node_prob, words_prop, sparse = {}, {}, {}\n",
    "load_nodes('profiles/')\n",
    "load_assignments('profiles/')\n",
    "load_data_files('profiles/adj/')\n",
    "load_words('profiles/')\n",
    "get_words_prop('profiles/')\n",
    "load_sparse('profiles/')\n",
    "# get_views('profiles/')\n",
    "# get_ids2nodes()\n",
    "# process_clusters(read_results('clusters/profiles/150_means.npy'))\n",
    "# export_nodes_json('profiles/evaluation/150_means.nodes.json')\n",
    "# create_topics_gephi('profiles_150_means')\n",
    "# create_topics_d3(\"profiles\", False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}