{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "prefixes = ['per', 'verb', 'org', 'loc', 'adj']\n",
    "\n",
    "def load_nodes(name):\n",
    "    n_trees = 0\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/topicTree.nodes.json', 'r') as read_file:\n",
    "            j_nodes = json.load(read_file)\n",
    "        q = deque()\n",
    "        for node in j_nodes:\n",
    "            node_parent[prefix + node['id']] = {'ROOT'}\n",
    "            q.append(node)\n",
    "            n_trees += 1\n",
    "        while len(q) > 0:\n",
    "            node = q.popleft()\n",
    "            node_id = prefix + node['id']\n",
    "            nodes[node_id] = len(nodes)\n",
    "            node_words[node_id] = node['text'].split(' ')\n",
    "            level = node['data']['level']\n",
    "            node_level[node_id] = level\n",
    "            if level not in nodes_per_level:\n",
    "                nodes_per_level[level] = {}\n",
    "                id2node[level] = {}\n",
    "            nodes_per_level[level][node_id] = len(nodes_per_level[level])\n",
    "            id2node[level][nodes_per_level[level][node_id]] = node_id\n",
    "            node_children[node_id] = set()\n",
    "            for child in node['children']:\n",
    "                child_id = prefix + child['id']\n",
    "                node_parent[child_id] = {node_id}\n",
    "                node_children[node_id].add(child_id)\n",
    "                q.append(child)\n",
    "    print(n_trees, ' trees found')\n",
    "    print(len(nodes), ' topics found')\n",
    "    for level in nodes_per_level:\n",
    "        print(len(nodes_per_level[level]), 'nodes in level', level)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_assignments(name):\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myAssignment.topics.json', 'r') as read_file:\n",
    "            j_nodes = json.load(read_file)\n",
    "        for node in j_nodes:\n",
    "            topic = node['topic']\n",
    "            assignments[prefix + topic] = {}\n",
    "            for doc in node['doc']:\n",
    "                assignments[prefix + topic][int(doc[0])] = doc[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data_files(name):\n",
    "    cnt = 0\n",
    "    with open(name + 'myData.files.txt', 'r') as read_file:\n",
    "        tmp = read_file.read().split('\\n')\n",
    "    for i in tmp:\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        data_files[cnt] = i\n",
    "        cnt += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_words(name):\n",
    "    repeated = {}\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myData.dict.csv', 'r') as read_file:\n",
    "            for tfidf in csv.reader(read_file):\n",
    "                if tfidf[3] != 'tfidf':\n",
    "                    if tfidf[0] not in words_dic:\n",
    "                        words_dic[tfidf[0]] = len(words_dic)\n",
    "                        tf_idf[tfidf[0]] = float(tfidf[3])\n",
    "                    else:\n",
    "                        if tfidf[0] not in repeated:\n",
    "                            repeated[tfidf[0]] = 2\n",
    "                        else:\n",
    "                            repeated[tfidf[0]] += 1\n",
    "                        tf_idf[tfidf[0]] += float(tfidf[3])\n",
    "    for word in repeated:\n",
    "        tf_idf[word] /= repeated[word]\n",
    "    print(len(repeated), 'repeated words')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_sparse(name):\n",
    "    with open(name + '/myData.sparse.txt', 'r') as read_file:\n",
    "        doc_words = read_file.read().split('\\n')\n",
    "    for i in data_files:\n",
    "        sparse[i] = []\n",
    "    for i in doc_words:\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        tmp = i.split(\", \")\n",
    "        tmp[0] = int(tmp[0])\n",
    "        sparse[tmp[0]].append(tmp[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def read_java_bayes(name):\n",
    "    for node in nodes:\n",
    "        words_prop[node] = {}\n",
    "    for prefix in prefixes:\n",
    "        with open(name + prefix + '/myModel.bif', 'r') as f:\n",
    "            for i in f:\n",
    "                if i.count('|') == 0:\n",
    "                    continue\n",
    "                if i.startswith('probability ( '):\n",
    "                    a = i[i.index('\"') + 1: i.index('|') - 2]\n",
    "                    b = prefix + i[i.index('|') + 3: i.index(')') - 2]\n",
    "                    prob = float(f.readline().split(' ')[1])\n",
    "                    if a.startswith('Z'):\n",
    "                        if b not in node_prob:\n",
    "                            node_prob[b] = {}\n",
    "                        a = prefix + a\n",
    "                        node_prob[b][a] = prob\n",
    "                    else:\n",
    "                        if a in words_dic:\n",
    "                            words_prop[b][a] = prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def prob_dfs(node):\n",
    "    for child in node_children[node]:\n",
    "        words = prob_dfs(child)\n",
    "        for word in words:\n",
    "            words_prop[node][word] = node_prob[node][child] * words_prop[child][word]\n",
    "    return words_prop[node]\n",
    "\n",
    "def get_words_prop(name):\n",
    "    read_java_bayes(name)\n",
    "    for node in nodes:\n",
    "        if 'ROOT' in node_parent[node]:\n",
    "            prob_dfs(node)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def save_np(folder, matrix, view_id):\n",
    "    m_min = matrix.min()\n",
    "    m_max = matrix.max()\n",
    "    matrix -= m_min\n",
    "    matrix /= (m_max - m_min)\n",
    "    matrix *= 2\n",
    "    matrix -= 1\n",
    "    with open(folder + view_id + \".npy\", 'wb') as f:\n",
    "        np.save(f, matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_views(name):\n",
    "    import os\n",
    "    folder = 'views/' + name + '/level_'\n",
    "    for level in nodes_per_level:\n",
    "        path = folder + str(level) + '/'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        nodes_number = len(nodes_per_level[level])\n",
    "        view_files = np.zeros((nodes_number, len(data_files)))\n",
    "        view_bayes = np.zeros((nodes_number, len(words_dic)))\n",
    "        view_tfidf = np.zeros((nodes_number, len(words_dic)))\n",
    "        for node, node_id in nodes_per_level[level].items():\n",
    "            for file in assignments[node]:\n",
    "                view_files[node_id][file] = assignments[node][file]\n",
    "            for word in words_prop[node]:\n",
    "                view_bayes[node_id][words_dic[word]] = words_prop[node][word]\n",
    "            for doc in assignments[node]:\n",
    "                for word in sparse[doc]:\n",
    "                    if word in tf_idf:\n",
    "                        view_tfidf[node_id][words_dic[word]] = tf_idf[word] * assignments[node][doc]\n",
    "        save_np(path, view_files, 'files')\n",
    "        save_np(path, view_bayes, 'bayes')\n",
    "        save_np(path, view_tfidf, 'tfidf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def read_results(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return np.load(f)\n",
    "\n",
    "def run_by_levels(clusters, name):\n",
    "    import os\n",
    "    for level in next(os.walk(clusters))[1]:\n",
    "        process_clusters(read_results(clusters + '/' + level + '/' + name), int(level[-1]))\n",
    "        print(len(nodes), 'nodes after fusion')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_node(u_name, cluster, c_id, level):\n",
    "    nodes[u_name] = len(node_words)\n",
    "    node_group[u_name] = c_id\n",
    "    node_children[u_name] = set()\n",
    "    node_parent[u_name] = set()\n",
    "    for node_id in cluster:\n",
    "        node = id2node[level][node_id]\n",
    "        for parent in node_parent[node]:\n",
    "            node_parent[u_name].add(parent)\n",
    "            if parent != 'ROOT':\n",
    "                if node in node_children[parent]:\n",
    "                    node_children[parent].remove(node)\n",
    "                node_children[parent].add(u_name)\n",
    "        for child in node_children[node]:\n",
    "            node_children[u_name].add(child)\n",
    "            if node in node_parent[child]:\n",
    "                node_parent[child].remove(node)\n",
    "            node_parent[child].add(u_name)\n",
    "        if level == 1:\n",
    "            node_children[u_name].add(node)\n",
    "            node_parent[node] = {u_name}\n",
    "            node_group[node] = c_id\n",
    "        else:\n",
    "            del nodes[node]\n",
    "            del node_children[node]\n",
    "            del node_parent[node]\n",
    "    node_words[u_name] = set()\n",
    "    i = 0\n",
    "    while True:\n",
    "        no_more = True\n",
    "        for node_id in cluster:\n",
    "            node = id2node[level][node_id]\n",
    "            if len(node_words[node]) > i:\n",
    "                no_more = False\n",
    "                if node_words[node][i] not in node_words[u_name]:\n",
    "                    node_words[u_name].add(node_words[node][i])\n",
    "                if len(node_words[u_name]) >= 7:\n",
    "                    no_more = True\n",
    "                    break\n",
    "        if no_more:\n",
    "            break\n",
    "        i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def process_clusters(labels, level):\n",
    "    print('processing level', level)\n",
    "    clusters = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in clusters:\n",
    "            clusters[labels[i]].append(i)\n",
    "        else:\n",
    "            clusters[labels[i]] = [i]\n",
    "    cnt = 0\n",
    "    for c_id in clusters:\n",
    "        if len(clusters[c_id]) == 1:\n",
    "            node_group[id2node[level][clusters[c_id][0]]] = c_id\n",
    "            continue\n",
    "        create_node('U_' + str(level) + '_' + str(cnt), clusters[c_id], c_id, level)\n",
    "        cnt += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def export_nodes_json(name):\n",
    "    graph = []\n",
    "    for node in nodes:\n",
    "        entry = {'id': node, 'text': ' '.join(node_words[node]), 'children': []}\n",
    "        graph.append(entry)\n",
    "    with open(name, 'w') as f:\n",
    "        json.dump(graph, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_topics_d3(name, expanded):\n",
    "    graph = {\"nodes\": {}, \"links\": []}\n",
    "    for node in nodes:\n",
    "        words = node if node.startswith('U') else node[:node.index('Z')]\n",
    "        words = words + ': ' + ' '.join(node_words[node]).replace('zzz', '_')\n",
    "        is_root = False\n",
    "        if 'ROOT' in node_parent[node]:\n",
    "            is_root = True\n",
    "        d = {\"id\":node, \"name\":words, \"group\":int(node_group[node]),\n",
    "             \"isRoot\":expanded or is_root, \"children\":list(node_children[node]), \"expanded\":expanded}\n",
    "        graph[\"nodes\"][node] = d\n",
    "        if expanded:\n",
    "            for child in node_children:\n",
    "                 graph[\"links\"].append((dict([('id', node+'-'+child), (\"source\", node), (\"target\", child)])))\n",
    "    with open(\"graphs/d3/\" + name + '.json', \"w\") as fp:\n",
    "        json.dump(graph, fp, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def create_topics_gephi(name):\n",
    "    colors = {}\n",
    "    r, g, b, rr, gr, br, cnt = 80, 70, 60, 80, 70, 60, 0\n",
    "    if len(node_group) == 0:\n",
    "        for i in nodes:\n",
    "            node_group[i] = 0\n",
    "        colors[0] = '000000'\n",
    "    else:\n",
    "        for i in node_group.values():\n",
    "            if i not in colors:\n",
    "                color = '{:02x}'.format(r) if cnt < 4 else '00'\n",
    "                color += '{:02x}'.format(g) if cnt % 2 == 0 else '00'\n",
    "                color += '{:02x}'.format(b) if 1 < cnt < 6 else '00'\n",
    "                cnt += 1\n",
    "                if cnt == 7:\n",
    "                    r = (r + rr) % 256\n",
    "                    g = (g + gr) % 256\n",
    "                    b = (b + br) % 256\n",
    "                    cnt = 0\n",
    "                colors[i] = color\n",
    "    graph = ['graph [ directed 1']\n",
    "    for node in nodes:\n",
    "        words = node if node.startswith('U') else node[:node.index('Z')]\n",
    "        words = words + ': ' + ' '.join(node_words[node]).replace('zzz', '_')\n",
    "        graph.append('node [')\n",
    "        graph.append('id ' + str(nodes[node]))\n",
    "        graph.append('label \"' + words + '\"')\n",
    "        graph.append('graphics [fill \"#' + colors[node_group[node]] + '\"]]')\n",
    "    for node in nodes:\n",
    "        for child in node_children[node]:\n",
    "            graph.append('edge [')\n",
    "            graph.append('source ' + str(nodes[node]))\n",
    "            graph.append('target ' + str(nodes[child]) + ' ]')\n",
    "    graph.append(']')\n",
    "    import os\n",
    "    os.makedirs('graphs/gephi/', exist_ok=True)\n",
    "    with open('graphs/gephi/' + name + '.gml', 'w') as f:\n",
    "        f.write('\\n'.join(graph))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76  trees found\n",
      "1236  topics found\n",
      "51 nodes in level 3\n",
      "237 nodes in level 2\n",
      "948 nodes in level 1\n",
      "241 repeated words\n",
      "processing level 2\n",
      "1022 nodes after fusion\n",
      "processing level 1\n",
      "168 nodes after fusion\n",
      "processing level 3\n",
      "122 nodes after fusion\n"
     ]
    }
   ],
   "source": [
    "nodes, words_dic, node_words = {}, {}, {}\n",
    "node_parent, node_level, node_children, nodes_per_level = {}, {}, {}, {}\n",
    "id2node, node_group, data_files, assignments, tf_idf = {}, {}, {}, {}, {}\n",
    "node_prob, words_prop, sparse = {}, {}, {}\n",
    "load_nodes('profiles/')\n",
    "load_assignments('profiles/')\n",
    "load_data_files('profiles/adj/')\n",
    "load_words('profiles/')\n",
    "get_words_prop('profiles/')\n",
    "load_sparse('profiles/')\n",
    "# get_views('profiles/')\n",
    "run_by_levels('clusters/profiles', 'labels.npy')\n",
    "export_nodes_json('profiles/evaluation/profiles.nodes.json')\n",
    "create_topics_gephi('profiles')\n",
    "create_topics_d3(\"profiles\", False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}