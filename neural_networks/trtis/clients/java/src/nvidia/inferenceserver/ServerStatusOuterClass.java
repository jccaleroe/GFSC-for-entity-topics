// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: server_status.proto

package nvidia.inferenceserver;

public final class ServerStatusOuterClass {
  private ServerStatusOuterClass() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: ModelReadyState
   *&#64;&#64;
   *&#64;&#64;   Readiness status for models.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code nvidia.inferenceserver.ModelReadyState}
   */
  public enum ModelReadyState
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNKNOWN = 0
     *&#64;&#64;
     *&#64;&#64;     The model is in an unknown state. The model is not available for
     *&#64;&#64;     inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNKNOWN = 0;</code>
     */
    MODEL_UNKNOWN(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_READY = 1
     *&#64;&#64;
     *&#64;&#64;     The model is ready and available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_READY = 1;</code>
     */
    MODEL_READY(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNAVAILABLE = 2
     *&#64;&#64;
     *&#64;&#64;     The model is unavailable, indicating that the model failed to
     *&#64;&#64;     load or has been implicitly or explicitly unloaded. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNAVAILABLE = 2;</code>
     */
    MODEL_UNAVAILABLE(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_LOADING = 3
     *&#64;&#64;
     *&#64;&#64;     The model is being loaded by the inference server. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_LOADING = 3;</code>
     */
    MODEL_LOADING(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNLOADING = 4
     *&#64;&#64;
     *&#64;&#64;     The model is being unloaded by the inference server. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNLOADING = 4;</code>
     */
    MODEL_UNLOADING(4),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNKNOWN = 0
     *&#64;&#64;
     *&#64;&#64;     The model is in an unknown state. The model is not available for
     *&#64;&#64;     inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNKNOWN = 0;</code>
     */
    public static final int MODEL_UNKNOWN_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_READY = 1
     *&#64;&#64;
     *&#64;&#64;     The model is ready and available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_READY = 1;</code>
     */
    public static final int MODEL_READY_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNAVAILABLE = 2
     *&#64;&#64;
     *&#64;&#64;     The model is unavailable, indicating that the model failed to
     *&#64;&#64;     load or has been implicitly or explicitly unloaded. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNAVAILABLE = 2;</code>
     */
    public static final int MODEL_UNAVAILABLE_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_LOADING = 3
     *&#64;&#64;
     *&#64;&#64;     The model is being loaded by the inference server. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_LOADING = 3;</code>
     */
    public static final int MODEL_LOADING_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ModelReadyState::MODEL_UNLOADING = 4
     *&#64;&#64;
     *&#64;&#64;     The model is being unloaded by the inference server. The model is
     *&#64;&#64;     not available for inferencing.
     *&#64;&#64;
     * </pre>
     *
     * <code>MODEL_UNLOADING = 4;</code>
     */
    public static final int MODEL_UNLOADING_VALUE = 4;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static ModelReadyState valueOf(int value) {
      return forNumber(value);
    }

    public static ModelReadyState forNumber(int value) {
      switch (value) {
        case 0: return MODEL_UNKNOWN;
        case 1: return MODEL_READY;
        case 2: return MODEL_UNAVAILABLE;
        case 3: return MODEL_LOADING;
        case 4: return MODEL_UNLOADING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ModelReadyState>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        ModelReadyState> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ModelReadyState>() {
            public ModelReadyState findValueByNumber(int number) {
              return ModelReadyState.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.getDescriptor().getEnumTypes().get(0);
    }

    private static final ModelReadyState[] VALUES = values();

    public static ModelReadyState valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private ModelReadyState(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelReadyState)
  }

  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: ServerReadyState
   *&#64;&#64;
   *&#64;&#64;   Readiness status for the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code nvidia.inferenceserver.ServerReadyState}
   */
  public enum ServerReadyState
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_INVALID = 0
     *&#64;&#64;
     *&#64;&#64;     The server is in an invalid state and will likely not
     *&#64;&#64;     response correctly to any requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_INVALID = 0;</code>
     */
    SERVER_INVALID(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_INITIALIZING = 1
     *&#64;&#64;
     *&#64;&#64;     The server is initializing.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_INITIALIZING = 1;</code>
     */
    SERVER_INITIALIZING(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_READY = 2
     *&#64;&#64;
     *&#64;&#64;     The server is ready and accepting requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_READY = 2;</code>
     */
    SERVER_READY(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_EXITING = 3
     *&#64;&#64;
     *&#64;&#64;     The server is exiting and will not respond to requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_EXITING = 3;</code>
     */
    SERVER_EXITING(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_FAILED_TO_INITIALIZE = 10
     *&#64;&#64;
     *&#64;&#64;     The server did not initialize correctly. Most requests will fail.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_FAILED_TO_INITIALIZE = 10;</code>
     */
    SERVER_FAILED_TO_INITIALIZE(10),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_INVALID = 0
     *&#64;&#64;
     *&#64;&#64;     The server is in an invalid state and will likely not
     *&#64;&#64;     response correctly to any requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_INVALID = 0;</code>
     */
    public static final int SERVER_INVALID_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_INITIALIZING = 1
     *&#64;&#64;
     *&#64;&#64;     The server is initializing.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_INITIALIZING = 1;</code>
     */
    public static final int SERVER_INITIALIZING_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_READY = 2
     *&#64;&#64;
     *&#64;&#64;     The server is ready and accepting requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_READY = 2;</code>
     */
    public static final int SERVER_READY_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_EXITING = 3
     *&#64;&#64;
     *&#64;&#64;     The server is exiting and will not respond to requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_EXITING = 3;</code>
     */
    public static final int SERVER_EXITING_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: ServerReadyState::SERVER_FAILED_TO_INITIALIZE = 10
     *&#64;&#64;
     *&#64;&#64;     The server did not initialize correctly. Most requests will fail.
     *&#64;&#64;
     * </pre>
     *
     * <code>SERVER_FAILED_TO_INITIALIZE = 10;</code>
     */
    public static final int SERVER_FAILED_TO_INITIALIZE_VALUE = 10;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static ServerReadyState valueOf(int value) {
      return forNumber(value);
    }

    public static ServerReadyState forNumber(int value) {
      switch (value) {
        case 0: return SERVER_INVALID;
        case 1: return SERVER_INITIALIZING;
        case 2: return SERVER_READY;
        case 3: return SERVER_EXITING;
        case 10: return SERVER_FAILED_TO_INITIALIZE;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ServerReadyState>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        ServerReadyState> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ServerReadyState>() {
            public ServerReadyState findValueByNumber(int number) {
              return ServerReadyState.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.getDescriptor().getEnumTypes().get(1);
    }

    private static final ServerReadyState[] VALUES = values();

    public static ServerReadyState valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private ServerReadyState(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ServerReadyState)
  }

  public interface StatDurationOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.StatDuration)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of times this metric occurred.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 count = 1;</code>
     */
    long getCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 total_time_ns
     *&#64;&#64;
     *&#64;&#64;     Total collected duration of this metric in nanoseconds.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 total_time_ns = 2;</code>
     */
    long getTotalTimeNs();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message StatDuration
   *&#64;&#64;
   *&#64;&#64;   Statistic collecting a duration metric.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.StatDuration}
   */
  public  static final class StatDuration extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.StatDuration)
      StatDurationOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StatDuration.newBuilder() to construct.
    private StatDuration(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StatDuration() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StatDuration();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StatDuration(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              count_ = input.readUInt64();
              break;
            }
            case 16: {

              totalTimeNs_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatDuration_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatDuration_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.class, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder.class);
    }

    public static final int COUNT_FIELD_NUMBER = 1;
    private long count_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of times this metric occurred.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 count = 1;</code>
     */
    public long getCount() {
      return count_;
    }

    public static final int TOTAL_TIME_NS_FIELD_NUMBER = 2;
    private long totalTimeNs_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 total_time_ns
     *&#64;&#64;
     *&#64;&#64;     Total collected duration of this metric in nanoseconds.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 total_time_ns = 2;</code>
     */
    public long getTotalTimeNs() {
      return totalTimeNs_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (count_ != 0L) {
        output.writeUInt64(1, count_);
      }
      if (totalTimeNs_ != 0L) {
        output.writeUInt64(2, totalTimeNs_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (count_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, count_);
      }
      if (totalTimeNs_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, totalTimeNs_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.StatDuration)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.StatDuration other = (nvidia.inferenceserver.ServerStatusOuterClass.StatDuration) obj;

      if (getCount()
          != other.getCount()) return false;
      if (getTotalTimeNs()
          != other.getTotalTimeNs()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getCount());
      hash = (37 * hash) + TOTAL_TIME_NS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getTotalTimeNs());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message StatDuration
     *&#64;&#64;
     *&#64;&#64;   Statistic collecting a duration metric.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.StatDuration}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.StatDuration)
        nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatDuration_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatDuration_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.class, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        count_ = 0L;

        totalTimeNs_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatDuration_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration build() {
        nvidia.inferenceserver.ServerStatusOuterClass.StatDuration result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.StatDuration result = new nvidia.inferenceserver.ServerStatusOuterClass.StatDuration(this);
        result.count_ = count_;
        result.totalTimeNs_ = totalTimeNs_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.StatDuration) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.StatDuration)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance()) return this;
        if (other.getCount() != 0L) {
          setCount(other.getCount());
        }
        if (other.getTotalTimeNs() != 0L) {
          setTotalTimeNs(other.getTotalTimeNs());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.StatDuration parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.StatDuration) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private long count_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of times this metric occurred.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 count = 1;</code>
       */
      public long getCount() {
        return count_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of times this metric occurred.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 count = 1;</code>
       */
      public Builder setCount(long value) {
        
        count_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of times this metric occurred.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 count = 1;</code>
       */
      public Builder clearCount() {
        
        count_ = 0L;
        onChanged();
        return this;
      }

      private long totalTimeNs_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 total_time_ns
       *&#64;&#64;
       *&#64;&#64;     Total collected duration of this metric in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 total_time_ns = 2;</code>
       */
      public long getTotalTimeNs() {
        return totalTimeNs_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 total_time_ns
       *&#64;&#64;
       *&#64;&#64;     Total collected duration of this metric in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 total_time_ns = 2;</code>
       */
      public Builder setTotalTimeNs(long value) {
        
        totalTimeNs_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 total_time_ns
       *&#64;&#64;
       *&#64;&#64;     Total collected duration of this metric in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 total_time_ns = 2;</code>
       */
      public Builder clearTotalTimeNs() {
        
        totalTimeNs_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.StatDuration)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatDuration)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.StatDuration DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.StatDuration();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StatDuration>
        PARSER = new com.google.protobuf.AbstractParser<StatDuration>() {
      @java.lang.Override
      public StatDuration parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StatDuration(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StatDuration> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StatDuration> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StatusRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.StatusRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message StatusRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for Status requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.StatusRequestStats}
   */
  public  static final class StatusRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.StatusRequestStats)
      StatusRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StatusRequestStats.newBuilder() to construct.
    private StatusRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StatusRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StatusRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StatusRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatusRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Status requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message StatusRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.StatusRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.StatusRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatusRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Status requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.StatusRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatusRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StatusRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<StatusRequestStats>() {
      @java.lang.Override
      public StatusRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StatusRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StatusRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StatusRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ProfileRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ProfileRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ProfileRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for Profile requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ProfileRequestStats}
   */
  public  static final class ProfileRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ProfileRequestStats)
      ProfileRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ProfileRequestStats.newBuilder() to construct.
    private ProfileRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ProfileRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ProfileRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ProfileRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ProfileRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Profile requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ProfileRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ProfileRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ProfileRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ProfileRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Profile requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ProfileRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ProfileRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ProfileRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<ProfileRequestStats>() {
      @java.lang.Override
      public ProfileRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ProfileRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ProfileRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ProfileRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface HealthRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.HealthRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message HealthRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for Health requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.HealthRequestStats}
   */
  public  static final class HealthRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.HealthRequestStats)
      HealthRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use HealthRequestStats.newBuilder() to construct.
    private HealthRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private HealthRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new HealthRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private HealthRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_HealthRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Health requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message HealthRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.HealthRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.HealthRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_HealthRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Health requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.HealthRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.HealthRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<HealthRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<HealthRequestStats>() {
      @java.lang.Override
      public HealthRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new HealthRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<HealthRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<HealthRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelControlRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelControlRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelControlRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for ModelControl requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelControlRequestStats}
   */
  public  static final class ModelControlRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelControlRequestStats)
      ModelControlRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelControlRequestStats.newBuilder() to construct.
    private ModelControlRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelControlRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelControlRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelControlRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelControlRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful ModelControl requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelControlRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelControlRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelControlRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelControlRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful ModelControl requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelControlRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelControlRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelControlRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<ModelControlRequestStats>() {
      @java.lang.Override
      public ModelControlRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelControlRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelControlRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelControlRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SharedMemoryControlRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.SharedMemoryControlRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message SharedMemoryControlRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for SharedMemoryControl requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.SharedMemoryControlRequestStats}
   */
  public  static final class SharedMemoryControlRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.SharedMemoryControlRequestStats)
      SharedMemoryControlRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SharedMemoryControlRequestStats.newBuilder() to construct.
    private SharedMemoryControlRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SharedMemoryControlRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SharedMemoryControlRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SharedMemoryControlRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful SharedMemoryControl
     *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message SharedMemoryControlRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.SharedMemoryControlRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.SharedMemoryControlRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful SharedMemoryControl
       *&#64;&#64;     requests, not including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.SharedMemoryControlRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryControlRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SharedMemoryControlRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<SharedMemoryControlRequestStats>() {
      @java.lang.Override
      public SharedMemoryControlRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SharedMemoryControlRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SharedMemoryControlRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SharedMemoryControlRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InferRequestStatsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferRequestStats)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    boolean hasSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    boolean hasFailed();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getFailed();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getFailedOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    boolean hasCompute();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getCompute();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getComputeOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    boolean hasQueue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getQueue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getQueueOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message InferRequestStats
   *&#64;&#64;
   *&#64;&#64;   Statistics collected for Infer requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.InferRequestStats}
   */
  public  static final class InferRequestStats extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferRequestStats)
      InferRequestStatsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InferRequestStats.newBuilder() to construct.
    private InferRequestStats(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InferRequestStats() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InferRequestStats();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InferRequestStats(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (success_ != null) {
                subBuilder = success_.toBuilder();
              }
              success_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(success_);
                success_ = subBuilder.buildPartial();
              }

              break;
            }
            case 18: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (failed_ != null) {
                subBuilder = failed_.toBuilder();
              }
              failed_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(failed_);
                failed_ = subBuilder.buildPartial();
              }

              break;
            }
            case 26: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (compute_ != null) {
                subBuilder = compute_.toBuilder();
              }
              compute_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(compute_);
                compute_ = subBuilder.buildPartial();
              }

              break;
            }
            case 34: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder subBuilder = null;
              if (queue_ != null) {
                subBuilder = queue_.toBuilder();
              }
              queue_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queue_);
                queue_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_InferRequestStats_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_InferRequestStats_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.Builder.class);
    }

    public static final int SUCCESS_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public boolean hasSuccess() {
      return success_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
      return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration success
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle successful Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
      return getSuccess();
    }

    public static final int FAILED_FIELD_NUMBER = 2;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration failed_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    public boolean hasFailed() {
      return failed_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getFailed() {
      return failed_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : failed_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration failed
     *&#64;&#64;
     *&#64;&#64;     Total time required to handle failed Infer requests, not
     *&#64;&#64;     including HTTP or gRPC endpoint termination time.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getFailedOrBuilder() {
      return getFailed();
    }

    public static final int COMPUTE_FIELD_NUMBER = 3;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration compute_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    public boolean hasCompute() {
      return compute_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getCompute() {
      return compute_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : compute_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration compute
     *&#64;&#64;
     *&#64;&#64;     Time required to run inferencing for an inference request;
     *&#64;&#64;     including time copying input tensors to GPU memory, time
     *&#64;&#64;     executing the model, and time copying output tensors from GPU
     *&#64;&#64;     memory.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getComputeOrBuilder() {
      return getCompute();
    }

    public static final int QUEUE_FIELD_NUMBER = 4;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration queue_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    public boolean hasQueue() {
      return queue_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getQueue() {
      return queue_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : queue_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatDuration queue
     *&#64;&#64;
     *&#64;&#64;     Time an inference request waits in scheduling queue for an
     *&#64;&#64;     available model instance.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getQueueOrBuilder() {
      return getQueue();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (success_ != null) {
        output.writeMessage(1, getSuccess());
      }
      if (failed_ != null) {
        output.writeMessage(2, getFailed());
      }
      if (compute_ != null) {
        output.writeMessage(3, getCompute());
      }
      if (queue_ != null) {
        output.writeMessage(4, getQueue());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (success_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getSuccess());
      }
      if (failed_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getFailed());
      }
      if (compute_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getCompute());
      }
      if (queue_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getQueue());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats other = (nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats) obj;

      if (hasSuccess() != other.hasSuccess()) return false;
      if (hasSuccess()) {
        if (!getSuccess()
            .equals(other.getSuccess())) return false;
      }
      if (hasFailed() != other.hasFailed()) return false;
      if (hasFailed()) {
        if (!getFailed()
            .equals(other.getFailed())) return false;
      }
      if (hasCompute() != other.hasCompute()) return false;
      if (hasCompute()) {
        if (!getCompute()
            .equals(other.getCompute())) return false;
      }
      if (hasQueue() != other.hasQueue()) return false;
      if (hasQueue()) {
        if (!getQueue()
            .equals(other.getQueue())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSuccess()) {
        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
        hash = (53 * hash) + getSuccess().hashCode();
      }
      if (hasFailed()) {
        hash = (37 * hash) + FAILED_FIELD_NUMBER;
        hash = (53 * hash) + getFailed().hashCode();
      }
      if (hasCompute()) {
        hash = (37 * hash) + COMPUTE_FIELD_NUMBER;
        hash = (53 * hash) + getCompute().hashCode();
      }
      if (hasQueue()) {
        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getQueue().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message InferRequestStats
     *&#64;&#64;
     *&#64;&#64;   Statistics collected for Infer requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferRequestStats}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferRequestStats)
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStatsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_InferRequestStats_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_InferRequestStats_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.class, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (successBuilder_ == null) {
          success_ = null;
        } else {
          success_ = null;
          successBuilder_ = null;
        }
        if (failedBuilder_ == null) {
          failed_ = null;
        } else {
          failed_ = null;
          failedBuilder_ = null;
        }
        if (computeBuilder_ == null) {
          compute_ = null;
        } else {
          compute_ = null;
          computeBuilder_ = null;
        }
        if (queueBuilder_ == null) {
          queue_ = null;
        } else {
          queue_ = null;
          queueBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_InferRequestStats_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats build() {
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats result = new nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats(this);
        if (successBuilder_ == null) {
          result.success_ = success_;
        } else {
          result.success_ = successBuilder_.build();
        }
        if (failedBuilder_ == null) {
          result.failed_ = failed_;
        } else {
          result.failed_ = failedBuilder_.build();
        }
        if (computeBuilder_ == null) {
          result.compute_ = compute_;
        } else {
          result.compute_ = computeBuilder_.build();
        }
        if (queueBuilder_ == null) {
          result.queue_ = queue_;
        } else {
          result.queue_ = queueBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.getDefaultInstance()) return this;
        if (other.hasSuccess()) {
          mergeSuccess(other.getSuccess());
        }
        if (other.hasFailed()) {
          mergeFailed(other.getFailed());
        }
        if (other.hasCompute()) {
          mergeCompute(other.getCompute());
        }
        if (other.hasQueue()) {
          mergeQueue(other.getQueue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration success_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> successBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public boolean hasSuccess() {
        return successBuilder_ != null || success_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getSuccess() {
        if (successBuilder_ == null) {
          return success_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        } else {
          return successBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          success_ = value;
          onChanged();
        } else {
          successBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder setSuccess(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (successBuilder_ == null) {
          success_ = builderForValue.build();
          onChanged();
        } else {
          successBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder mergeSuccess(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (successBuilder_ == null) {
          if (success_ != null) {
            success_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(success_).mergeFrom(value).buildPartial();
          } else {
            success_ = value;
          }
          onChanged();
        } else {
          successBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public Builder clearSuccess() {
        if (successBuilder_ == null) {
          success_ = null;
          onChanged();
        } else {
          success_ = null;
          successBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getSuccessBuilder() {
        
        onChanged();
        return getSuccessFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getSuccessOrBuilder() {
        if (successBuilder_ != null) {
          return successBuilder_.getMessageOrBuilder();
        } else {
          return success_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : success_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration success
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle successful Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration success = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getSuccessFieldBuilder() {
        if (successBuilder_ == null) {
          successBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getSuccess(),
                  getParentForChildren(),
                  isClean());
          success_ = null;
        }
        return successBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration failed_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> failedBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public boolean hasFailed() {
        return failedBuilder_ != null || failed_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getFailed() {
        if (failedBuilder_ == null) {
          return failed_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : failed_;
        } else {
          return failedBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public Builder setFailed(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (failedBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          failed_ = value;
          onChanged();
        } else {
          failedBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public Builder setFailed(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (failedBuilder_ == null) {
          failed_ = builderForValue.build();
          onChanged();
        } else {
          failedBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public Builder mergeFailed(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (failedBuilder_ == null) {
          if (failed_ != null) {
            failed_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(failed_).mergeFrom(value).buildPartial();
          } else {
            failed_ = value;
          }
          onChanged();
        } else {
          failedBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public Builder clearFailed() {
        if (failedBuilder_ == null) {
          failed_ = null;
          onChanged();
        } else {
          failed_ = null;
          failedBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getFailedBuilder() {
        
        onChanged();
        return getFailedFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getFailedOrBuilder() {
        if (failedBuilder_ != null) {
          return failedBuilder_.getMessageOrBuilder();
        } else {
          return failed_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : failed_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration failed
       *&#64;&#64;
       *&#64;&#64;     Total time required to handle failed Infer requests, not
       *&#64;&#64;     including HTTP or gRPC endpoint termination time.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration failed = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getFailedFieldBuilder() {
        if (failedBuilder_ == null) {
          failedBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getFailed(),
                  getParentForChildren(),
                  isClean());
          failed_ = null;
        }
        return failedBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration compute_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> computeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public boolean hasCompute() {
        return computeBuilder_ != null || compute_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getCompute() {
        if (computeBuilder_ == null) {
          return compute_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : compute_;
        } else {
          return computeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public Builder setCompute(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (computeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          compute_ = value;
          onChanged();
        } else {
          computeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public Builder setCompute(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (computeBuilder_ == null) {
          compute_ = builderForValue.build();
          onChanged();
        } else {
          computeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public Builder mergeCompute(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (computeBuilder_ == null) {
          if (compute_ != null) {
            compute_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(compute_).mergeFrom(value).buildPartial();
          } else {
            compute_ = value;
          }
          onChanged();
        } else {
          computeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public Builder clearCompute() {
        if (computeBuilder_ == null) {
          compute_ = null;
          onChanged();
        } else {
          compute_ = null;
          computeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getComputeBuilder() {
        
        onChanged();
        return getComputeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getComputeOrBuilder() {
        if (computeBuilder_ != null) {
          return computeBuilder_.getMessageOrBuilder();
        } else {
          return compute_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : compute_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration compute
       *&#64;&#64;
       *&#64;&#64;     Time required to run inferencing for an inference request;
       *&#64;&#64;     including time copying input tensors to GPU memory, time
       *&#64;&#64;     executing the model, and time copying output tensors from GPU
       *&#64;&#64;     memory.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration compute = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getComputeFieldBuilder() {
        if (computeBuilder_ == null) {
          computeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getCompute(),
                  getParentForChildren(),
                  isClean());
          compute_ = null;
        }
        return computeBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatDuration queue_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> queueBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public boolean hasQueue() {
        return queueBuilder_ != null || queue_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration getQueue() {
        if (queueBuilder_ == null) {
          return queue_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : queue_;
        } else {
          return queueBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public Builder setQueue(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (queueBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queue_ = value;
          onChanged();
        } else {
          queueBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public Builder setQueue(
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder builderForValue) {
        if (queueBuilder_ == null) {
          queue_ = builderForValue.build();
          onChanged();
        } else {
          queueBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public Builder mergeQueue(nvidia.inferenceserver.ServerStatusOuterClass.StatDuration value) {
        if (queueBuilder_ == null) {
          if (queue_ != null) {
            queue_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.newBuilder(queue_).mergeFrom(value).buildPartial();
          } else {
            queue_ = value;
          }
          onChanged();
        } else {
          queueBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public Builder clearQueue() {
        if (queueBuilder_ == null) {
          queue_ = null;
          onChanged();
        } else {
          queue_ = null;
          queueBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder getQueueBuilder() {
        
        onChanged();
        return getQueueFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder getQueueOrBuilder() {
        if (queueBuilder_ != null) {
          return queueBuilder_.getMessageOrBuilder();
        } else {
          return queue_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.getDefaultInstance() : queue_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatDuration queue
       *&#64;&#64;
       *&#64;&#64;     Time an inference request waits in scheduling queue for an
       *&#64;&#64;     available model instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatDuration queue = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder> 
          getQueueFieldBuilder() {
        if (queueBuilder_ == null) {
          queueBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatDuration, nvidia.inferenceserver.ServerStatusOuterClass.StatDuration.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatDurationOrBuilder>(
                  getQueue(),
                  getParentForChildren(),
                  isClean());
          queue_ = null;
        }
        return queueBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferRequestStats)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestStats)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InferRequestStats>
        PARSER = new com.google.protobuf.AbstractParser<InferRequestStats>() {
      @java.lang.Override
      public InferRequestStats parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InferRequestStats(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InferRequestStats> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InferRequestStats> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelVersionStatusOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionStatus)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
     */
    int getReadyStateValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState getReadyState();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */
    int getInferStatsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */
    boolean containsInferStats(
        int key);
    /**
     * Use {@link #getInferStatsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
    getInferStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */
    java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
    getInferStatsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrDefault(
        int key,
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrThrow(
        int key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 model_execution_count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of model executions performed for the
     *&#64;&#64;     model. A single model execution performs inferencing for
     *&#64;&#64;     the entire request batch and can perform inferencing for multiple
     *&#64;&#64;     requests if dynamic batching is enabled.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 model_execution_count = 3;</code>
     */
    long getModelExecutionCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 model_inference_count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of model inferences performed for the
     *&#64;&#64;     model. Each inference in a batched request is counted as
     *&#64;&#64;     an individual inference.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 model_inference_count = 4;</code>
     */
    long getModelInferenceCount();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelVersionStatus
   *&#64;&#64;
   *&#64;&#64;   Status for a version of a model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelVersionStatus}
   */
  public  static final class ModelVersionStatus extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionStatus)
      ModelVersionStatusOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelVersionStatus.newBuilder() to construct.
    private ModelVersionStatus(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelVersionStatus() {
      readyState_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelVersionStatus();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelVersionStatus(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();

              readyState_ = rawValue;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                inferStats_ = com.google.protobuf.MapField.newMapField(
                    InferStatsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
              inferStats__ = input.readMessage(
                  InferStatsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              inferStats_.getMutableMap().put(
                  inferStats__.getKey(), inferStats__.getValue());
              break;
            }
            case 24: {

              modelExecutionCount_ = input.readUInt64();
              break;
            }
            case 32: {

              modelInferenceCount_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 2:
          return internalGetInferStats();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.Builder.class);
    }

    public static final int READY_STATE_FIELD_NUMBER = 1;
    private int readyState_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
     */
    public int getReadyStateValue() {
      return readyState_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState getReadyState() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState result = nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.valueOf(readyState_);
      return result == null ? nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.UNRECOGNIZED : result;
    }

    public static final int INFER_STATS_FIELD_NUMBER = 2;
    private static final class InferStatsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>newDefaultInstance(
                  nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.UINT32,
                  0,
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> inferStats_;
    private com.google.protobuf.MapField<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
    internalGetInferStats() {
      if (inferStats_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            InferStatsDefaultEntryHolder.defaultEntry);
      }
      return inferStats_;
    }

    public int getInferStatsCount() {
      return internalGetInferStats().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    public boolean containsInferStats(
        int key) {
      
      return internalGetInferStats().getMap().containsKey(key);
    }
    /**
     * Use {@link #getInferStatsMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> getInferStats() {
      return getInferStatsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    public java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> getInferStatsMap() {
      return internalGetInferStats().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrDefault(
        int key,
        nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats defaultValue) {
      
      java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> map =
          internalGetInferStats().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
     *&#64;&#64;
     *&#64;&#64;     Inference statistics for the model, as a map from batch size
     *&#64;&#64;     to the statistics. A batch size will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that batch size.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrThrow(
        int key) {
      
      java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> map =
          internalGetInferStats().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int MODEL_EXECUTION_COUNT_FIELD_NUMBER = 3;
    private long modelExecutionCount_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 model_execution_count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of model executions performed for the
     *&#64;&#64;     model. A single model execution performs inferencing for
     *&#64;&#64;     the entire request batch and can perform inferencing for multiple
     *&#64;&#64;     requests if dynamic batching is enabled.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 model_execution_count = 3;</code>
     */
    public long getModelExecutionCount() {
      return modelExecutionCount_;
    }

    public static final int MODEL_INFERENCE_COUNT_FIELD_NUMBER = 4;
    private long modelInferenceCount_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 model_inference_count
     *&#64;&#64;
     *&#64;&#64;     Cumulative number of model inferences performed for the
     *&#64;&#64;     model. Each inference in a batched request is counted as
     *&#64;&#64;     an individual inference.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 model_inference_count = 4;</code>
     */
    public long getModelInferenceCount() {
      return modelInferenceCount_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (readyState_ != nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.MODEL_UNKNOWN.getNumber()) {
        output.writeEnum(1, readyState_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeIntegerMapTo(
          output,
          internalGetInferStats(),
          InferStatsDefaultEntryHolder.defaultEntry,
          2);
      if (modelExecutionCount_ != 0L) {
        output.writeUInt64(3, modelExecutionCount_);
      }
      if (modelInferenceCount_ != 0L) {
        output.writeUInt64(4, modelInferenceCount_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (readyState_ != nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.MODEL_UNKNOWN.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, readyState_);
      }
      for (java.util.Map.Entry<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> entry
           : internalGetInferStats().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
        inferStats__ = InferStatsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, inferStats__);
      }
      if (modelExecutionCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, modelExecutionCount_);
      }
      if (modelInferenceCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, modelInferenceCount_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus other = (nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus) obj;

      if (readyState_ != other.readyState_) return false;
      if (!internalGetInferStats().equals(
          other.internalGetInferStats())) return false;
      if (getModelExecutionCount()
          != other.getModelExecutionCount()) return false;
      if (getModelInferenceCount()
          != other.getModelInferenceCount()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + READY_STATE_FIELD_NUMBER;
      hash = (53 * hash) + readyState_;
      if (!internalGetInferStats().getMap().isEmpty()) {
        hash = (37 * hash) + INFER_STATS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetInferStats().hashCode();
      }
      hash = (37 * hash) + MODEL_EXECUTION_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getModelExecutionCount());
      hash = (37 * hash) + MODEL_INFERENCE_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getModelInferenceCount());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelVersionStatus
     *&#64;&#64;
     *&#64;&#64;   Status for a version of a model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionStatus}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionStatus)
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatusOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetInferStats();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetMutableInferStats();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        readyState_ = 0;

        internalGetMutableInferStats().clear();
        modelExecutionCount_ = 0L;

        modelInferenceCount_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus build() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus result = new nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus(this);
        int from_bitField0_ = bitField0_;
        result.readyState_ = readyState_;
        result.inferStats_ = internalGetInferStats();
        result.inferStats_.makeImmutable();
        result.modelExecutionCount_ = modelExecutionCount_;
        result.modelInferenceCount_ = modelInferenceCount_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.getDefaultInstance()) return this;
        if (other.readyState_ != 0) {
          setReadyStateValue(other.getReadyStateValue());
        }
        internalGetMutableInferStats().mergeFrom(
            other.internalGetInferStats());
        if (other.getModelExecutionCount() != 0L) {
          setModelExecutionCount(other.getModelExecutionCount());
        }
        if (other.getModelInferenceCount() != 0L) {
          setModelInferenceCount(other.getModelInferenceCount());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int readyState_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
       */
      public int getReadyStateValue() {
        return readyState_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
       */
      public Builder setReadyStateValue(int value) {
        readyState_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState getReadyState() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState result = nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.valueOf(readyState_);
        return result == null ? nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
       */
      public Builder setReadyState(nvidia.inferenceserver.ServerStatusOuterClass.ModelReadyState value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        readyState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelReadyState ready_statue
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelReadyState ready_state = 1;</code>
       */
      public Builder clearReadyState() {
        
        readyState_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> inferStats_;
      private com.google.protobuf.MapField<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
      internalGetInferStats() {
        if (inferStats_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InferStatsDefaultEntryHolder.defaultEntry);
        }
        return inferStats_;
      }
      private com.google.protobuf.MapField<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
      internalGetMutableInferStats() {
        onChanged();;
        if (inferStats_ == null) {
          inferStats_ = com.google.protobuf.MapField.newMapField(
              InferStatsDefaultEntryHolder.defaultEntry);
        }
        if (!inferStats_.isMutable()) {
          inferStats_ = inferStats_.copy();
        }
        return inferStats_;
      }

      public int getInferStatsCount() {
        return internalGetInferStats().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public boolean containsInferStats(
          int key) {
        
        return internalGetInferStats().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInferStatsMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> getInferStats() {
        return getInferStatsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> getInferStatsMap() {
        return internalGetInferStats().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrDefault(
          int key,
          nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats defaultValue) {
        
        java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> map =
            internalGetInferStats().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats getInferStatsOrThrow(
          int key) {
        
        java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> map =
            internalGetInferStats().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearInferStats() {
        internalGetMutableInferStats().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public Builder removeInferStats(
          int key) {
        
        internalGetMutableInferStats().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats>
      getMutableInferStats() {
        return internalGetMutableInferStats().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */
      public Builder putInferStats(
          int key,
          nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableInferStats().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, InferRequestStats&gt; infer_stats
       *&#64;&#64;
       *&#64;&#64;     Inference statistics for the model, as a map from batch size
       *&#64;&#64;     to the statistics. A batch size will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that batch size.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .nvidia.inferenceserver.InferRequestStats&gt; infer_stats = 2;</code>
       */

      public Builder putAllInferStats(
          java.util.Map<java.lang.Integer, nvidia.inferenceserver.ServerStatusOuterClass.InferRequestStats> values) {
        internalGetMutableInferStats().getMutableMap()
            .putAll(values);
        return this;
      }

      private long modelExecutionCount_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_execution_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model executions performed for the
       *&#64;&#64;     model. A single model execution performs inferencing for
       *&#64;&#64;     the entire request batch and can perform inferencing for multiple
       *&#64;&#64;     requests if dynamic batching is enabled.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_execution_count = 3;</code>
       */
      public long getModelExecutionCount() {
        return modelExecutionCount_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_execution_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model executions performed for the
       *&#64;&#64;     model. A single model execution performs inferencing for
       *&#64;&#64;     the entire request batch and can perform inferencing for multiple
       *&#64;&#64;     requests if dynamic batching is enabled.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_execution_count = 3;</code>
       */
      public Builder setModelExecutionCount(long value) {
        
        modelExecutionCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_execution_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model executions performed for the
       *&#64;&#64;     model. A single model execution performs inferencing for
       *&#64;&#64;     the entire request batch and can perform inferencing for multiple
       *&#64;&#64;     requests if dynamic batching is enabled.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_execution_count = 3;</code>
       */
      public Builder clearModelExecutionCount() {
        
        modelExecutionCount_ = 0L;
        onChanged();
        return this;
      }

      private long modelInferenceCount_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_inference_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model inferences performed for the
       *&#64;&#64;     model. Each inference in a batched request is counted as
       *&#64;&#64;     an individual inference.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_inference_count = 4;</code>
       */
      public long getModelInferenceCount() {
        return modelInferenceCount_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_inference_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model inferences performed for the
       *&#64;&#64;     model. Each inference in a batched request is counted as
       *&#64;&#64;     an individual inference.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_inference_count = 4;</code>
       */
      public Builder setModelInferenceCount(long value) {
        
        modelInferenceCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 model_inference_count
       *&#64;&#64;
       *&#64;&#64;     Cumulative number of model inferences performed for the
       *&#64;&#64;     model. Each inference in a batched request is counted as
       *&#64;&#64;     an individual inference.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 model_inference_count = 4;</code>
       */
      public Builder clearModelInferenceCount() {
        
        modelInferenceCount_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionStatus)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionStatus)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelVersionStatus>
        PARSER = new com.google.protobuf.AbstractParser<ModelVersionStatus>() {
      @java.lang.Override
      public ModelVersionStatus parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelVersionStatus(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelVersionStatus> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelVersionStatus> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelStatusOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelStatus)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    boolean hasConfig();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getConfig();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder getConfigOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */
    int getVersionStatusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */
    boolean containsVersionStatus(
        long key);
    /**
     * Use {@link #getVersionStatusMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
    getVersionStatus();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */
    java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
    getVersionStatusMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrDefault(
        long key,
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrThrow(
        long key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelStatus
   *&#64;&#64;
   *&#64;&#64;   Status for a model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelStatus}
   */
  public  static final class ModelStatus extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelStatus)
      ModelStatusOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelStatus.newBuilder() to construct.
    private ModelStatus(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelStatus() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelStatus();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelStatus(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder subBuilder = null;
              if (config_ != null) {
                subBuilder = config_.toBuilder();
              }
              config_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(config_);
                config_ = subBuilder.buildPartial();
              }

              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                versionStatus_ = com.google.protobuf.MapField.newMapField(
                    VersionStatusDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
              versionStatus__ = input.readMessage(
                  VersionStatusDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              versionStatus_.getMutableMap().put(
                  versionStatus__.getKey(), versionStatus__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 2:
          return internalGetVersionStatus();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.Builder.class);
    }

    public static final int CONFIG_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig config_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    public boolean hasConfig() {
      return config_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getConfig() {
      return config_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance() : config_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelConfig config
     *&#64;&#64;
     *&#64;&#64;     The configuration for the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder getConfigOrBuilder() {
      return getConfig();
    }

    public static final int VERSION_STATUS_FIELD_NUMBER = 2;
    private static final class VersionStatusDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>newDefaultInstance(
                  nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.INT64,
                  0L,
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> versionStatus_;
    private com.google.protobuf.MapField<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
    internalGetVersionStatus() {
      if (versionStatus_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            VersionStatusDefaultEntryHolder.defaultEntry);
      }
      return versionStatus_;
    }

    public int getVersionStatusCount() {
      return internalGetVersionStatus().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    public boolean containsVersionStatus(
        long key) {
      
      return internalGetVersionStatus().getMap().containsKey(key);
    }
    /**
     * Use {@link #getVersionStatusMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> getVersionStatus() {
      return getVersionStatusMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    public java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> getVersionStatusMap() {
      return internalGetVersionStatus().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrDefault(
        long key,
        nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus defaultValue) {
      
      java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> map =
          internalGetVersionStatus().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
     *&#64;&#64;
     *&#64;&#64;     Duration statistics for each version of the model, as a map
     *&#64;&#64;     from version to the status. A version will not occur in the map
     *&#64;&#64;     unless there has been at least one inference request of
     *&#64;&#64;     that model version. A version of -1 indicates the status is
     *&#64;&#64;     for requests for which the version could not be determined.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrThrow(
        long key) {
      
      java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> map =
          internalGetVersionStatus().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (config_ != null) {
        output.writeMessage(1, getConfig());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeLongMapTo(
          output,
          internalGetVersionStatus(),
          VersionStatusDefaultEntryHolder.defaultEntry,
          2);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (config_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getConfig());
      }
      for (java.util.Map.Entry<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> entry
           : internalGetVersionStatus().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
        versionStatus__ = VersionStatusDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, versionStatus__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus other = (nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus) obj;

      if (hasConfig() != other.hasConfig()) return false;
      if (hasConfig()) {
        if (!getConfig()
            .equals(other.getConfig())) return false;
      }
      if (!internalGetVersionStatus().equals(
          other.internalGetVersionStatus())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasConfig()) {
        hash = (37 * hash) + CONFIG_FIELD_NUMBER;
        hash = (53 * hash) + getConfig().hashCode();
      }
      if (!internalGetVersionStatus().getMap().isEmpty()) {
        hash = (37 * hash) + VERSION_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetVersionStatus().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelStatus
     *&#64;&#64;
     *&#64;&#64;   Status for a model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelStatus}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelStatus)
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatusOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetVersionStatus();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 2:
            return internalGetMutableVersionStatus();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (configBuilder_ == null) {
          config_ = null;
        } else {
          config_ = null;
          configBuilder_ = null;
        }
        internalGetMutableVersionStatus().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ModelStatus_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus build() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus result = new nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus(this);
        int from_bitField0_ = bitField0_;
        if (configBuilder_ == null) {
          result.config_ = config_;
        } else {
          result.config_ = configBuilder_.build();
        }
        result.versionStatus_ = internalGetVersionStatus();
        result.versionStatus_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.getDefaultInstance()) return this;
        if (other.hasConfig()) {
          mergeConfig(other.getConfig());
        }
        internalGetMutableVersionStatus().mergeFrom(
            other.internalGetVersionStatus());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig config_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder> configBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public boolean hasConfig() {
        return configBuilder_ != null || config_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getConfig() {
        if (configBuilder_ == null) {
          return config_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance() : config_;
        } else {
          return configBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public Builder setConfig(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig value) {
        if (configBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          config_ = value;
          onChanged();
        } else {
          configBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public Builder setConfig(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder builderForValue) {
        if (configBuilder_ == null) {
          config_ = builderForValue.build();
          onChanged();
        } else {
          configBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public Builder mergeConfig(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig value) {
        if (configBuilder_ == null) {
          if (config_ != null) {
            config_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.newBuilder(config_).mergeFrom(value).buildPartial();
          } else {
            config_ = value;
          }
          onChanged();
        } else {
          configBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public Builder clearConfig() {
        if (configBuilder_ == null) {
          config_ = null;
          onChanged();
        } else {
          config_ = null;
          configBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder getConfigBuilder() {
        
        onChanged();
        return getConfigFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder getConfigOrBuilder() {
        if (configBuilder_ != null) {
          return configBuilder_.getMessageOrBuilder();
        } else {
          return config_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance() : config_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelConfig config
       *&#64;&#64;
       *&#64;&#64;     The configuration for the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelConfig config = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder> 
          getConfigFieldBuilder() {
        if (configBuilder_ == null) {
          configBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder>(
                  getConfig(),
                  getParentForChildren(),
                  isClean());
          config_ = null;
        }
        return configBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> versionStatus_;
      private com.google.protobuf.MapField<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
      internalGetVersionStatus() {
        if (versionStatus_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              VersionStatusDefaultEntryHolder.defaultEntry);
        }
        return versionStatus_;
      }
      private com.google.protobuf.MapField<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
      internalGetMutableVersionStatus() {
        onChanged();;
        if (versionStatus_ == null) {
          versionStatus_ = com.google.protobuf.MapField.newMapField(
              VersionStatusDefaultEntryHolder.defaultEntry);
        }
        if (!versionStatus_.isMutable()) {
          versionStatus_ = versionStatus_.copy();
        }
        return versionStatus_;
      }

      public int getVersionStatusCount() {
        return internalGetVersionStatus().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public boolean containsVersionStatus(
          long key) {
        
        return internalGetVersionStatus().getMap().containsKey(key);
      }
      /**
       * Use {@link #getVersionStatusMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> getVersionStatus() {
        return getVersionStatusMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> getVersionStatusMap() {
        return internalGetVersionStatus().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrDefault(
          long key,
          nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus defaultValue) {
        
        java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> map =
            internalGetVersionStatus().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus getVersionStatusOrThrow(
          long key) {
        
        java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> map =
            internalGetVersionStatus().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearVersionStatus() {
        internalGetMutableVersionStatus().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public Builder removeVersionStatus(
          long key) {
        
        internalGetMutableVersionStatus().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus>
      getMutableVersionStatus() {
        return internalGetMutableVersionStatus().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */
      public Builder putVersionStatus(
          long key,
          nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableVersionStatus().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;int64, ModelVersionStatus&gt; version_status
       *&#64;&#64;
       *&#64;&#64;     Duration statistics for each version of the model, as a map
       *&#64;&#64;     from version to the status. A version will not occur in the map
       *&#64;&#64;     unless there has been at least one inference request of
       *&#64;&#64;     that model version. A version of -1 indicates the status is
       *&#64;&#64;     for requests for which the version could not be determined.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;int64, .nvidia.inferenceserver.ModelVersionStatus&gt; version_status = 2;</code>
       */

      public Builder putAllVersionStatus(
          java.util.Map<java.lang.Long, nvidia.inferenceserver.ServerStatusOuterClass.ModelVersionStatus> values) {
        internalGetMutableVersionStatus().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelStatus)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStatus)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelStatus>
        PARSER = new com.google.protobuf.AbstractParser<ModelStatus>() {
      @java.lang.Override
      public ModelStatus parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelStatus(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelStatus> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelStatus> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ServerStatusOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ServerStatus)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string id
     *&#64;&#64;
     *&#64;&#64;     The server's ID.
     *&#64;&#64;
     * </pre>
     *
     * <code>string id = 1;</code>
     */
    java.lang.String getId();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string id
     *&#64;&#64;
     *&#64;&#64;     The server's ID.
     *&#64;&#64;
     * </pre>
     *
     * <code>string id = 1;</code>
     */
    com.google.protobuf.ByteString
        getIdBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The server's version.
     *&#64;&#64;
     * </pre>
     *
     * <code>string version = 2;</code>
     */
    java.lang.String getVersion();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The server's version.
     *&#64;&#64;
     * </pre>
     *
     * <code>string version = 2;</code>
     */
    com.google.protobuf.ByteString
        getVersionBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the server.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
     */
    int getReadyStateValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the server.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState getReadyState();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 uptime_ns
     *&#64;&#64;
     *&#64;&#64;     Server uptime in nanoseconds.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 uptime_ns = 3;</code>
     */
    long getUptimeNs();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */
    int getModelStatusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */
    boolean containsModelStatus(
        java.lang.String key);
    /**
     * Use {@link #getModelStatusMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
    getModelStatus();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */
    java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
    getModelStatusMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    boolean hasStatusStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getStatusStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder getStatusStatsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    boolean hasProfileStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getProfileStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder getProfileStatsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    boolean hasHealthStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getHealthStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder getHealthStatsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    boolean hasModelControlStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getModelControlStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder getModelControlStatsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    boolean hasShmControlStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getShmControlStats();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder getShmControlStatsOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ServerStatus
   *&#64;&#64;
   *&#64;&#64;   Status for the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ServerStatus}
   */
  public  static final class ServerStatus extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ServerStatus)
      ServerStatusOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ServerStatus.newBuilder() to construct.
    private ServerStatus(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ServerStatus() {
      id_ = "";
      version_ = "";
      readyState_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ServerStatus();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ServerStatus(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              id_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();

              version_ = s;
              break;
            }
            case 24: {

              uptimeNs_ = input.readUInt64();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                modelStatus_ = com.google.protobuf.MapField.newMapField(
                    ModelStatusDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
              modelStatus__ = input.readMessage(
                  ModelStatusDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              modelStatus_.getMutableMap().put(
                  modelStatus__.getKey(), modelStatus__.getValue());
              break;
            }
            case 42: {
              nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder subBuilder = null;
              if (statusStats_ != null) {
                subBuilder = statusStats_.toBuilder();
              }
              statusStats_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(statusStats_);
                statusStats_ = subBuilder.buildPartial();
              }

              break;
            }
            case 50: {
              nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder subBuilder = null;
              if (profileStats_ != null) {
                subBuilder = profileStats_.toBuilder();
              }
              profileStats_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(profileStats_);
                profileStats_ = subBuilder.buildPartial();
              }

              break;
            }
            case 56: {
              int rawValue = input.readEnum();

              readyState_ = rawValue;
              break;
            }
            case 66: {
              nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder subBuilder = null;
              if (healthStats_ != null) {
                subBuilder = healthStats_.toBuilder();
              }
              healthStats_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(healthStats_);
                healthStats_ = subBuilder.buildPartial();
              }

              break;
            }
            case 74: {
              nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder subBuilder = null;
              if (modelControlStats_ != null) {
                subBuilder = modelControlStats_.toBuilder();
              }
              modelControlStats_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(modelControlStats_);
                modelControlStats_ = subBuilder.buildPartial();
              }

              break;
            }
            case 82: {
              nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder subBuilder = null;
              if (shmControlStats_ != null) {
                subBuilder = shmControlStats_.toBuilder();
              }
              shmControlStats_ = input.readMessage(nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(shmControlStats_);
                shmControlStats_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 4:
          return internalGetModelStatus();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.Builder.class);
    }

    public static final int ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object id_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string id
     *&#64;&#64;
     *&#64;&#64;     The server's ID.
     *&#64;&#64;
     * </pre>
     *
     * <code>string id = 1;</code>
     */
    public java.lang.String getId() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        id_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string id
     *&#64;&#64;
     *&#64;&#64;     The server's ID.
     *&#64;&#64;
     * </pre>
     *
     * <code>string id = 1;</code>
     */
    public com.google.protobuf.ByteString
        getIdBytes() {
      java.lang.Object ref = id_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        id_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERSION_FIELD_NUMBER = 2;
    private volatile java.lang.Object version_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The server's version.
     *&#64;&#64;
     * </pre>
     *
     * <code>string version = 2;</code>
     */
    public java.lang.String getVersion() {
      java.lang.Object ref = version_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        version_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The server's version.
     *&#64;&#64;
     * </pre>
     *
     * <code>string version = 2;</code>
     */
    public com.google.protobuf.ByteString
        getVersionBytes() {
      java.lang.Object ref = version_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        version_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int READY_STATE_FIELD_NUMBER = 7;
    private int readyState_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the server.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
     */
    public int getReadyStateValue() {
      return readyState_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
     *&#64;&#64;
     *&#64;&#64;     Current readiness state for the server.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState getReadyState() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState result = nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.valueOf(readyState_);
      return result == null ? nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.UNRECOGNIZED : result;
    }

    public static final int UPTIME_NS_FIELD_NUMBER = 3;
    private long uptimeNs_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 uptime_ns
     *&#64;&#64;
     *&#64;&#64;     Server uptime in nanoseconds.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 uptime_ns = 3;</code>
     */
    public long getUptimeNs() {
      return uptimeNs_;
    }

    public static final int MODEL_STATUS_FIELD_NUMBER = 4;
    private static final class ModelStatusDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>newDefaultInstance(
                  nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> modelStatus_;
    private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
    internalGetModelStatus() {
      if (modelStatus_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ModelStatusDefaultEntryHolder.defaultEntry);
      }
      return modelStatus_;
    }

    public int getModelStatusCount() {
      return internalGetModelStatus().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    public boolean containsModelStatus(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetModelStatus().getMap().containsKey(key);
    }
    /**
     * Use {@link #getModelStatusMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> getModelStatus() {
      return getModelStatusMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    public java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> getModelStatusMap() {
      return internalGetModelStatus().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> map =
          internalGetModelStatus().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
     *&#64;&#64;
     *&#64;&#64;     Status for each model, as a map from model name to the
     *&#64;&#64;     status.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
     */

    public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> map =
          internalGetModelStatus().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int STATUS_STATS_FIELD_NUMBER = 5;
    private nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats statusStats_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    public boolean hasStatusStats() {
      return statusStats_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getStatusStats() {
      return statusStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.getDefaultInstance() : statusStats_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Status requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder getStatusStatsOrBuilder() {
      return getStatusStats();
    }

    public static final int PROFILE_STATS_FIELD_NUMBER = 6;
    private nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats profileStats_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    public boolean hasProfileStats() {
      return profileStats_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getProfileStats() {
      return profileStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.getDefaultInstance() : profileStats_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Profile requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder getProfileStatsOrBuilder() {
      return getProfileStats();
    }

    public static final int HEALTH_STATS_FIELD_NUMBER = 8;
    private nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats healthStats_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    public boolean hasHealthStats() {
      return healthStats_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getHealthStats() {
      return healthStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.getDefaultInstance() : healthStats_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for Health requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder getHealthStatsOrBuilder() {
      return getHealthStats();
    }

    public static final int MODEL_CONTROL_STATS_FIELD_NUMBER = 9;
    private nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats modelControlStats_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    public boolean hasModelControlStats() {
      return modelControlStats_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getModelControlStats() {
      return modelControlStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.getDefaultInstance() : modelControlStats_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for ModelControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder getModelControlStatsOrBuilder() {
      return getModelControlStats();
    }

    public static final int SHM_CONTROL_STATS_FIELD_NUMBER = 10;
    private nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats shmControlStats_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    public boolean hasShmControlStats() {
      return shmControlStats_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getShmControlStats() {
      return shmControlStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.getDefaultInstance() : shmControlStats_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
     *&#64;&#64;
     *&#64;&#64;     Statistics for SharedMemoryControl requests.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
     */
    public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder getShmControlStatsOrBuilder() {
      return getShmControlStats();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getIdBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, id_);
      }
      if (!getVersionBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, version_);
      }
      if (uptimeNs_ != 0L) {
        output.writeUInt64(3, uptimeNs_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetModelStatus(),
          ModelStatusDefaultEntryHolder.defaultEntry,
          4);
      if (statusStats_ != null) {
        output.writeMessage(5, getStatusStats());
      }
      if (profileStats_ != null) {
        output.writeMessage(6, getProfileStats());
      }
      if (readyState_ != nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.SERVER_INVALID.getNumber()) {
        output.writeEnum(7, readyState_);
      }
      if (healthStats_ != null) {
        output.writeMessage(8, getHealthStats());
      }
      if (modelControlStats_ != null) {
        output.writeMessage(9, getModelControlStats());
      }
      if (shmControlStats_ != null) {
        output.writeMessage(10, getShmControlStats());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getIdBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, id_);
      }
      if (!getVersionBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, version_);
      }
      if (uptimeNs_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, uptimeNs_);
      }
      for (java.util.Map.Entry<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> entry
           : internalGetModelStatus().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
        modelStatus__ = ModelStatusDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, modelStatus__);
      }
      if (statusStats_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getStatusStats());
      }
      if (profileStats_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, getProfileStats());
      }
      if (readyState_ != nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.SERVER_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(7, readyState_);
      }
      if (healthStats_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, getHealthStats());
      }
      if (modelControlStats_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, getModelControlStats());
      }
      if (shmControlStats_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, getShmControlStats());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus other = (nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus) obj;

      if (!getId()
          .equals(other.getId())) return false;
      if (!getVersion()
          .equals(other.getVersion())) return false;
      if (readyState_ != other.readyState_) return false;
      if (getUptimeNs()
          != other.getUptimeNs()) return false;
      if (!internalGetModelStatus().equals(
          other.internalGetModelStatus())) return false;
      if (hasStatusStats() != other.hasStatusStats()) return false;
      if (hasStatusStats()) {
        if (!getStatusStats()
            .equals(other.getStatusStats())) return false;
      }
      if (hasProfileStats() != other.hasProfileStats()) return false;
      if (hasProfileStats()) {
        if (!getProfileStats()
            .equals(other.getProfileStats())) return false;
      }
      if (hasHealthStats() != other.hasHealthStats()) return false;
      if (hasHealthStats()) {
        if (!getHealthStats()
            .equals(other.getHealthStats())) return false;
      }
      if (hasModelControlStats() != other.hasModelControlStats()) return false;
      if (hasModelControlStats()) {
        if (!getModelControlStats()
            .equals(other.getModelControlStats())) return false;
      }
      if (hasShmControlStats() != other.hasShmControlStats()) return false;
      if (hasShmControlStats()) {
        if (!getShmControlStats()
            .equals(other.getShmControlStats())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + getId().hashCode();
      hash = (37 * hash) + VERSION_FIELD_NUMBER;
      hash = (53 * hash) + getVersion().hashCode();
      hash = (37 * hash) + READY_STATE_FIELD_NUMBER;
      hash = (53 * hash) + readyState_;
      hash = (37 * hash) + UPTIME_NS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getUptimeNs());
      if (!internalGetModelStatus().getMap().isEmpty()) {
        hash = (37 * hash) + MODEL_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetModelStatus().hashCode();
      }
      if (hasStatusStats()) {
        hash = (37 * hash) + STATUS_STATS_FIELD_NUMBER;
        hash = (53 * hash) + getStatusStats().hashCode();
      }
      if (hasProfileStats()) {
        hash = (37 * hash) + PROFILE_STATS_FIELD_NUMBER;
        hash = (53 * hash) + getProfileStats().hashCode();
      }
      if (hasHealthStats()) {
        hash = (37 * hash) + HEALTH_STATS_FIELD_NUMBER;
        hash = (53 * hash) + getHealthStats().hashCode();
      }
      if (hasModelControlStats()) {
        hash = (37 * hash) + MODEL_CONTROL_STATS_FIELD_NUMBER;
        hash = (53 * hash) + getModelControlStats().hashCode();
      }
      if (hasShmControlStats()) {
        hash = (37 * hash) + SHM_CONTROL_STATS_FIELD_NUMBER;
        hash = (53 * hash) + getShmControlStats().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ServerStatus
     *&#64;&#64;
     *&#64;&#64;   Status for the inference server.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ServerStatus}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ServerStatus)
        nvidia.inferenceserver.ServerStatusOuterClass.ServerStatusOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetModelStatus();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetMutableModelStatus();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.class, nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = "";

        version_ = "";

        readyState_ = 0;

        uptimeNs_ = 0L;

        internalGetMutableModelStatus().clear();
        if (statusStatsBuilder_ == null) {
          statusStats_ = null;
        } else {
          statusStats_ = null;
          statusStatsBuilder_ = null;
        }
        if (profileStatsBuilder_ == null) {
          profileStats_ = null;
        } else {
          profileStats_ = null;
          profileStatsBuilder_ = null;
        }
        if (healthStatsBuilder_ == null) {
          healthStats_ = null;
        } else {
          healthStats_ = null;
          healthStatsBuilder_ = null;
        }
        if (modelControlStatsBuilder_ == null) {
          modelControlStats_ = null;
        } else {
          modelControlStats_ = null;
          modelControlStatsBuilder_ = null;
        }
        if (shmControlStatsBuilder_ == null) {
          shmControlStats_ = null;
        } else {
          shmControlStats_ = null;
          shmControlStatsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.internal_static_nvidia_inferenceserver_ServerStatus_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus getDefaultInstanceForType() {
        return nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus build() {
        nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus buildPartial() {
        nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus result = new nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus(this);
        int from_bitField0_ = bitField0_;
        result.id_ = id_;
        result.version_ = version_;
        result.readyState_ = readyState_;
        result.uptimeNs_ = uptimeNs_;
        result.modelStatus_ = internalGetModelStatus();
        result.modelStatus_.makeImmutable();
        if (statusStatsBuilder_ == null) {
          result.statusStats_ = statusStats_;
        } else {
          result.statusStats_ = statusStatsBuilder_.build();
        }
        if (profileStatsBuilder_ == null) {
          result.profileStats_ = profileStats_;
        } else {
          result.profileStats_ = profileStatsBuilder_.build();
        }
        if (healthStatsBuilder_ == null) {
          result.healthStats_ = healthStats_;
        } else {
          result.healthStats_ = healthStatsBuilder_.build();
        }
        if (modelControlStatsBuilder_ == null) {
          result.modelControlStats_ = modelControlStats_;
        } else {
          result.modelControlStats_ = modelControlStatsBuilder_.build();
        }
        if (shmControlStatsBuilder_ == null) {
          result.shmControlStats_ = shmControlStats_;
        } else {
          result.shmControlStats_ = shmControlStatsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus) {
          return mergeFrom((nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus other) {
        if (other == nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus.getDefaultInstance()) return this;
        if (!other.getId().isEmpty()) {
          id_ = other.id_;
          onChanged();
        }
        if (!other.getVersion().isEmpty()) {
          version_ = other.version_;
          onChanged();
        }
        if (other.readyState_ != 0) {
          setReadyStateValue(other.getReadyStateValue());
        }
        if (other.getUptimeNs() != 0L) {
          setUptimeNs(other.getUptimeNs());
        }
        internalGetMutableModelStatus().mergeFrom(
            other.internalGetModelStatus());
        if (other.hasStatusStats()) {
          mergeStatusStats(other.getStatusStats());
        }
        if (other.hasProfileStats()) {
          mergeProfileStats(other.getProfileStats());
        }
        if (other.hasHealthStats()) {
          mergeHealthStats(other.getHealthStats());
        }
        if (other.hasModelControlStats()) {
          mergeModelControlStats(other.getModelControlStats());
        }
        if (other.hasShmControlStats()) {
          mergeShmControlStats(other.getShmControlStats());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object id_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string id
       *&#64;&#64;
       *&#64;&#64;     The server's ID.
       *&#64;&#64;
       * </pre>
       *
       * <code>string id = 1;</code>
       */
      public java.lang.String getId() {
        java.lang.Object ref = id_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          id_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string id
       *&#64;&#64;
       *&#64;&#64;     The server's ID.
       *&#64;&#64;
       * </pre>
       *
       * <code>string id = 1;</code>
       */
      public com.google.protobuf.ByteString
          getIdBytes() {
        java.lang.Object ref = id_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          id_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string id
       *&#64;&#64;
       *&#64;&#64;     The server's ID.
       *&#64;&#64;
       * </pre>
       *
       * <code>string id = 1;</code>
       */
      public Builder setId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string id
       *&#64;&#64;
       *&#64;&#64;     The server's ID.
       *&#64;&#64;
       * </pre>
       *
       * <code>string id = 1;</code>
       */
      public Builder clearId() {
        
        id_ = getDefaultInstance().getId();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string id
       *&#64;&#64;
       *&#64;&#64;     The server's ID.
       *&#64;&#64;
       * </pre>
       *
       * <code>string id = 1;</code>
       */
      public Builder setIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        id_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object version_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string version
       *&#64;&#64;
       *&#64;&#64;     The server's version.
       *&#64;&#64;
       * </pre>
       *
       * <code>string version = 2;</code>
       */
      public java.lang.String getVersion() {
        java.lang.Object ref = version_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          version_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string version
       *&#64;&#64;
       *&#64;&#64;     The server's version.
       *&#64;&#64;
       * </pre>
       *
       * <code>string version = 2;</code>
       */
      public com.google.protobuf.ByteString
          getVersionBytes() {
        java.lang.Object ref = version_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          version_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string version
       *&#64;&#64;
       *&#64;&#64;     The server's version.
       *&#64;&#64;
       * </pre>
       *
       * <code>string version = 2;</code>
       */
      public Builder setVersion(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string version
       *&#64;&#64;
       *&#64;&#64;     The server's version.
       *&#64;&#64;
       * </pre>
       *
       * <code>string version = 2;</code>
       */
      public Builder clearVersion() {
        
        version_ = getDefaultInstance().getVersion();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string version
       *&#64;&#64;
       *&#64;&#64;     The server's version.
       *&#64;&#64;
       * </pre>
       *
       * <code>string version = 2;</code>
       */
      public Builder setVersionBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        version_ = value;
        onChanged();
        return this;
      }

      private int readyState_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the server.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
       */
      public int getReadyStateValue() {
        return readyState_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the server.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
       */
      public Builder setReadyStateValue(int value) {
        readyState_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the server.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState getReadyState() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState result = nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.valueOf(readyState_);
        return result == null ? nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the server.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
       */
      public Builder setReadyState(nvidia.inferenceserver.ServerStatusOuterClass.ServerReadyState value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        readyState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ServerReadyState ready_state
       *&#64;&#64;
       *&#64;&#64;     Current readiness state for the server.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ServerReadyState ready_state = 7;</code>
       */
      public Builder clearReadyState() {
        
        readyState_ = 0;
        onChanged();
        return this;
      }

      private long uptimeNs_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 uptime_ns
       *&#64;&#64;
       *&#64;&#64;     Server uptime in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 uptime_ns = 3;</code>
       */
      public long getUptimeNs() {
        return uptimeNs_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 uptime_ns
       *&#64;&#64;
       *&#64;&#64;     Server uptime in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 uptime_ns = 3;</code>
       */
      public Builder setUptimeNs(long value) {
        
        uptimeNs_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 uptime_ns
       *&#64;&#64;
       *&#64;&#64;     Server uptime in nanoseconds.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 uptime_ns = 3;</code>
       */
      public Builder clearUptimeNs() {
        
        uptimeNs_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> modelStatus_;
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
      internalGetModelStatus() {
        if (modelStatus_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ModelStatusDefaultEntryHolder.defaultEntry);
        }
        return modelStatus_;
      }
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
      internalGetMutableModelStatus() {
        onChanged();;
        if (modelStatus_ == null) {
          modelStatus_ = com.google.protobuf.MapField.newMapField(
              ModelStatusDefaultEntryHolder.defaultEntry);
        }
        if (!modelStatus_.isMutable()) {
          modelStatus_ = modelStatus_.copy();
        }
        return modelStatus_;
      }

      public int getModelStatusCount() {
        return internalGetModelStatus().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public boolean containsModelStatus(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetModelStatus().getMap().containsKey(key);
      }
      /**
       * Use {@link #getModelStatusMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> getModelStatus() {
        return getModelStatusMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> getModelStatusMap() {
        return internalGetModelStatus().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrDefault(
          java.lang.String key,
          nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> map =
            internalGetModelStatus().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus getModelStatusOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> map =
            internalGetModelStatus().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearModelStatus() {
        internalGetMutableModelStatus().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public Builder removeModelStatus(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableModelStatus().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus>
      getMutableModelStatus() {
        return internalGetMutableModelStatus().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */
      public Builder putModelStatus(
          java.lang.String key,
          nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableModelStatus().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, ModelStatus&gt; model_status
       *&#64;&#64;
       *&#64;&#64;     Status for each model, as a map from model name to the
       *&#64;&#64;     status.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelStatus&gt; model_status = 4;</code>
       */

      public Builder putAllModelStatus(
          java.util.Map<java.lang.String, nvidia.inferenceserver.ServerStatusOuterClass.ModelStatus> values) {
        internalGetMutableModelStatus().getMutableMap()
            .putAll(values);
        return this;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats statusStats_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder> statusStatsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public boolean hasStatusStats() {
        return statusStatsBuilder_ != null || statusStats_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats getStatusStats() {
        if (statusStatsBuilder_ == null) {
          return statusStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.getDefaultInstance() : statusStats_;
        } else {
          return statusStatsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public Builder setStatusStats(nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats value) {
        if (statusStatsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          statusStats_ = value;
          onChanged();
        } else {
          statusStatsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public Builder setStatusStats(
          nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder builderForValue) {
        if (statusStatsBuilder_ == null) {
          statusStats_ = builderForValue.build();
          onChanged();
        } else {
          statusStatsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public Builder mergeStatusStats(nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats value) {
        if (statusStatsBuilder_ == null) {
          if (statusStats_ != null) {
            statusStats_ =
              nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.newBuilder(statusStats_).mergeFrom(value).buildPartial();
          } else {
            statusStats_ = value;
          }
          onChanged();
        } else {
          statusStatsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public Builder clearStatusStats() {
        if (statusStatsBuilder_ == null) {
          statusStats_ = null;
          onChanged();
        } else {
          statusStats_ = null;
          statusStatsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder getStatusStatsBuilder() {
        
        onChanged();
        return getStatusStatsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder getStatusStatsOrBuilder() {
        if (statusStatsBuilder_ != null) {
          return statusStatsBuilder_.getMessageOrBuilder();
        } else {
          return statusStats_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.getDefaultInstance() : statusStats_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: StatusRequestStats status_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Status requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.StatusRequestStats status_stats = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder> 
          getStatusStatsFieldBuilder() {
        if (statusStatsBuilder_ == null) {
          statusStatsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.StatusRequestStatsOrBuilder>(
                  getStatusStats(),
                  getParentForChildren(),
                  isClean());
          statusStats_ = null;
        }
        return statusStatsBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats profileStats_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder> profileStatsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public boolean hasProfileStats() {
        return profileStatsBuilder_ != null || profileStats_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats getProfileStats() {
        if (profileStatsBuilder_ == null) {
          return profileStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.getDefaultInstance() : profileStats_;
        } else {
          return profileStatsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public Builder setProfileStats(nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats value) {
        if (profileStatsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          profileStats_ = value;
          onChanged();
        } else {
          profileStatsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public Builder setProfileStats(
          nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder builderForValue) {
        if (profileStatsBuilder_ == null) {
          profileStats_ = builderForValue.build();
          onChanged();
        } else {
          profileStatsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public Builder mergeProfileStats(nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats value) {
        if (profileStatsBuilder_ == null) {
          if (profileStats_ != null) {
            profileStats_ =
              nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.newBuilder(profileStats_).mergeFrom(value).buildPartial();
          } else {
            profileStats_ = value;
          }
          onChanged();
        } else {
          profileStatsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public Builder clearProfileStats() {
        if (profileStatsBuilder_ == null) {
          profileStats_ = null;
          onChanged();
        } else {
          profileStats_ = null;
          profileStatsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder getProfileStatsBuilder() {
        
        onChanged();
        return getProfileStatsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder getProfileStatsOrBuilder() {
        if (profileStatsBuilder_ != null) {
          return profileStatsBuilder_.getMessageOrBuilder();
        } else {
          return profileStats_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.getDefaultInstance() : profileStats_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ProfileRequestStats profile_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Profile requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder> 
          getProfileStatsFieldBuilder() {
        if (profileStatsBuilder_ == null) {
          profileStatsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ProfileRequestStatsOrBuilder>(
                  getProfileStats(),
                  getParentForChildren(),
                  isClean());
          profileStats_ = null;
        }
        return profileStatsBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats healthStats_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder> healthStatsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public boolean hasHealthStats() {
        return healthStatsBuilder_ != null || healthStats_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats getHealthStats() {
        if (healthStatsBuilder_ == null) {
          return healthStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.getDefaultInstance() : healthStats_;
        } else {
          return healthStatsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public Builder setHealthStats(nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats value) {
        if (healthStatsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          healthStats_ = value;
          onChanged();
        } else {
          healthStatsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public Builder setHealthStats(
          nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder builderForValue) {
        if (healthStatsBuilder_ == null) {
          healthStats_ = builderForValue.build();
          onChanged();
        } else {
          healthStatsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public Builder mergeHealthStats(nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats value) {
        if (healthStatsBuilder_ == null) {
          if (healthStats_ != null) {
            healthStats_ =
              nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.newBuilder(healthStats_).mergeFrom(value).buildPartial();
          } else {
            healthStats_ = value;
          }
          onChanged();
        } else {
          healthStatsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public Builder clearHealthStats() {
        if (healthStatsBuilder_ == null) {
          healthStats_ = null;
          onChanged();
        } else {
          healthStats_ = null;
          healthStatsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder getHealthStatsBuilder() {
        
        onChanged();
        return getHealthStatsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder getHealthStatsOrBuilder() {
        if (healthStatsBuilder_ != null) {
          return healthStatsBuilder_.getMessageOrBuilder();
        } else {
          return healthStats_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.getDefaultInstance() : healthStats_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: HealthRequestStats health_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for Health requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.HealthRequestStats health_stats = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder> 
          getHealthStatsFieldBuilder() {
        if (healthStatsBuilder_ == null) {
          healthStatsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.HealthRequestStatsOrBuilder>(
                  getHealthStats(),
                  getParentForChildren(),
                  isClean());
          healthStats_ = null;
        }
        return healthStatsBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats modelControlStats_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder> modelControlStatsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public boolean hasModelControlStats() {
        return modelControlStatsBuilder_ != null || modelControlStats_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats getModelControlStats() {
        if (modelControlStatsBuilder_ == null) {
          return modelControlStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.getDefaultInstance() : modelControlStats_;
        } else {
          return modelControlStatsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public Builder setModelControlStats(nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats value) {
        if (modelControlStatsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          modelControlStats_ = value;
          onChanged();
        } else {
          modelControlStatsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public Builder setModelControlStats(
          nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder builderForValue) {
        if (modelControlStatsBuilder_ == null) {
          modelControlStats_ = builderForValue.build();
          onChanged();
        } else {
          modelControlStatsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public Builder mergeModelControlStats(nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats value) {
        if (modelControlStatsBuilder_ == null) {
          if (modelControlStats_ != null) {
            modelControlStats_ =
              nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.newBuilder(modelControlStats_).mergeFrom(value).buildPartial();
          } else {
            modelControlStats_ = value;
          }
          onChanged();
        } else {
          modelControlStatsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public Builder clearModelControlStats() {
        if (modelControlStatsBuilder_ == null) {
          modelControlStats_ = null;
          onChanged();
        } else {
          modelControlStats_ = null;
          modelControlStatsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder getModelControlStatsBuilder() {
        
        onChanged();
        return getModelControlStatsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder getModelControlStatsOrBuilder() {
        if (modelControlStatsBuilder_ != null) {
          return modelControlStatsBuilder_.getMessageOrBuilder();
        } else {
          return modelControlStats_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.getDefaultInstance() : modelControlStats_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelControlRequestStats model_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for ModelControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder> 
          getModelControlStatsFieldBuilder() {
        if (modelControlStatsBuilder_ == null) {
          modelControlStatsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.ModelControlRequestStatsOrBuilder>(
                  getModelControlStats(),
                  getParentForChildren(),
                  isClean());
          modelControlStats_ = null;
        }
        return modelControlStatsBuilder_;
      }

      private nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats shmControlStats_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder> shmControlStatsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public boolean hasShmControlStats() {
        return shmControlStatsBuilder_ != null || shmControlStats_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats getShmControlStats() {
        if (shmControlStatsBuilder_ == null) {
          return shmControlStats_ == null ? nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.getDefaultInstance() : shmControlStats_;
        } else {
          return shmControlStatsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public Builder setShmControlStats(nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats value) {
        if (shmControlStatsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          shmControlStats_ = value;
          onChanged();
        } else {
          shmControlStatsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public Builder setShmControlStats(
          nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder builderForValue) {
        if (shmControlStatsBuilder_ == null) {
          shmControlStats_ = builderForValue.build();
          onChanged();
        } else {
          shmControlStatsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public Builder mergeShmControlStats(nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats value) {
        if (shmControlStatsBuilder_ == null) {
          if (shmControlStats_ != null) {
            shmControlStats_ =
              nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.newBuilder(shmControlStats_).mergeFrom(value).buildPartial();
          } else {
            shmControlStats_ = value;
          }
          onChanged();
        } else {
          shmControlStatsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public Builder clearShmControlStats() {
        if (shmControlStatsBuilder_ == null) {
          shmControlStats_ = null;
          onChanged();
        } else {
          shmControlStats_ = null;
          shmControlStatsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder getShmControlStatsBuilder() {
        
        onChanged();
        return getShmControlStatsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      public nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder getShmControlStatsOrBuilder() {
        if (shmControlStatsBuilder_ != null) {
          return shmControlStatsBuilder_.getMessageOrBuilder();
        } else {
          return shmControlStats_ == null ?
              nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.getDefaultInstance() : shmControlStats_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: SharedMemoryControlRequestStats shm_control_stats
       *&#64;&#64;
       *&#64;&#64;     Statistics for SharedMemoryControl requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder> 
          getShmControlStatsFieldBuilder() {
        if (shmControlStatsBuilder_ == null) {
          shmControlStatsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStats.Builder, nvidia.inferenceserver.ServerStatusOuterClass.SharedMemoryControlRequestStatsOrBuilder>(
                  getShmControlStats(),
                  getParentForChildren(),
                  isClean());
          shmControlStats_ = null;
        }
        return shmControlStatsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ServerStatus)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerStatus)
    private static final nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus();
    }

    public static nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ServerStatus>
        PARSER = new com.google.protobuf.AbstractParser<ServerStatus>() {
      @java.lang.Override
      public ServerStatus parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ServerStatus(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ServerStatus> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ServerStatus> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ServerStatusOuterClass.ServerStatus getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_StatDuration_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_StatDuration_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_StatusRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ProfileRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_HealthRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelControlRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferRequestStats_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferRequestStats_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionStatus_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelStatus_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelStatus_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ServerStatus_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ServerStatus_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\023server_status.proto\022\026nvidia.inferences" +
      "erver\032\022model_config.proto\"4\n\014StatDuratio" +
      "n\022\r\n\005count\030\001 \001(\004\022\025\n\rtotal_time_ns\030\002 \001(\004\"" +
      "K\n\022StatusRequestStats\0225\n\007success\030\001 \001(\0132$" +
      ".nvidia.inferenceserver.StatDuration\"L\n\023" +
      "ProfileRequestStats\0225\n\007success\030\001 \001(\0132$.n" +
      "vidia.inferenceserver.StatDuration\"K\n\022He" +
      "althRequestStats\0225\n\007success\030\001 \001(\0132$.nvid" +
      "ia.inferenceserver.StatDuration\"Q\n\030Model" +
      "ControlRequestStats\0225\n\007success\030\001 \001(\0132$.n" +
      "vidia.inferenceserver.StatDuration\"X\n\037Sh" +
      "aredMemoryControlRequestStats\0225\n\007success" +
      "\030\001 \001(\0132$.nvidia.inferenceserver.StatDura" +
      "tion\"\354\001\n\021InferRequestStats\0225\n\007success\030\001 " +
      "\001(\0132$.nvidia.inferenceserver.StatDuratio" +
      "n\0224\n\006failed\030\002 \001(\0132$.nvidia.inferenceserv" +
      "er.StatDuration\0225\n\007compute\030\003 \001(\0132$.nvidi" +
      "a.inferenceserver.StatDuration\0223\n\005queue\030" +
      "\004 \001(\0132$.nvidia.inferenceserver.StatDurat" +
      "ion\"\277\002\n\022ModelVersionStatus\022<\n\013ready_stat" +
      "e\030\001 \001(\0162\'.nvidia.inferenceserver.ModelRe" +
      "adyState\022O\n\013infer_stats\030\002 \003(\0132:.nvidia.i" +
      "nferenceserver.ModelVersionStatus.InferS" +
      "tatsEntry\022\035\n\025model_execution_count\030\003 \001(\004" +
      "\022\035\n\025model_inference_count\030\004 \001(\004\032\\\n\017Infer" +
      "StatsEntry\022\013\n\003key\030\001 \001(\r\0228\n\005value\030\002 \001(\0132)" +
      ".nvidia.inferenceserver.InferRequestStat" +
      "s:\0028\001\"\364\001\n\013ModelStatus\0223\n\006config\030\001 \001(\0132#." +
      "nvidia.inferenceserver.ModelConfig\022N\n\016ve" +
      "rsion_status\030\002 \003(\01326.nvidia.inferenceser" +
      "ver.ModelStatus.VersionStatusEntry\032`\n\022Ve" +
      "rsionStatusEntry\022\013\n\003key\030\001 \001(\003\0229\n\005value\030\002" +
      " \001(\0132*.nvidia.inferenceserver.ModelVersi" +
      "onStatus:\0028\001\"\216\005\n\014ServerStatus\022\n\n\002id\030\001 \001(" +
      "\t\022\017\n\007version\030\002 \001(\t\022=\n\013ready_state\030\007 \001(\0162" +
      "(.nvidia.inferenceserver.ServerReadyStat" +
      "e\022\021\n\tuptime_ns\030\003 \001(\004\022K\n\014model_status\030\004 \003" +
      "(\01325.nvidia.inferenceserver.ServerStatus" +
      ".ModelStatusEntry\022@\n\014status_stats\030\005 \001(\0132" +
      "*.nvidia.inferenceserver.StatusRequestSt" +
      "ats\022B\n\rprofile_stats\030\006 \001(\0132+.nvidia.infe" +
      "renceserver.ProfileRequestStats\022@\n\014healt" +
      "h_stats\030\010 \001(\0132*.nvidia.inferenceserver.H" +
      "ealthRequestStats\022M\n\023model_control_stats" +
      "\030\t \001(\01320.nvidia.inferenceserver.ModelCon" +
      "trolRequestStats\022R\n\021shm_control_stats\030\n " +
      "\001(\01327.nvidia.inferenceserver.SharedMemor" +
      "yControlRequestStats\032W\n\020ModelStatusEntry" +
      "\022\013\n\003key\030\001 \001(\t\0222\n\005value\030\002 \001(\0132#.nvidia.in" +
      "ferenceserver.ModelStatus:\0028\001*t\n\017ModelRe" +
      "adyState\022\021\n\rMODEL_UNKNOWN\020\000\022\017\n\013MODEL_REA" +
      "DY\020\001\022\025\n\021MODEL_UNAVAILABLE\020\002\022\021\n\rMODEL_LOA" +
      "DING\020\003\022\023\n\017MODEL_UNLOADING\020\004*\206\001\n\020ServerRe" +
      "adyState\022\022\n\016SERVER_INVALID\020\000\022\027\n\023SERVER_I" +
      "NITIALIZING\020\001\022\020\n\014SERVER_READY\020\002\022\022\n\016SERVE" +
      "R_EXITING\020\003\022\037\n\033SERVER_FAILED_TO_INITIALI" +
      "ZE\020\nb\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          nvidia.inferenceserver.ModelConfigOuterClass.getDescriptor(),
        });
    internal_static_nvidia_inferenceserver_StatDuration_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_nvidia_inferenceserver_StatDuration_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_StatDuration_descriptor,
        new java.lang.String[] { "Count", "TotalTimeNs", });
    internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_nvidia_inferenceserver_StatusRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_StatusRequestStats_descriptor,
        new java.lang.String[] { "Success", });
    internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_nvidia_inferenceserver_ProfileRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ProfileRequestStats_descriptor,
        new java.lang.String[] { "Success", });
    internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_nvidia_inferenceserver_HealthRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_HealthRequestStats_descriptor,
        new java.lang.String[] { "Success", });
    internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_nvidia_inferenceserver_ModelControlRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelControlRequestStats_descriptor,
        new java.lang.String[] { "Success", });
    internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_SharedMemoryControlRequestStats_descriptor,
        new java.lang.String[] { "Success", });
    internal_static_nvidia_inferenceserver_InferRequestStats_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_nvidia_inferenceserver_InferRequestStats_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferRequestStats_descriptor,
        new java.lang.String[] { "Success", "Failed", "Compute", "Queue", });
    internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_nvidia_inferenceserver_ModelVersionStatus_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor,
        new java.lang.String[] { "ReadyState", "InferStats", "ModelExecutionCount", "ModelInferenceCount", });
    internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionStatus_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionStatus_InferStatsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelStatus_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_nvidia_inferenceserver_ModelStatus_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelStatus_descriptor,
        new java.lang.String[] { "Config", "VersionStatus", });
    internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelStatus_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelStatus_VersionStatusEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ServerStatus_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_nvidia_inferenceserver_ServerStatus_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ServerStatus_descriptor,
        new java.lang.String[] { "Id", "Version", "ReadyState", "UptimeNs", "ModelStatus", "StatusStats", "ProfileStats", "HealthStats", "ModelControlStats", "ShmControlStats", });
    internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_descriptor =
      internal_static_nvidia_inferenceserver_ServerStatus_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ServerStatus_ModelStatusEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    nvidia.inferenceserver.ModelConfigOuterClass.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
