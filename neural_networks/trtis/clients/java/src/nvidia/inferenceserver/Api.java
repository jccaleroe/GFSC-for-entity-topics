// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: api.proto

package nvidia.inferenceserver;

public final class Api {
  private Api() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  public interface InferSharedMemoryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferSharedMemory)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name given during registration of a shared memory region that
     *&#64;&#64;     holds the input data (or where the output data should be written).
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name given during registration of a shared memory region that
     *&#64;&#64;     holds the input data (or where the output data should be written).
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 offset
     *&#64;&#64;
     *&#64;&#64;     The offset from the start of the shared memory region.
     *&#64;&#64;     start = offset, end = offset + size;
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 offset = 2;</code>
     */
    long getOffset();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 byte_size
     *&#64;&#64;
     *&#64;&#64;     Size of the memory block, in bytes.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 byte_size = 3;</code>
     */
    long getByteSize();
  }
  /**
   * <pre>
   *&#64;&#64;.. cpp:var:: message InferSharedMemory
   *&#64;&#64;
   *&#64;&#64;   The meta-data for the shared memory from which to read the input
   *&#64;&#64;   data and/or write the output data.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.InferSharedMemory}
   */
  public  static final class InferSharedMemory extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferSharedMemory)
      InferSharedMemoryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InferSharedMemory.newBuilder() to construct.
    private InferSharedMemory(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InferSharedMemory() {
      name_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InferSharedMemory();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InferSharedMemory(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              offset_ = input.readUInt64();
              break;
            }
            case 24: {

              byteSize_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferSharedMemory_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.Api.InferSharedMemory.class, nvidia.inferenceserver.Api.InferSharedMemory.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name given during registration of a shared memory region that
     *&#64;&#64;     holds the input data (or where the output data should be written).
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name given during registration of a shared memory region that
     *&#64;&#64;     holds the input data (or where the output data should be written).
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int OFFSET_FIELD_NUMBER = 2;
    private long offset_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 offset
     *&#64;&#64;
     *&#64;&#64;     The offset from the start of the shared memory region.
     *&#64;&#64;     start = offset, end = offset + size;
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 offset = 2;</code>
     */
    public long getOffset() {
      return offset_;
    }

    public static final int BYTE_SIZE_FIELD_NUMBER = 3;
    private long byteSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 byte_size
     *&#64;&#64;
     *&#64;&#64;     Size of the memory block, in bytes.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 byte_size = 3;</code>
     */
    public long getByteSize() {
      return byteSize_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (offset_ != 0L) {
        output.writeUInt64(2, offset_);
      }
      if (byteSize_ != 0L) {
        output.writeUInt64(3, byteSize_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (offset_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, offset_);
      }
      if (byteSize_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, byteSize_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.Api.InferSharedMemory)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.Api.InferSharedMemory other = (nvidia.inferenceserver.Api.InferSharedMemory) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (getOffset()
          != other.getOffset()) return false;
      if (getByteSize()
          != other.getByteSize()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + OFFSET_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getOffset());
      hash = (37 * hash) + BYTE_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getByteSize());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferSharedMemory parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.Api.InferSharedMemory prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;.. cpp:var:: message InferSharedMemory
     *&#64;&#64;
     *&#64;&#64;   The meta-data for the shared memory from which to read the input
     *&#64;&#64;   data and/or write the output data.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferSharedMemory}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferSharedMemory)
        nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferSharedMemory_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferSharedMemory.class, nvidia.inferenceserver.Api.InferSharedMemory.Builder.class);
      }

      // Construct using nvidia.inferenceserver.Api.InferSharedMemory.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        offset_ = 0L;

        byteSize_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferSharedMemory getDefaultInstanceForType() {
        return nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferSharedMemory build() {
        nvidia.inferenceserver.Api.InferSharedMemory result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferSharedMemory buildPartial() {
        nvidia.inferenceserver.Api.InferSharedMemory result = new nvidia.inferenceserver.Api.InferSharedMemory(this);
        result.name_ = name_;
        result.offset_ = offset_;
        result.byteSize_ = byteSize_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.Api.InferSharedMemory) {
          return mergeFrom((nvidia.inferenceserver.Api.InferSharedMemory)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.Api.InferSharedMemory other) {
        if (other == nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.getOffset() != 0L) {
          setOffset(other.getOffset());
        }
        if (other.getByteSize() != 0L) {
          setByteSize(other.getByteSize());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.Api.InferSharedMemory parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.Api.InferSharedMemory) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name given during registration of a shared memory region that
       *&#64;&#64;     holds the input data (or where the output data should be written).
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name given during registration of a shared memory region that
       *&#64;&#64;     holds the input data (or where the output data should be written).
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name given during registration of a shared memory region that
       *&#64;&#64;     holds the input data (or where the output data should be written).
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name given during registration of a shared memory region that
       *&#64;&#64;     holds the input data (or where the output data should be written).
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name given during registration of a shared memory region that
       *&#64;&#64;     holds the input data (or where the output data should be written).
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private long offset_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 offset
       *&#64;&#64;
       *&#64;&#64;     The offset from the start of the shared memory region.
       *&#64;&#64;     start = offset, end = offset + size;
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 offset = 2;</code>
       */
      public long getOffset() {
        return offset_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 offset
       *&#64;&#64;
       *&#64;&#64;     The offset from the start of the shared memory region.
       *&#64;&#64;     start = offset, end = offset + size;
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 offset = 2;</code>
       */
      public Builder setOffset(long value) {
        
        offset_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 offset
       *&#64;&#64;
       *&#64;&#64;     The offset from the start of the shared memory region.
       *&#64;&#64;     start = offset, end = offset + size;
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 offset = 2;</code>
       */
      public Builder clearOffset() {
        
        offset_ = 0L;
        onChanged();
        return this;
      }

      private long byteSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 byte_size
       *&#64;&#64;
       *&#64;&#64;     Size of the memory block, in bytes.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 byte_size = 3;</code>
       */
      public long getByteSize() {
        return byteSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 byte_size
       *&#64;&#64;
       *&#64;&#64;     Size of the memory block, in bytes.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 byte_size = 3;</code>
       */
      public Builder setByteSize(long value) {
        
        byteSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 byte_size
       *&#64;&#64;
       *&#64;&#64;     Size of the memory block, in bytes.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 byte_size = 3;</code>
       */
      public Builder clearByteSize() {
        
        byteSize_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferSharedMemory)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferSharedMemory)
    private static final nvidia.inferenceserver.Api.InferSharedMemory DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferSharedMemory();
    }

    public static nvidia.inferenceserver.Api.InferSharedMemory getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InferSharedMemory>
        PARSER = new com.google.protobuf.AbstractParser<InferSharedMemory>() {
      @java.lang.Override
      public InferSharedMemory parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InferSharedMemory(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InferSharedMemory> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InferSharedMemory> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.Api.InferSharedMemory getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InferRequestHeaderOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferRequestHeader)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 id
     *&#64;&#64;
     *&#64;&#64;     The ID of the inference request. The response of the request will
     *&#64;&#64;     have the same ID in InferResponseHeader. The request sender can use
     *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 id = 5;</code>
     */
    long getId();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 flags
     *&#64;&#64;
     *&#64;&#64;     The flags associated with this request. This field holds a bitwise-or
     *&#64;&#64;     of all flag values.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 flags = 6;</code>
     */
    int getFlags();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 correlation_id
     *&#64;&#64;
     *&#64;&#64;     The correlation ID of the inference request. Default is 0, which
     *&#64;&#64;     indictes that the request has no correlation ID. The correlation ID
     *&#64;&#64;     is used to indicate two or more inference request are related to
     *&#64;&#64;     each other. How this relationship is handled by the inference
     *&#64;&#64;     server is determined by the model's scheduling policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 correlation_id = 4;</code>
     */
    long getCorrelationId();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 1;</code>
     */
    int getBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input> 
        getInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    nvidia.inferenceserver.Api.InferRequestHeader.Input getInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    int getInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder> 
        getInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder getInputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    nvidia.inferenceserver.Api.InferRequestHeader.Output getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    int getOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder> 
        getOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder getOutputOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message InferRequestHeader
   *&#64;&#64;
   *&#64;&#64;   Meta-data for an inferencing request. The actual input data is
   *&#64;&#64;   delivered separate from this header, in the HTTP body for an HTTP
   *&#64;&#64;   request, or in the :cpp:var:`InferRequest` message for a gRPC request.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader}
   */
  public  static final class InferRequestHeader extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferRequestHeader)
      InferRequestHeaderOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InferRequestHeader.newBuilder() to construct.
    private InferRequestHeader(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InferRequestHeader() {
      input_ = java.util.Collections.emptyList();
      output_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InferRequestHeader();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InferRequestHeader(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              batchSize_ = input.readUInt32();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                input_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferRequestHeader.Input>();
                mutable_bitField0_ |= 0x00000001;
              }
              input_.add(
                  input.readMessage(nvidia.inferenceserver.Api.InferRequestHeader.Input.parser(), extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                output_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferRequestHeader.Output>();
                mutable_bitField0_ |= 0x00000002;
              }
              output_.add(
                  input.readMessage(nvidia.inferenceserver.Api.InferRequestHeader.Output.parser(), extensionRegistry));
              break;
            }
            case 32: {

              correlationId_ = input.readUInt64();
              break;
            }
            case 40: {

              id_ = input.readUInt64();
              break;
            }
            case 48: {

              flags_ = input.readUInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          input_ = java.util.Collections.unmodifiableList(input_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          output_ = java.util.Collections.unmodifiableList(output_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.Api.InferRequestHeader.class, nvidia.inferenceserver.Api.InferRequestHeader.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enum:: Flag
     *&#64;&#64;
     *&#64;&#64;     Flags that can be associated with an inference request.
     *&#64;&#64;     All flags are packed bitwise into the 'flags' field and
     *&#64;&#64;     so the value of each must be a power-of-2.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.InferRequestHeader.Flag}
     */
    public enum Flag
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       Value indicating no flags are enabled.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_NONE = 0;</code>
       */
      FLAG_NONE(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_SEQUENCE_START = 1 &lt;&lt; 0
       *&#64;&#64;
       *&#64;&#64;       This request is the start of a related sequence of requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_SEQUENCE_START = 1;</code>
       */
      FLAG_SEQUENCE_START(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_SEQUENCE_END = 1 &lt;&lt; 1
       *&#64;&#64;
       *&#64;&#64;       This request is the end of a related sequence of requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_SEQUENCE_END = 2;</code>
       */
      FLAG_SEQUENCE_END(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       Value indicating no flags are enabled.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_NONE = 0;</code>
       */
      public static final int FLAG_NONE_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_SEQUENCE_START = 1 &lt;&lt; 0
       *&#64;&#64;
       *&#64;&#64;       This request is the start of a related sequence of requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_SEQUENCE_START = 1;</code>
       */
      public static final int FLAG_SEQUENCE_START_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Flag::FLAG_SEQUENCE_END = 1 &lt;&lt; 1
       *&#64;&#64;
       *&#64;&#64;       This request is the end of a related sequence of requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>FLAG_SEQUENCE_END = 2;</code>
       */
      public static final int FLAG_SEQUENCE_END_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Flag valueOf(int value) {
        return forNumber(value);
      }

      public static Flag forNumber(int value) {
        switch (value) {
          case 0: return FLAG_NONE;
          case 1: return FLAG_SEQUENCE_START;
          case 2: return FLAG_SEQUENCE_END;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Flag>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Flag> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Flag>() {
              public Flag findValueByNumber(int number) {
                return Flag.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.InferRequestHeader.getDescriptor().getEnumTypes().get(0);
      }

      private static final Flag[] VALUES = values();

      public static Flag valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Flag(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.InferRequestHeader.Flag)
    }

    public interface InputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferRequestHeader.Input)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the input tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the input tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      java.util.List<java.lang.Long> getDimsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      int getDimsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      long getDims(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 batch_byte_size
       *&#64;&#64;
       *&#64;&#64;       The size of the full batch of the input tensor, in bytes.
       *&#64;&#64;       Optional for tensors with fixed-sized datatypes. Required
       *&#64;&#64;       for tensors with a non-fixed-size datatype (like STRING).
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 batch_byte_size = 3;</code>
       */
      long getBatchByteSize();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      boolean hasSharedMemory();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Input
     *&#64;&#64;
     *&#64;&#64;     Meta-data for an input tensor provided as part of an inferencing
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Input}
     */
    public  static final class Input extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferRequestHeader.Input)
        InputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Input.newBuilder() to construct.
      private Input(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Input() {
        name_ = "";
        dims_ = emptyLongList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Input();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Input(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  dims_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                dims_.addLong(input.readInt64());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  dims_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  dims_.addLong(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {

                batchByteSize_ = input.readUInt64();
                break;
              }
              case 34: {
                nvidia.inferenceserver.Api.InferSharedMemory.Builder subBuilder = null;
                if (sharedMemory_ != null) {
                  subBuilder = sharedMemory_.toBuilder();
                }
                sharedMemory_ = input.readMessage(nvidia.inferenceserver.Api.InferSharedMemory.parser(), extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(sharedMemory_);
                  sharedMemory_ = subBuilder.buildPartial();
                }

                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            dims_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Input_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferRequestHeader.Input.class, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the input tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the input tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int DIMS_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.LongList dims_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return dims_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;       Optional if the model configuration for this input explicitly
       *&#64;&#64;       specifies all dimensions of the shape. Required if the model
       *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      private int dimsMemoizedSerializedSize = -1;

      public static final int BATCH_BYTE_SIZE_FIELD_NUMBER = 3;
      private long batchByteSize_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 batch_byte_size
       *&#64;&#64;
       *&#64;&#64;       The size of the full batch of the input tensor, in bytes.
       *&#64;&#64;       Optional for tensors with fixed-sized datatypes. Required
       *&#64;&#64;       for tensors with a non-fixed-size datatype (like STRING).
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 batch_byte_size = 3;</code>
       */
      public long getBatchByteSize() {
        return batchByteSize_;
      }

      public static final int SHARED_MEMORY_FIELD_NUMBER = 4;
      private nvidia.inferenceserver.Api.InferSharedMemory sharedMemory_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public boolean hasSharedMemory() {
        return sharedMemory_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory() {
        return sharedMemory_ == null ? nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that contains the tensor data
       *&#64;&#64;       for this input. Using shared memory is optional but if this
       *&#64;&#64;       message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder() {
        return getSharedMemory();
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (getDimsList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(dimsMemoizedSerializedSize);
        }
        for (int i = 0; i < dims_.size(); i++) {
          output.writeInt64NoTag(dims_.getLong(i));
        }
        if (batchByteSize_ != 0L) {
          output.writeUInt64(3, batchByteSize_);
        }
        if (sharedMemory_ != null) {
          output.writeMessage(4, getSharedMemory());
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < dims_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(dims_.getLong(i));
          }
          size += dataSize;
          if (!getDimsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          dimsMemoizedSerializedSize = dataSize;
        }
        if (batchByteSize_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, batchByteSize_);
        }
        if (sharedMemory_ != null) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, getSharedMemory());
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.Api.InferRequestHeader.Input)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.Api.InferRequestHeader.Input other = (nvidia.inferenceserver.Api.InferRequestHeader.Input) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (!getDimsList()
            .equals(other.getDimsList())) return false;
        if (getBatchByteSize()
            != other.getBatchByteSize()) return false;
        if (hasSharedMemory() != other.hasSharedMemory()) return false;
        if (hasSharedMemory()) {
          if (!getSharedMemory()
              .equals(other.getSharedMemory())) return false;
        }
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (getDimsCount() > 0) {
          hash = (37 * hash) + DIMS_FIELD_NUMBER;
          hash = (53 * hash) + getDimsList().hashCode();
        }
        hash = (37 * hash) + BATCH_BYTE_SIZE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getBatchByteSize());
        if (hasSharedMemory()) {
          hash = (37 * hash) + SHARED_MEMORY_FIELD_NUMBER;
          hash = (53 * hash) + getSharedMemory().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Input parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.Api.InferRequestHeader.Input prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Input
       *&#64;&#64;
       *&#64;&#64;     Meta-data for an input tensor provided as part of an inferencing
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Input}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferRequestHeader.Input)
          nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Input_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferRequestHeader.Input.class, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder.class);
        }

        // Construct using nvidia.inferenceserver.Api.InferRequestHeader.Input.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          dims_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          batchByteSize_ = 0L;

          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = null;
          } else {
            sharedMemory_ = null;
            sharedMemoryBuilder_ = null;
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Input getDefaultInstanceForType() {
          return nvidia.inferenceserver.Api.InferRequestHeader.Input.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Input build() {
          nvidia.inferenceserver.Api.InferRequestHeader.Input result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Input buildPartial() {
          nvidia.inferenceserver.Api.InferRequestHeader.Input result = new nvidia.inferenceserver.Api.InferRequestHeader.Input(this);
          int from_bitField0_ = bitField0_;
          result.name_ = name_;
          if (((bitField0_ & 0x00000001) != 0)) {
            dims_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.dims_ = dims_;
          result.batchByteSize_ = batchByteSize_;
          if (sharedMemoryBuilder_ == null) {
            result.sharedMemory_ = sharedMemory_;
          } else {
            result.sharedMemory_ = sharedMemoryBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.Api.InferRequestHeader.Input) {
            return mergeFrom((nvidia.inferenceserver.Api.InferRequestHeader.Input)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.Api.InferRequestHeader.Input other) {
          if (other == nvidia.inferenceserver.Api.InferRequestHeader.Input.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (!other.dims_.isEmpty()) {
            if (dims_.isEmpty()) {
              dims_ = other.dims_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureDimsIsMutable();
              dims_.addAll(other.dims_);
            }
            onChanged();
          }
          if (other.getBatchByteSize() != 0L) {
            setBatchByteSize(other.getBatchByteSize());
          }
          if (other.hasSharedMemory()) {
            mergeSharedMemory(other.getSharedMemory());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.Api.InferRequestHeader.Input parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.Api.InferRequestHeader.Input) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the input tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the input tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the input tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the input tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the input tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
        private void ensureDimsIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            dims_ = mutableCopy(dims_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public java.util.List<java.lang.Long>
            getDimsList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(dims_) : dims_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public int getDimsCount() {
          return dims_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public long getDims(int index) {
          return dims_.getLong(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder setDims(
            int index, long value) {
          ensureDimsIsMutable();
          dims_.setLong(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder addDims(long value) {
          ensureDimsIsMutable();
          dims_.addLong(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder addAllDims(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureDimsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, dims_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;       Optional if the model configuration for this input explicitly
         *&#64;&#64;       specifies all dimensions of the shape. Required if the model
         *&#64;&#64;       configuration for this input has any wildcard dimensions (-1).
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder clearDims() {
          dims_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private long batchByteSize_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 batch_byte_size
         *&#64;&#64;
         *&#64;&#64;       The size of the full batch of the input tensor, in bytes.
         *&#64;&#64;       Optional for tensors with fixed-sized datatypes. Required
         *&#64;&#64;       for tensors with a non-fixed-size datatype (like STRING).
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 batch_byte_size = 3;</code>
         */
        public long getBatchByteSize() {
          return batchByteSize_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 batch_byte_size
         *&#64;&#64;
         *&#64;&#64;       The size of the full batch of the input tensor, in bytes.
         *&#64;&#64;       Optional for tensors with fixed-sized datatypes. Required
         *&#64;&#64;       for tensors with a non-fixed-size datatype (like STRING).
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 batch_byte_size = 3;</code>
         */
        public Builder setBatchByteSize(long value) {
          
          batchByteSize_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 batch_byte_size
         *&#64;&#64;
         *&#64;&#64;       The size of the full batch of the input tensor, in bytes.
         *&#64;&#64;       Optional for tensors with fixed-sized datatypes. Required
         *&#64;&#64;       for tensors with a non-fixed-size datatype (like STRING).
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 batch_byte_size = 3;</code>
         */
        public Builder clearBatchByteSize() {
          
          batchByteSize_ = 0L;
          onChanged();
          return this;
        }

        private nvidia.inferenceserver.Api.InferSharedMemory sharedMemory_;
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder> sharedMemoryBuilder_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public boolean hasSharedMemory() {
          return sharedMemoryBuilder_ != null || sharedMemory_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory() {
          if (sharedMemoryBuilder_ == null) {
            return sharedMemory_ == null ? nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
          } else {
            return sharedMemoryBuilder_.getMessage();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder setSharedMemory(nvidia.inferenceserver.Api.InferSharedMemory value) {
          if (sharedMemoryBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            sharedMemory_ = value;
            onChanged();
          } else {
            sharedMemoryBuilder_.setMessage(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder setSharedMemory(
            nvidia.inferenceserver.Api.InferSharedMemory.Builder builderForValue) {
          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = builderForValue.build();
            onChanged();
          } else {
            sharedMemoryBuilder_.setMessage(builderForValue.build());
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder mergeSharedMemory(nvidia.inferenceserver.Api.InferSharedMemory value) {
          if (sharedMemoryBuilder_ == null) {
            if (sharedMemory_ != null) {
              sharedMemory_ =
                nvidia.inferenceserver.Api.InferSharedMemory.newBuilder(sharedMemory_).mergeFrom(value).buildPartial();
            } else {
              sharedMemory_ = value;
            }
            onChanged();
          } else {
            sharedMemoryBuilder_.mergeFrom(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder clearSharedMemory() {
          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = null;
            onChanged();
          } else {
            sharedMemory_ = null;
            sharedMemoryBuilder_ = null;
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemory.Builder getSharedMemoryBuilder() {
          
          onChanged();
          return getSharedMemoryFieldBuilder().getBuilder();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder() {
          if (sharedMemoryBuilder_ != null) {
            return sharedMemoryBuilder_.getMessageOrBuilder();
          } else {
            return sharedMemory_ == null ?
                nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that contains the tensor data
         *&#64;&#64;       for this input. Using shared memory is optional but if this
         *&#64;&#64;       message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder> 
            getSharedMemoryFieldBuilder() {
          if (sharedMemoryBuilder_ == null) {
            sharedMemoryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
                nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder>(
                    getSharedMemory(),
                    getParentForChildren(),
                    isClean());
            sharedMemory_ = null;
          }
          return sharedMemoryBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferRequestHeader.Input)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestHeader.Input)
      private static final nvidia.inferenceserver.Api.InferRequestHeader.Input DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferRequestHeader.Input();
      }

      public static nvidia.inferenceserver.Api.InferRequestHeader.Input getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Input>
          PARSER = new com.google.protobuf.AbstractParser<Input>() {
        @java.lang.Override
        public Input parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Input(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Input> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Input> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferRequestHeader.Input getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface OutputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferRequestHeader.Output)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      boolean hasCls();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getCls();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder getClsOrBuilder();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      boolean hasSharedMemory();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Output
     *&#64;&#64;
     *&#64;&#64;     Meta-data for a requested output tensor as part of an inferencing
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Output}
     */
    public  static final class Output extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferRequestHeader.Output)
        OutputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Output.newBuilder() to construct.
      private Output(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Output() {
        name_ = "";
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Output();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Output(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 26: {
                nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder subBuilder = null;
                if (cls_ != null) {
                  subBuilder = cls_.toBuilder();
                }
                cls_ = input.readMessage(nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.parser(), extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(cls_);
                  cls_ = subBuilder.buildPartial();
                }

                break;
              }
              case 34: {
                nvidia.inferenceserver.Api.InferSharedMemory.Builder subBuilder = null;
                if (sharedMemory_ != null) {
                  subBuilder = sharedMemory_.toBuilder();
                }
                sharedMemory_ = input.readMessage(nvidia.inferenceserver.Api.InferSharedMemory.parser(), extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(sharedMemory_);
                  sharedMemory_ = subBuilder.buildPartial();
                }

                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferRequestHeader.Output.class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder.class);
      }

      public interface ClassOrBuilder extends
          // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferRequestHeader.Output.Class)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;         Indicates how many classification values should be returned
         *&#64;&#64;         for the output. The 'count' highest priority values are
         *&#64;&#64;         returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 1;</code>
         */
        int getCount();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message Class
       *&#64;&#64;
       *&#64;&#64;       Options for an output returned as a classification.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Output.Class}
       */
      public  static final class Class extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferRequestHeader.Output.Class)
          ClassOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Class.newBuilder() to construct.
        private Class(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Class() {
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new Class();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Class(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 8: {

                  count_ = input.readUInt32();
                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder.class);
        }

        public static final int COUNT_FIELD_NUMBER = 1;
        private int count_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;         Indicates how many classification values should be returned
         *&#64;&#64;         for the output. The 'count' highest priority values are
         *&#64;&#64;         returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 1;</code>
         */
        public int getCount() {
          return count_;
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          if (count_ != 0) {
            output.writeUInt32(1, count_);
          }
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          if (count_ != 0) {
            size += com.google.protobuf.CodedOutputStream
              .computeUInt32Size(1, count_);
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof nvidia.inferenceserver.Api.InferRequestHeader.Output.Class)) {
            return super.equals(obj);
          }
          nvidia.inferenceserver.Api.InferRequestHeader.Output.Class other = (nvidia.inferenceserver.Api.InferRequestHeader.Output.Class) obj;

          if (getCount()
              != other.getCount()) return false;
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          hash = (37 * hash) + COUNT_FIELD_NUMBER;
          hash = (53 * hash) + getCount();
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(nvidia.inferenceserver.Api.InferRequestHeader.Output.Class prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message Class
         *&#64;&#64;
         *&#64;&#64;       Options for an output returned as a classification.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Output.Class}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferRequestHeader.Output.Class)
            nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor;
          }

          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder.class);
          }

          // Construct using nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            count_ = 0;

            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getDefaultInstanceForType() {
            return nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.getDefaultInstance();
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class build() {
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class buildPartial() {
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class result = new nvidia.inferenceserver.Api.InferRequestHeader.Output.Class(this);
            result.count_ = count_;
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof nvidia.inferenceserver.Api.InferRequestHeader.Output.Class) {
              return mergeFrom((nvidia.inferenceserver.Api.InferRequestHeader.Output.Class)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(nvidia.inferenceserver.Api.InferRequestHeader.Output.Class other) {
            if (other == nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.getDefaultInstance()) return this;
            if (other.getCount() != 0) {
              setCount(other.getCount());
            }
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (nvidia.inferenceserver.Api.InferRequestHeader.Output.Class) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }

          private int count_ ;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint32 count
           *&#64;&#64;
           *&#64;&#64;         Indicates how many classification values should be returned
           *&#64;&#64;         for the output. The 'count' highest priority values are
           *&#64;&#64;         returned.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint32 count = 1;</code>
           */
          public int getCount() {
            return count_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint32 count
           *&#64;&#64;
           *&#64;&#64;         Indicates how many classification values should be returned
           *&#64;&#64;         for the output. The 'count' highest priority values are
           *&#64;&#64;         returned.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint32 count = 1;</code>
           */
          public Builder setCount(int value) {
            
            count_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint32 count
           *&#64;&#64;
           *&#64;&#64;         Indicates how many classification values should be returned
           *&#64;&#64;         for the output. The 'count' highest priority values are
           *&#64;&#64;         returned.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint32 count = 1;</code>
           */
          public Builder clearCount() {
            
            count_ = 0;
            onChanged();
            return this;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferRequestHeader.Output.Class)
        }

        // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestHeader.Output.Class)
        private static final nvidia.inferenceserver.Api.InferRequestHeader.Output.Class DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferRequestHeader.Output.Class();
        }

        public static nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Class>
            PARSER = new com.google.protobuf.AbstractParser<Class>() {
          @java.lang.Override
          public Class parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Class(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Class> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Class> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int CLS_FIELD_NUMBER = 3;
      private nvidia.inferenceserver.Api.InferRequestHeader.Output.Class cls_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      public boolean hasCls() {
        return cls_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getCls() {
        return cls_ == null ? nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.getDefaultInstance() : cls_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Class cls
       *&#64;&#64;
       *&#64;&#64;       Optional. If defined return this output as a classification
       *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
       *&#64;&#64;       probabilities and the classifications associated with the
       *&#64;&#64;       highest probabilities will be returned.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder getClsOrBuilder() {
        return getCls();
      }

      public static final int SHARED_MEMORY_FIELD_NUMBER = 4;
      private nvidia.inferenceserver.Api.InferSharedMemory sharedMemory_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public boolean hasSharedMemory() {
        return sharedMemory_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory() {
        return sharedMemory_ == null ? nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
       *&#64;&#64;
       *&#64;&#64;       It is the location in shared memory that the result tensor data
       *&#64;&#64;       for this output will be written. Using shared memory is optional
       *&#64;&#64;       but if this message is used, all fields are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder() {
        return getSharedMemory();
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (cls_ != null) {
          output.writeMessage(3, getCls());
        }
        if (sharedMemory_ != null) {
          output.writeMessage(4, getSharedMemory());
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        if (cls_ != null) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, getCls());
        }
        if (sharedMemory_ != null) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, getSharedMemory());
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.Api.InferRequestHeader.Output)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.Api.InferRequestHeader.Output other = (nvidia.inferenceserver.Api.InferRequestHeader.Output) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (hasCls() != other.hasCls()) return false;
        if (hasCls()) {
          if (!getCls()
              .equals(other.getCls())) return false;
        }
        if (hasSharedMemory() != other.hasSharedMemory()) return false;
        if (hasSharedMemory()) {
          if (!getSharedMemory()
              .equals(other.getSharedMemory())) return false;
        }
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (hasCls()) {
          hash = (37 * hash) + CLS_FIELD_NUMBER;
          hash = (53 * hash) + getCls().hashCode();
        }
        if (hasSharedMemory()) {
          hash = (37 * hash) + SHARED_MEMORY_FIELD_NUMBER;
          hash = (53 * hash) + getSharedMemory().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferRequestHeader.Output parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.Api.InferRequestHeader.Output prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Output
       *&#64;&#64;
       *&#64;&#64;     Meta-data for a requested output tensor as part of an inferencing
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader.Output}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferRequestHeader.Output)
          nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferRequestHeader.Output.class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder.class);
        }

        // Construct using nvidia.inferenceserver.Api.InferRequestHeader.Output.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          if (clsBuilder_ == null) {
            cls_ = null;
          } else {
            cls_ = null;
            clsBuilder_ = null;
          }
          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = null;
          } else {
            sharedMemory_ = null;
            sharedMemoryBuilder_ = null;
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Output getDefaultInstanceForType() {
          return nvidia.inferenceserver.Api.InferRequestHeader.Output.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Output build() {
          nvidia.inferenceserver.Api.InferRequestHeader.Output result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferRequestHeader.Output buildPartial() {
          nvidia.inferenceserver.Api.InferRequestHeader.Output result = new nvidia.inferenceserver.Api.InferRequestHeader.Output(this);
          result.name_ = name_;
          if (clsBuilder_ == null) {
            result.cls_ = cls_;
          } else {
            result.cls_ = clsBuilder_.build();
          }
          if (sharedMemoryBuilder_ == null) {
            result.sharedMemory_ = sharedMemory_;
          } else {
            result.sharedMemory_ = sharedMemoryBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.Api.InferRequestHeader.Output) {
            return mergeFrom((nvidia.inferenceserver.Api.InferRequestHeader.Output)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.Api.InferRequestHeader.Output other) {
          if (other == nvidia.inferenceserver.Api.InferRequestHeader.Output.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (other.hasCls()) {
            mergeCls(other.getCls());
          }
          if (other.hasSharedMemory()) {
            mergeSharedMemory(other.getSharedMemory());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.Api.InferRequestHeader.Output parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.Api.InferRequestHeader.Output) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private nvidia.inferenceserver.Api.InferRequestHeader.Output.Class cls_;
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder> clsBuilder_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public boolean hasCls() {
          return clsBuilder_ != null || cls_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class getCls() {
          if (clsBuilder_ == null) {
            return cls_ == null ? nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.getDefaultInstance() : cls_;
          } else {
            return clsBuilder_.getMessage();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public Builder setCls(nvidia.inferenceserver.Api.InferRequestHeader.Output.Class value) {
          if (clsBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            cls_ = value;
            onChanged();
          } else {
            clsBuilder_.setMessage(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public Builder setCls(
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder builderForValue) {
          if (clsBuilder_ == null) {
            cls_ = builderForValue.build();
            onChanged();
          } else {
            clsBuilder_.setMessage(builderForValue.build());
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public Builder mergeCls(nvidia.inferenceserver.Api.InferRequestHeader.Output.Class value) {
          if (clsBuilder_ == null) {
            if (cls_ != null) {
              cls_ =
                nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.newBuilder(cls_).mergeFrom(value).buildPartial();
            } else {
              cls_ = value;
            }
            onChanged();
          } else {
            clsBuilder_.mergeFrom(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public Builder clearCls() {
          if (clsBuilder_ == null) {
            cls_ = null;
            onChanged();
          } else {
            cls_ = null;
            clsBuilder_ = null;
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder getClsBuilder() {
          
          onChanged();
          return getClsFieldBuilder().getBuilder();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder getClsOrBuilder() {
          if (clsBuilder_ != null) {
            return clsBuilder_.getMessageOrBuilder();
          } else {
            return cls_ == null ?
                nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.getDefaultInstance() : cls_;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Class cls
         *&#64;&#64;
         *&#64;&#64;       Optional. If defined return this output as a classification
         *&#64;&#64;       instead of raw data. The output tensor will be interpreted as
         *&#64;&#64;       probabilities and the classifications associated with the
         *&#64;&#64;       highest probabilities will be returned.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferRequestHeader.Output.Class cls = 3;</code>
         */
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferRequestHeader.Output.Class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder> 
            getClsFieldBuilder() {
          if (clsBuilder_ == null) {
            clsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
                nvidia.inferenceserver.Api.InferRequestHeader.Output.Class, nvidia.inferenceserver.Api.InferRequestHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferRequestHeader.Output.ClassOrBuilder>(
                    getCls(),
                    getParentForChildren(),
                    isClean());
            cls_ = null;
          }
          return clsBuilder_;
        }

        private nvidia.inferenceserver.Api.InferSharedMemory sharedMemory_;
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder> sharedMemoryBuilder_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public boolean hasSharedMemory() {
          return sharedMemoryBuilder_ != null || sharedMemory_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemory getSharedMemory() {
          if (sharedMemoryBuilder_ == null) {
            return sharedMemory_ == null ? nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
          } else {
            return sharedMemoryBuilder_.getMessage();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder setSharedMemory(nvidia.inferenceserver.Api.InferSharedMemory value) {
          if (sharedMemoryBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            sharedMemory_ = value;
            onChanged();
          } else {
            sharedMemoryBuilder_.setMessage(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder setSharedMemory(
            nvidia.inferenceserver.Api.InferSharedMemory.Builder builderForValue) {
          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = builderForValue.build();
            onChanged();
          } else {
            sharedMemoryBuilder_.setMessage(builderForValue.build());
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder mergeSharedMemory(nvidia.inferenceserver.Api.InferSharedMemory value) {
          if (sharedMemoryBuilder_ == null) {
            if (sharedMemory_ != null) {
              sharedMemory_ =
                nvidia.inferenceserver.Api.InferSharedMemory.newBuilder(sharedMemory_).mergeFrom(value).buildPartial();
            } else {
              sharedMemory_ = value;
            }
            onChanged();
          } else {
            sharedMemoryBuilder_.mergeFrom(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public Builder clearSharedMemory() {
          if (sharedMemoryBuilder_ == null) {
            sharedMemory_ = null;
            onChanged();
          } else {
            sharedMemory_ = null;
            sharedMemoryBuilder_ = null;
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemory.Builder getSharedMemoryBuilder() {
          
          onChanged();
          return getSharedMemoryFieldBuilder().getBuilder();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        public nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder getSharedMemoryOrBuilder() {
          if (sharedMemoryBuilder_ != null) {
            return sharedMemoryBuilder_.getMessageOrBuilder();
          } else {
            return sharedMemory_ == null ?
                nvidia.inferenceserver.Api.InferSharedMemory.getDefaultInstance() : sharedMemory_;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: InferSharedMemory shared_memory
         *&#64;&#64;
         *&#64;&#64;       It is the location in shared memory that the result tensor data
         *&#64;&#64;       for this output will be written. Using shared memory is optional
         *&#64;&#64;       but if this message is used, all fields are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferSharedMemory shared_memory = 4;</code>
         */
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder> 
            getSharedMemoryFieldBuilder() {
          if (sharedMemoryBuilder_ == null) {
            sharedMemoryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
                nvidia.inferenceserver.Api.InferSharedMemory, nvidia.inferenceserver.Api.InferSharedMemory.Builder, nvidia.inferenceserver.Api.InferSharedMemoryOrBuilder>(
                    getSharedMemory(),
                    getParentForChildren(),
                    isClean());
            sharedMemory_ = null;
          }
          return sharedMemoryBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferRequestHeader.Output)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestHeader.Output)
      private static final nvidia.inferenceserver.Api.InferRequestHeader.Output DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferRequestHeader.Output();
      }

      public static nvidia.inferenceserver.Api.InferRequestHeader.Output getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Output>
          PARSER = new com.google.protobuf.AbstractParser<Output>() {
        @java.lang.Override
        public Output parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Output(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Output> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Output> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferRequestHeader.Output getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int ID_FIELD_NUMBER = 5;
    private long id_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 id
     *&#64;&#64;
     *&#64;&#64;     The ID of the inference request. The response of the request will
     *&#64;&#64;     have the same ID in InferResponseHeader. The request sender can use
     *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 id = 5;</code>
     */
    public long getId() {
      return id_;
    }

    public static final int FLAGS_FIELD_NUMBER = 6;
    private int flags_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 flags
     *&#64;&#64;
     *&#64;&#64;     The flags associated with this request. This field holds a bitwise-or
     *&#64;&#64;     of all flag values.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 flags = 6;</code>
     */
    public int getFlags() {
      return flags_;
    }

    public static final int CORRELATION_ID_FIELD_NUMBER = 4;
    private long correlationId_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 correlation_id
     *&#64;&#64;
     *&#64;&#64;     The correlation ID of the inference request. Default is 0, which
     *&#64;&#64;     indictes that the request has no correlation ID. The correlation ID
     *&#64;&#64;     is used to indicate two or more inference request are related to
     *&#64;&#64;     each other. How this relationship is handled by the inference
     *&#64;&#64;     server is determined by the model's scheduling policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 correlation_id = 4;</code>
     */
    public long getCorrelationId() {
      return correlationId_;
    }

    public static final int BATCH_SIZE_FIELD_NUMBER = 1;
    private int batchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 1;</code>
     */
    public int getBatchSize() {
      return batchSize_;
    }

    public static final int INPUT_FIELD_NUMBER = 2;
    private java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input> input_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input> getInputList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder> 
        getInputOrBuilderList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    public int getInputCount() {
      return input_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    public nvidia.inferenceserver.Api.InferRequestHeader.Input getInput(int index) {
      return input_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Input input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The input meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
     */
    public nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder getInputOrBuilder(
        int index) {
      return input_.get(index);
    }

    public static final int OUTPUT_FIELD_NUMBER = 3;
    private java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    public nvidia.inferenceserver.Api.InferRequestHeader.Output getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The output meta-data for the inputs provided with the the inference
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
     */
    public nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (batchSize_ != 0) {
        output.writeUInt32(1, batchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        output.writeMessage(2, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        output.writeMessage(3, output_.get(i));
      }
      if (correlationId_ != 0L) {
        output.writeUInt64(4, correlationId_);
      }
      if (id_ != 0L) {
        output.writeUInt64(5, id_);
      }
      if (flags_ != 0) {
        output.writeUInt32(6, flags_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (batchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(1, batchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, output_.get(i));
      }
      if (correlationId_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, correlationId_);
      }
      if (id_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, id_);
      }
      if (flags_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(6, flags_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.Api.InferRequestHeader)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.Api.InferRequestHeader other = (nvidia.inferenceserver.Api.InferRequestHeader) obj;

      if (getId()
          != other.getId()) return false;
      if (getFlags()
          != other.getFlags()) return false;
      if (getCorrelationId()
          != other.getCorrelationId()) return false;
      if (getBatchSize()
          != other.getBatchSize()) return false;
      if (!getInputList()
          .equals(other.getInputList())) return false;
      if (!getOutputList()
          .equals(other.getOutputList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getId());
      hash = (37 * hash) + FLAGS_FIELD_NUMBER;
      hash = (53 * hash) + getFlags();
      hash = (37 * hash) + CORRELATION_ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getCorrelationId());
      hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getBatchSize();
      if (getInputCount() > 0) {
        hash = (37 * hash) + INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getInputList().hashCode();
      }
      if (getOutputCount() > 0) {
        hash = (37 * hash) + OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getOutputList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferRequestHeader parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.Api.InferRequestHeader prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message InferRequestHeader
     *&#64;&#64;
     *&#64;&#64;   Meta-data for an inferencing request. The actual input data is
     *&#64;&#64;   delivered separate from this header, in the HTTP body for an HTTP
     *&#64;&#64;   request, or in the :cpp:var:`InferRequest` message for a gRPC request.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferRequestHeader}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferRequestHeader)
        nvidia.inferenceserver.Api.InferRequestHeaderOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferRequestHeader.class, nvidia.inferenceserver.Api.InferRequestHeader.Builder.class);
      }

      // Construct using nvidia.inferenceserver.Api.InferRequestHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getInputFieldBuilder();
          getOutputFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0L;

        flags_ = 0;

        correlationId_ = 0L;

        batchSize_ = 0;

        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          inputBuilder_.clear();
        }
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          outputBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferRequestHeader getDefaultInstanceForType() {
        return nvidia.inferenceserver.Api.InferRequestHeader.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferRequestHeader build() {
        nvidia.inferenceserver.Api.InferRequestHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferRequestHeader buildPartial() {
        nvidia.inferenceserver.Api.InferRequestHeader result = new nvidia.inferenceserver.Api.InferRequestHeader(this);
        int from_bitField0_ = bitField0_;
        result.id_ = id_;
        result.flags_ = flags_;
        result.correlationId_ = correlationId_;
        result.batchSize_ = batchSize_;
        if (inputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            input_ = java.util.Collections.unmodifiableList(input_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.input_ = input_;
        } else {
          result.input_ = inputBuilder_.build();
        }
        if (outputBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            output_ = java.util.Collections.unmodifiableList(output_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.output_ = output_;
        } else {
          result.output_ = outputBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.Api.InferRequestHeader) {
          return mergeFrom((nvidia.inferenceserver.Api.InferRequestHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.Api.InferRequestHeader other) {
        if (other == nvidia.inferenceserver.Api.InferRequestHeader.getDefaultInstance()) return this;
        if (other.getId() != 0L) {
          setId(other.getId());
        }
        if (other.getFlags() != 0) {
          setFlags(other.getFlags());
        }
        if (other.getCorrelationId() != 0L) {
          setCorrelationId(other.getCorrelationId());
        }
        if (other.getBatchSize() != 0) {
          setBatchSize(other.getBatchSize());
        }
        if (inputBuilder_ == null) {
          if (!other.input_.isEmpty()) {
            if (input_.isEmpty()) {
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureInputIsMutable();
              input_.addAll(other.input_);
            }
            onChanged();
          }
        } else {
          if (!other.input_.isEmpty()) {
            if (inputBuilder_.isEmpty()) {
              inputBuilder_.dispose();
              inputBuilder_ = null;
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
              inputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInputFieldBuilder() : null;
            } else {
              inputBuilder_.addAllMessages(other.input_);
            }
          }
        }
        if (outputBuilder_ == null) {
          if (!other.output_.isEmpty()) {
            if (output_.isEmpty()) {
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureOutputIsMutable();
              output_.addAll(other.output_);
            }
            onChanged();
          }
        } else {
          if (!other.output_.isEmpty()) {
            if (outputBuilder_.isEmpty()) {
              outputBuilder_.dispose();
              outputBuilder_ = null;
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
              outputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputFieldBuilder() : null;
            } else {
              outputBuilder_.addAllMessages(other.output_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.Api.InferRequestHeader parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.Api.InferRequestHeader) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long id_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference request. The response of the request will
       *&#64;&#64;     have the same ID in InferResponseHeader. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public long getId() {
        return id_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference request. The response of the request will
       *&#64;&#64;     have the same ID in InferResponseHeader. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public Builder setId(long value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference request. The response of the request will
       *&#64;&#64;     have the same ID in InferResponseHeader. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public Builder clearId() {
        
        id_ = 0L;
        onChanged();
        return this;
      }

      private int flags_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 flags
       *&#64;&#64;
       *&#64;&#64;     The flags associated with this request. This field holds a bitwise-or
       *&#64;&#64;     of all flag values.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 flags = 6;</code>
       */
      public int getFlags() {
        return flags_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 flags
       *&#64;&#64;
       *&#64;&#64;     The flags associated with this request. This field holds a bitwise-or
       *&#64;&#64;     of all flag values.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 flags = 6;</code>
       */
      public Builder setFlags(int value) {
        
        flags_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 flags
       *&#64;&#64;
       *&#64;&#64;     The flags associated with this request. This field holds a bitwise-or
       *&#64;&#64;     of all flag values.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 flags = 6;</code>
       */
      public Builder clearFlags() {
        
        flags_ = 0;
        onChanged();
        return this;
      }

      private long correlationId_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 correlation_id
       *&#64;&#64;
       *&#64;&#64;     The correlation ID of the inference request. Default is 0, which
       *&#64;&#64;     indictes that the request has no correlation ID. The correlation ID
       *&#64;&#64;     is used to indicate two or more inference request are related to
       *&#64;&#64;     each other. How this relationship is handled by the inference
       *&#64;&#64;     server is determined by the model's scheduling policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 correlation_id = 4;</code>
       */
      public long getCorrelationId() {
        return correlationId_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 correlation_id
       *&#64;&#64;
       *&#64;&#64;     The correlation ID of the inference request. Default is 0, which
       *&#64;&#64;     indictes that the request has no correlation ID. The correlation ID
       *&#64;&#64;     is used to indicate two or more inference request are related to
       *&#64;&#64;     each other. How this relationship is handled by the inference
       *&#64;&#64;     server is determined by the model's scheduling policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 correlation_id = 4;</code>
       */
      public Builder setCorrelationId(long value) {
        
        correlationId_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 correlation_id
       *&#64;&#64;
       *&#64;&#64;     The correlation ID of the inference request. Default is 0, which
       *&#64;&#64;     indictes that the request has no correlation ID. The correlation ID
       *&#64;&#64;     is used to indicate two or more inference request are related to
       *&#64;&#64;     each other. How this relationship is handled by the inference
       *&#64;&#64;     server is determined by the model's scheduling policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 correlation_id = 4;</code>
       */
      public Builder clearCorrelationId() {
        
        correlationId_ = 0L;
        onChanged();
        return this;
      }

      private int batchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 1;</code>
       */
      public int getBatchSize() {
        return batchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 1;</code>
       */
      public Builder setBatchSize(int value) {
        
        batchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 1;</code>
       */
      public Builder clearBatchSize() {
        
        batchSize_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input> input_ =
        java.util.Collections.emptyList();
      private void ensureInputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          input_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferRequestHeader.Input>(input_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferRequestHeader.Input, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder, nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder> inputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input> getInputList() {
        if (inputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(input_);
        } else {
          return inputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public int getInputCount() {
        if (inputBuilder_ == null) {
          return input_.size();
        } else {
          return inputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Input getInput(int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);
        } else {
          return inputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Input value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.set(index, value);
          onChanged();
        } else {
          inputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.set(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder addInput(nvidia.inferenceserver.Api.InferRequestHeader.Input value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(value);
          onChanged();
        } else {
          inputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Input value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(index, value);
          onChanged();
        } else {
          inputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder addInput(
          nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder addAllInput(
          java.lang.Iterable<? extends nvidia.inferenceserver.Api.InferRequestHeader.Input> values) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, input_);
          onChanged();
        } else {
          inputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder clearInput() {
        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          inputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public Builder removeInput(int index) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.remove(index);
          onChanged();
        } else {
          inputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder getInputBuilder(
          int index) {
        return getInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder getInputOrBuilder(
          int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);  } else {
          return inputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder> 
           getInputOrBuilderList() {
        if (inputBuilder_ != null) {
          return inputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(input_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder addInputBuilder() {
        return getInputFieldBuilder().addBuilder(
            nvidia.inferenceserver.Api.InferRequestHeader.Input.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder addInputBuilder(
          int index) {
        return getInputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.Api.InferRequestHeader.Input.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Input input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The input meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Input input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder> 
           getInputBuilderList() {
        return getInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferRequestHeader.Input, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder, nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder> 
          getInputFieldBuilder() {
        if (inputBuilder_ == null) {
          inputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.Api.InferRequestHeader.Input, nvidia.inferenceserver.Api.InferRequestHeader.Input.Builder, nvidia.inferenceserver.Api.InferRequestHeader.InputOrBuilder>(
                  input_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          input_ = null;
        }
        return inputBuilder_;
      }

      private java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output> output_ =
        java.util.Collections.emptyList();
      private void ensureOutputIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          output_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferRequestHeader.Output>(output_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferRequestHeader.Output, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder, nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder> outputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output> getOutputList() {
        if (outputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(output_);
        } else {
          return outputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public int getOutputCount() {
        if (outputBuilder_ == null) {
          return output_.size();
        } else {
          return outputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output getOutput(int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);
        } else {
          return outputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.set(index, value);
          onChanged();
        } else {
          outputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder addOutput(nvidia.inferenceserver.Api.InferRequestHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(value);
          onChanged();
        } else {
          outputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(index, value);
          onChanged();
        } else {
          outputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder addOutput(
          nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends nvidia.inferenceserver.Api.InferRequestHeader.Output> values) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, output_);
          onChanged();
        } else {
          outputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder clearOutput() {
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          outputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public Builder removeOutput(int index) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.remove(index);
          onChanged();
        } else {
          outputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder getOutputBuilder(
          int index) {
        return getOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder getOutputOrBuilder(
          int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);  } else {
          return outputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder> 
           getOutputOrBuilderList() {
        if (outputBuilder_ != null) {
          return outputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(output_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder addOutputBuilder() {
        return getOutputFieldBuilder().addBuilder(
            nvidia.inferenceserver.Api.InferRequestHeader.Output.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder addOutputBuilder(
          int index) {
        return getOutputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.Api.InferRequestHeader.Output.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The output meta-data for the inputs provided with the the inference
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferRequestHeader.Output output = 3;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder> 
           getOutputBuilderList() {
        return getOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferRequestHeader.Output, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder, nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder> 
          getOutputFieldBuilder() {
        if (outputBuilder_ == null) {
          outputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.Api.InferRequestHeader.Output, nvidia.inferenceserver.Api.InferRequestHeader.Output.Builder, nvidia.inferenceserver.Api.InferRequestHeader.OutputOrBuilder>(
                  output_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          output_ = null;
        }
        return outputBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferRequestHeader)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestHeader)
    private static final nvidia.inferenceserver.Api.InferRequestHeader DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferRequestHeader();
    }

    public static nvidia.inferenceserver.Api.InferRequestHeader getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InferRequestHeader>
        PARSER = new com.google.protobuf.AbstractParser<InferRequestHeader>() {
      @java.lang.Override
      public InferRequestHeader parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InferRequestHeader(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InferRequestHeader> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InferRequestHeader> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.Api.InferRequestHeader getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InferResponseHeaderOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferResponseHeader)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 id
     *&#64;&#64;
     *&#64;&#64;     The ID of the inference response. The response will have the same ID
     *&#64;&#64;     as the ID of its originated request. The request sender can use
     *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 id = 5;</code>
     */
    long getId();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string model_name
     *&#64;&#64;
     *&#64;&#64;     The name of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>string model_name = 1;</code>
     */
    java.lang.String getModelName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string model_name
     *&#64;&#64;
     *&#64;&#64;     The name of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>string model_name = 1;</code>
     */
    com.google.protobuf.ByteString
        getModelNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 model_version
     *&#64;&#64;
     *&#64;&#64;     The version of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>int64 model_version = 2;</code>
     */
    long getModelVersion();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the outputs. This will always be equal to the
     *&#64;&#64;     batch size of the inputs. For models that don't support
     *&#64;&#64;     batching the batch_size will be 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 3;</code>
     */
    int getBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    nvidia.inferenceserver.Api.InferResponseHeader.Output getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    int getOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder> 
        getOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder getOutputOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message InferResponseHeader
   *&#64;&#64;
   *&#64;&#64;   Meta-data for the response to an inferencing request. The actual output
   *&#64;&#64;   data is delivered separate from this header, in the HTTP body for an HTTP
   *&#64;&#64;   request, or in the :cpp:var:`InferResponse` message for a gRPC request.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader}
   */
  public  static final class InferResponseHeader extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferResponseHeader)
      InferResponseHeaderOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InferResponseHeader.newBuilder() to construct.
    private InferResponseHeader(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InferResponseHeader() {
      modelName_ = "";
      output_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new InferResponseHeader();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private InferResponseHeader(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              modelName_ = s;
              break;
            }
            case 16: {

              modelVersion_ = input.readInt64();
              break;
            }
            case 24: {

              batchSize_ = input.readUInt32();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                output_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output>();
                mutable_bitField0_ |= 0x00000001;
              }
              output_.add(
                  input.readMessage(nvidia.inferenceserver.Api.InferResponseHeader.Output.parser(), extensionRegistry));
              break;
            }
            case 40: {

              id_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          output_ = java.util.Collections.unmodifiableList(output_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.Api.InferResponseHeader.class, nvidia.inferenceserver.Api.InferResponseHeader.Builder.class);
    }

    public interface OutputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferResponseHeader.Output)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      boolean hasRaw();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getRaw();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder getRawOrBuilder();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> 
          getBatchClassesList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getBatchClasses(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      int getBatchClassesCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder> 
          getBatchClassesOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder getBatchClassesOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Output
     *&#64;&#64;
     *&#64;&#64;     Meta-data for an output tensor requested as part of an inferencing
     *&#64;&#64;     request.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output}
     */
    public  static final class Output extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferResponseHeader.Output)
        OutputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Output.newBuilder() to construct.
      private Output(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Output() {
        name_ = "";
        batchClasses_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Output();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Output(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 18: {
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder subBuilder = null;
                if (raw_ != null) {
                  subBuilder = raw_.toBuilder();
                }
                raw_ = input.readMessage(nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.parser(), extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(raw_);
                  raw_ = subBuilder.buildPartial();
                }

                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  batchClasses_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes>();
                  mutable_bitField0_ |= 0x00000001;
                }
                batchClasses_.add(
                    input.readMessage(nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            batchClasses_ = java.util.Collections.unmodifiableList(batchClasses_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferResponseHeader.Output.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder.class);
      }

      public interface RawOrBuilder extends
          // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferResponseHeader.Output.Raw)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        java.util.List<java.lang.Long> getDimsList();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        int getDimsCount();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        long getDims(int index);

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: uint64 batch_byte_size
         *&#64;&#64;
         *&#64;&#64;         The full size of the output tensor, in bytes. For a
         *&#64;&#64;         batch output, this is the size of the entire batch.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 batch_byte_size = 2;</code>
         */
        long getBatchByteSize();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message Raw
       *&#64;&#64;
       *&#64;&#64;       Meta-data for an output tensor being returned as raw data.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Raw}
       */
      public  static final class Raw extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferResponseHeader.Output.Raw)
          RawOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Raw.newBuilder() to construct.
        private Raw(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Raw() {
          dims_ = emptyLongList();
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new Raw();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Raw(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          int mutable_bitField0_ = 0;
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 8: {
                  if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                    dims_ = newLongList();
                    mutable_bitField0_ |= 0x00000001;
                  }
                  dims_.addLong(input.readInt64());
                  break;
                }
                case 10: {
                  int length = input.readRawVarint32();
                  int limit = input.pushLimit(length);
                  if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                    dims_ = newLongList();
                    mutable_bitField0_ |= 0x00000001;
                  }
                  while (input.getBytesUntilLimit() > 0) {
                    dims_.addLong(input.readInt64());
                  }
                  input.popLimit(limit);
                  break;
                }
                case 16: {

                  batchByteSize_ = input.readUInt64();
                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            if (((mutable_bitField0_ & 0x00000001) != 0)) {
              dims_.makeImmutable(); // C
            }
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder.class);
        }

        public static final int DIMS_FIELD_NUMBER = 1;
        private com.google.protobuf.Internal.LongList dims_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        public java.util.List<java.lang.Long>
            getDimsList() {
          return dims_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        public int getDimsCount() {
          return dims_.size();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;         The shape of the output tensor, not including the batch
         *&#64;&#64;         dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 1;</code>
         */
        public long getDims(int index) {
          return dims_.getLong(index);
        }
        private int dimsMemoizedSerializedSize = -1;

        public static final int BATCH_BYTE_SIZE_FIELD_NUMBER = 2;
        private long batchByteSize_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: uint64 batch_byte_size
         *&#64;&#64;
         *&#64;&#64;         The full size of the output tensor, in bytes. For a
         *&#64;&#64;         batch output, this is the size of the entire batch.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 batch_byte_size = 2;</code>
         */
        public long getBatchByteSize() {
          return batchByteSize_;
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          getSerializedSize();
          if (getDimsList().size() > 0) {
            output.writeUInt32NoTag(10);
            output.writeUInt32NoTag(dimsMemoizedSerializedSize);
          }
          for (int i = 0; i < dims_.size(); i++) {
            output.writeInt64NoTag(dims_.getLong(i));
          }
          if (batchByteSize_ != 0L) {
            output.writeUInt64(2, batchByteSize_);
          }
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          {
            int dataSize = 0;
            for (int i = 0; i < dims_.size(); i++) {
              dataSize += com.google.protobuf.CodedOutputStream
                .computeInt64SizeNoTag(dims_.getLong(i));
            }
            size += dataSize;
            if (!getDimsList().isEmpty()) {
              size += 1;
              size += com.google.protobuf.CodedOutputStream
                  .computeInt32SizeNoTag(dataSize);
            }
            dimsMemoizedSerializedSize = dataSize;
          }
          if (batchByteSize_ != 0L) {
            size += com.google.protobuf.CodedOutputStream
              .computeUInt64Size(2, batchByteSize_);
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw)) {
            return super.equals(obj);
          }
          nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw other = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw) obj;

          if (!getDimsList()
              .equals(other.getDimsList())) return false;
          if (getBatchByteSize()
              != other.getBatchByteSize()) return false;
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          if (getDimsCount() > 0) {
            hash = (37 * hash) + DIMS_FIELD_NUMBER;
            hash = (53 * hash) + getDimsList().hashCode();
          }
          hash = (37 * hash) + BATCH_BYTE_SIZE_FIELD_NUMBER;
          hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
              getBatchByteSize());
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message Raw
         *&#64;&#64;
         *&#64;&#64;       Meta-data for an output tensor being returned as raw data.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Raw}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferResponseHeader.Output.Raw)
            nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor;
          }

          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder.class);
          }

          // Construct using nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            dims_ = emptyLongList();
            bitField0_ = (bitField0_ & ~0x00000001);
            batchByteSize_ = 0L;

            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getDefaultInstanceForType() {
            return nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.getDefaultInstance();
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw build() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw buildPartial() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw result = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw(this);
            int from_bitField0_ = bitField0_;
            if (((bitField0_ & 0x00000001) != 0)) {
              dims_.makeImmutable();
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.dims_ = dims_;
            result.batchByteSize_ = batchByteSize_;
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw) {
              return mergeFrom((nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw other) {
            if (other == nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.getDefaultInstance()) return this;
            if (!other.dims_.isEmpty()) {
              if (dims_.isEmpty()) {
                dims_ = other.dims_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureDimsIsMutable();
                dims_.addAll(other.dims_);
              }
              onChanged();
            }
            if (other.getBatchByteSize() != 0L) {
              setBatchByteSize(other.getBatchByteSize());
            }
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }
          private int bitField0_;

          private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
          private void ensureDimsIsMutable() {
            if (!((bitField0_ & 0x00000001) != 0)) {
              dims_ = mutableCopy(dims_);
              bitField0_ |= 0x00000001;
             }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public java.util.List<java.lang.Long>
              getDimsList() {
            return ((bitField0_ & 0x00000001) != 0) ?
                     java.util.Collections.unmodifiableList(dims_) : dims_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public int getDimsCount() {
            return dims_.size();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public long getDims(int index) {
            return dims_.getLong(index);
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public Builder setDims(
              int index, long value) {
            ensureDimsIsMutable();
            dims_.setLong(index, value);
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public Builder addDims(long value) {
            ensureDimsIsMutable();
            dims_.addLong(value);
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public Builder addAllDims(
              java.lang.Iterable<? extends java.lang.Long> values) {
            ensureDimsIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, dims_);
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int64 dims (repeated)
           *&#64;&#64;
           *&#64;&#64;         The shape of the output tensor, not including the batch
           *&#64;&#64;         dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dims = 1;</code>
           */
          public Builder clearDims() {
            dims_ = emptyLongList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
            return this;
          }

          private long batchByteSize_ ;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint64 batch_byte_size
           *&#64;&#64;
           *&#64;&#64;         The full size of the output tensor, in bytes. For a
           *&#64;&#64;         batch output, this is the size of the entire batch.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint64 batch_byte_size = 2;</code>
           */
          public long getBatchByteSize() {
            return batchByteSize_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint64 batch_byte_size
           *&#64;&#64;
           *&#64;&#64;         The full size of the output tensor, in bytes. For a
           *&#64;&#64;         batch output, this is the size of the entire batch.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint64 batch_byte_size = 2;</code>
           */
          public Builder setBatchByteSize(long value) {
            
            batchByteSize_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: uint64 batch_byte_size
           *&#64;&#64;
           *&#64;&#64;         The full size of the output tensor, in bytes. For a
           *&#64;&#64;         batch output, this is the size of the entire batch.
           *&#64;&#64;
           * </pre>
           *
           * <code>uint64 batch_byte_size = 2;</code>
           */
          public Builder clearBatchByteSize() {
            
            batchByteSize_ = 0L;
            onChanged();
            return this;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferResponseHeader.Output.Raw)
        }

        // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferResponseHeader.Output.Raw)
        private static final nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw();
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Raw>
            PARSER = new com.google.protobuf.AbstractParser<Raw>() {
          @java.lang.Override
          public Raw parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Raw(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Raw> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Raw> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public interface ClassOrBuilder extends
          // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferResponseHeader.Output.Class)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 idx
         *&#64;&#64;
         *&#64;&#64;         The classification index.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 idx = 1;</code>
         */
        int getIdx();

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: float value
         *&#64;&#64;
         *&#64;&#64;         The classification value as a float (typically a
         *&#64;&#64;         probability).
         *&#64;&#64;
         * </pre>
         *
         * <code>float value = 2;</code>
         */
        float getValue();

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: string label
         *&#64;&#64;
         *&#64;&#64;         The label for the class (optional, only available if provided
         *&#64;&#64;         by the model).
         *&#64;&#64;
         * </pre>
         *
         * <code>string label = 3;</code>
         */
        java.lang.String getLabel();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: string label
         *&#64;&#64;
         *&#64;&#64;         The label for the class (optional, only available if provided
         *&#64;&#64;         by the model).
         *&#64;&#64;
         * </pre>
         *
         * <code>string label = 3;</code>
         */
        com.google.protobuf.ByteString
            getLabelBytes();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message Class
       *&#64;&#64;
       *&#64;&#64;       Information about each classification for this output.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Class}
       */
      public  static final class Class extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferResponseHeader.Output.Class)
          ClassOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Class.newBuilder() to construct.
        private Class(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Class() {
          label_ = "";
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new Class();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Class(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 8: {

                  idx_ = input.readInt32();
                  break;
                }
                case 21: {

                  value_ = input.readFloat();
                  break;
                }
                case 26: {
                  java.lang.String s = input.readStringRequireUtf8();

                  label_ = s;
                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder.class);
        }

        public static final int IDX_FIELD_NUMBER = 1;
        private int idx_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 idx
         *&#64;&#64;
         *&#64;&#64;         The classification index.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 idx = 1;</code>
         */
        public int getIdx() {
          return idx_;
        }

        public static final int VALUE_FIELD_NUMBER = 2;
        private float value_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: float value
         *&#64;&#64;
         *&#64;&#64;         The classification value as a float (typically a
         *&#64;&#64;         probability).
         *&#64;&#64;
         * </pre>
         *
         * <code>float value = 2;</code>
         */
        public float getValue() {
          return value_;
        }

        public static final int LABEL_FIELD_NUMBER = 3;
        private volatile java.lang.Object label_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: string label
         *&#64;&#64;
         *&#64;&#64;         The label for the class (optional, only available if provided
         *&#64;&#64;         by the model).
         *&#64;&#64;
         * </pre>
         *
         * <code>string label = 3;</code>
         */
        public java.lang.String getLabel() {
          java.lang.Object ref = label_;
          if (ref instanceof java.lang.String) {
            return (java.lang.String) ref;
          } else {
            com.google.protobuf.ByteString bs = 
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            label_ = s;
            return s;
          }
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: string label
         *&#64;&#64;
         *&#64;&#64;         The label for the class (optional, only available if provided
         *&#64;&#64;         by the model).
         *&#64;&#64;
         * </pre>
         *
         * <code>string label = 3;</code>
         */
        public com.google.protobuf.ByteString
            getLabelBytes() {
          java.lang.Object ref = label_;
          if (ref instanceof java.lang.String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            label_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          if (idx_ != 0) {
            output.writeInt32(1, idx_);
          }
          if (value_ != 0F) {
            output.writeFloat(2, value_);
          }
          if (!getLabelBytes().isEmpty()) {
            com.google.protobuf.GeneratedMessageV3.writeString(output, 3, label_);
          }
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          if (idx_ != 0) {
            size += com.google.protobuf.CodedOutputStream
              .computeInt32Size(1, idx_);
          }
          if (value_ != 0F) {
            size += com.google.protobuf.CodedOutputStream
              .computeFloatSize(2, value_);
          }
          if (!getLabelBytes().isEmpty()) {
            size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, label_);
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Class)) {
            return super.equals(obj);
          }
          nvidia.inferenceserver.Api.InferResponseHeader.Output.Class other = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Class) obj;

          if (getIdx()
              != other.getIdx()) return false;
          if (java.lang.Float.floatToIntBits(getValue())
              != java.lang.Float.floatToIntBits(
                  other.getValue())) return false;
          if (!getLabel()
              .equals(other.getLabel())) return false;
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          hash = (37 * hash) + IDX_FIELD_NUMBER;
          hash = (53 * hash) + getIdx();
          hash = (37 * hash) + VALUE_FIELD_NUMBER;
          hash = (53 * hash) + java.lang.Float.floatToIntBits(
              getValue());
          hash = (37 * hash) + LABEL_FIELD_NUMBER;
          hash = (53 * hash) + getLabel().hashCode();
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(nvidia.inferenceserver.Api.InferResponseHeader.Output.Class prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message Class
         *&#64;&#64;
         *&#64;&#64;       Information about each classification for this output.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Class}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferResponseHeader.Output.Class)
            nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor;
          }

          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder.class);
          }

          // Construct using nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            idx_ = 0;

            value_ = 0F;

            label_ = "";

            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getDefaultInstanceForType() {
            return nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.getDefaultInstance();
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class build() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Class result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class buildPartial() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Class result = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Class(this);
            result.idx_ = idx_;
            result.value_ = value_;
            result.label_ = label_;
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Class) {
              return mergeFrom((nvidia.inferenceserver.Api.InferResponseHeader.Output.Class)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(nvidia.inferenceserver.Api.InferResponseHeader.Output.Class other) {
            if (other == nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.getDefaultInstance()) return this;
            if (other.getIdx() != 0) {
              setIdx(other.getIdx());
            }
            if (other.getValue() != 0F) {
              setValue(other.getValue());
            }
            if (!other.getLabel().isEmpty()) {
              label_ = other.label_;
              onChanged();
            }
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Class parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Class) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }

          private int idx_ ;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 idx
           *&#64;&#64;
           *&#64;&#64;         The classification index.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 idx = 1;</code>
           */
          public int getIdx() {
            return idx_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 idx
           *&#64;&#64;
           *&#64;&#64;         The classification index.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 idx = 1;</code>
           */
          public Builder setIdx(int value) {
            
            idx_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 idx
           *&#64;&#64;
           *&#64;&#64;         The classification index.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 idx = 1;</code>
           */
          public Builder clearIdx() {
            
            idx_ = 0;
            onChanged();
            return this;
          }

          private float value_ ;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: float value
           *&#64;&#64;
           *&#64;&#64;         The classification value as a float (typically a
           *&#64;&#64;         probability).
           *&#64;&#64;
           * </pre>
           *
           * <code>float value = 2;</code>
           */
          public float getValue() {
            return value_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: float value
           *&#64;&#64;
           *&#64;&#64;         The classification value as a float (typically a
           *&#64;&#64;         probability).
           *&#64;&#64;
           * </pre>
           *
           * <code>float value = 2;</code>
           */
          public Builder setValue(float value) {
            
            value_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: float value
           *&#64;&#64;
           *&#64;&#64;         The classification value as a float (typically a
           *&#64;&#64;         probability).
           *&#64;&#64;
           * </pre>
           *
           * <code>float value = 2;</code>
           */
          public Builder clearValue() {
            
            value_ = 0F;
            onChanged();
            return this;
          }

          private java.lang.Object label_ = "";
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: string label
           *&#64;&#64;
           *&#64;&#64;         The label for the class (optional, only available if provided
           *&#64;&#64;         by the model).
           *&#64;&#64;
           * </pre>
           *
           * <code>string label = 3;</code>
           */
          public java.lang.String getLabel() {
            java.lang.Object ref = label_;
            if (!(ref instanceof java.lang.String)) {
              com.google.protobuf.ByteString bs =
                  (com.google.protobuf.ByteString) ref;
              java.lang.String s = bs.toStringUtf8();
              label_ = s;
              return s;
            } else {
              return (java.lang.String) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: string label
           *&#64;&#64;
           *&#64;&#64;         The label for the class (optional, only available if provided
           *&#64;&#64;         by the model).
           *&#64;&#64;
           * </pre>
           *
           * <code>string label = 3;</code>
           */
          public com.google.protobuf.ByteString
              getLabelBytes() {
            java.lang.Object ref = label_;
            if (ref instanceof String) {
              com.google.protobuf.ByteString b = 
                  com.google.protobuf.ByteString.copyFromUtf8(
                      (java.lang.String) ref);
              label_ = b;
              return b;
            } else {
              return (com.google.protobuf.ByteString) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: string label
           *&#64;&#64;
           *&#64;&#64;         The label for the class (optional, only available if provided
           *&#64;&#64;         by the model).
           *&#64;&#64;
           * </pre>
           *
           * <code>string label = 3;</code>
           */
          public Builder setLabel(
              java.lang.String value) {
            if (value == null) {
    throw new NullPointerException();
  }
  
            label_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: string label
           *&#64;&#64;
           *&#64;&#64;         The label for the class (optional, only available if provided
           *&#64;&#64;         by the model).
           *&#64;&#64;
           * </pre>
           *
           * <code>string label = 3;</code>
           */
          public Builder clearLabel() {
            
            label_ = getDefaultInstance().getLabel();
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: string label
           *&#64;&#64;
           *&#64;&#64;         The label for the class (optional, only available if provided
           *&#64;&#64;         by the model).
           *&#64;&#64;
           * </pre>
           *
           * <code>string label = 3;</code>
           */
          public Builder setLabelBytes(
              com.google.protobuf.ByteString value) {
            if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
            
            label_ = value;
            onChanged();
            return this;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferResponseHeader.Output.Class)
        }

        // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferResponseHeader.Output.Class)
        private static final nvidia.inferenceserver.Api.InferResponseHeader.Output.Class DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Class();
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Class>
            PARSER = new com.google.protobuf.AbstractParser<Class>() {
          @java.lang.Override
          public Class parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Class(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Class> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Class> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public interface ClassesOrBuilder extends
          // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.InferResponseHeader.Output.Classes)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> 
            getClsList();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getCls(int index);
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        int getClsCount();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder> 
            getClsOrBuilderList();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder getClsOrBuilder(
            int index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message Classes
       *&#64;&#64;
       *&#64;&#64;       Meta-data for an output tensor being returned as classifications.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Classes}
       */
      public  static final class Classes extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.InferResponseHeader.Output.Classes)
          ClassesOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Classes.newBuilder() to construct.
        private Classes(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Classes() {
          cls_ = java.util.Collections.emptyList();
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new Classes();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Classes(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          int mutable_bitField0_ = 0;
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 10: {
                  if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                    cls_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class>();
                    mutable_bitField0_ |= 0x00000001;
                  }
                  cls_.add(
                      input.readMessage(nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.parser(), extensionRegistry));
                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            if (((mutable_bitField0_ & 0x00000001) != 0)) {
              cls_ = java.util.Collections.unmodifiableList(cls_);
            }
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder.class);
        }

        public static final int CLS_FIELD_NUMBER = 1;
        private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> cls_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> getClsList() {
          return cls_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder> 
            getClsOrBuilderList() {
          return cls_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        public int getClsCount() {
          return cls_.size();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getCls(int index) {
          return cls_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: Class cls (repeated)
         *&#64;&#64;
         *&#64;&#64;         The topk classes for this output.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder getClsOrBuilder(
            int index) {
          return cls_.get(index);
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          for (int i = 0; i < cls_.size(); i++) {
            output.writeMessage(1, cls_.get(i));
          }
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          for (int i = 0; i < cls_.size(); i++) {
            size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(1, cls_.get(i));
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes)) {
            return super.equals(obj);
          }
          nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes other = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes) obj;

          if (!getClsList()
              .equals(other.getClsList())) return false;
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          if (getClsCount() > 0) {
            hash = (37 * hash) + CLS_FIELD_NUMBER;
            hash = (53 * hash) + getClsList().hashCode();
          }
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message Classes
         *&#64;&#64;
         *&#64;&#64;       Meta-data for an output tensor being returned as classifications.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output.Classes}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferResponseHeader.Output.Classes)
            nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor;
          }

          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder.class);
          }

          // Construct using nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
              getClsFieldBuilder();
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            if (clsBuilder_ == null) {
              cls_ = java.util.Collections.emptyList();
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              clsBuilder_.clear();
            }
            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getDefaultInstanceForType() {
            return nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.getDefaultInstance();
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes build() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes buildPartial() {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes result = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes(this);
            int from_bitField0_ = bitField0_;
            if (clsBuilder_ == null) {
              if (((bitField0_ & 0x00000001) != 0)) {
                cls_ = java.util.Collections.unmodifiableList(cls_);
                bitField0_ = (bitField0_ & ~0x00000001);
              }
              result.cls_ = cls_;
            } else {
              result.cls_ = clsBuilder_.build();
            }
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes) {
              return mergeFrom((nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes other) {
            if (other == nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.getDefaultInstance()) return this;
            if (clsBuilder_ == null) {
              if (!other.cls_.isEmpty()) {
                if (cls_.isEmpty()) {
                  cls_ = other.cls_;
                  bitField0_ = (bitField0_ & ~0x00000001);
                } else {
                  ensureClsIsMutable();
                  cls_.addAll(other.cls_);
                }
                onChanged();
              }
            } else {
              if (!other.cls_.isEmpty()) {
                if (clsBuilder_.isEmpty()) {
                  clsBuilder_.dispose();
                  clsBuilder_ = null;
                  cls_ = other.cls_;
                  bitField0_ = (bitField0_ & ~0x00000001);
                  clsBuilder_ = 
                    com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                       getClsFieldBuilder() : null;
                } else {
                  clsBuilder_.addAllMessages(other.cls_);
                }
              }
            }
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }
          private int bitField0_;

          private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> cls_ =
            java.util.Collections.emptyList();
          private void ensureClsIsMutable() {
            if (!((bitField0_ & 0x00000001) != 0)) {
              cls_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class>(cls_);
              bitField0_ |= 0x00000001;
             }
          }

          private com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.Api.InferResponseHeader.Output.Class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder> clsBuilder_;

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> getClsList() {
            if (clsBuilder_ == null) {
              return java.util.Collections.unmodifiableList(cls_);
            } else {
              return clsBuilder_.getMessageList();
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public int getClsCount() {
            if (clsBuilder_ == null) {
              return cls_.size();
            } else {
              return clsBuilder_.getCount();
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class getCls(int index) {
            if (clsBuilder_ == null) {
              return cls_.get(index);
            } else {
              return clsBuilder_.getMessage(index);
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder setCls(
              int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class value) {
            if (clsBuilder_ == null) {
              if (value == null) {
                throw new NullPointerException();
              }
              ensureClsIsMutable();
              cls_.set(index, value);
              onChanged();
            } else {
              clsBuilder_.setMessage(index, value);
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder setCls(
              int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder builderForValue) {
            if (clsBuilder_ == null) {
              ensureClsIsMutable();
              cls_.set(index, builderForValue.build());
              onChanged();
            } else {
              clsBuilder_.setMessage(index, builderForValue.build());
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder addCls(nvidia.inferenceserver.Api.InferResponseHeader.Output.Class value) {
            if (clsBuilder_ == null) {
              if (value == null) {
                throw new NullPointerException();
              }
              ensureClsIsMutable();
              cls_.add(value);
              onChanged();
            } else {
              clsBuilder_.addMessage(value);
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder addCls(
              int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class value) {
            if (clsBuilder_ == null) {
              if (value == null) {
                throw new NullPointerException();
              }
              ensureClsIsMutable();
              cls_.add(index, value);
              onChanged();
            } else {
              clsBuilder_.addMessage(index, value);
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder addCls(
              nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder builderForValue) {
            if (clsBuilder_ == null) {
              ensureClsIsMutable();
              cls_.add(builderForValue.build());
              onChanged();
            } else {
              clsBuilder_.addMessage(builderForValue.build());
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder addCls(
              int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder builderForValue) {
            if (clsBuilder_ == null) {
              ensureClsIsMutable();
              cls_.add(index, builderForValue.build());
              onChanged();
            } else {
              clsBuilder_.addMessage(index, builderForValue.build());
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder addAllCls(
              java.lang.Iterable<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.Class> values) {
            if (clsBuilder_ == null) {
              ensureClsIsMutable();
              com.google.protobuf.AbstractMessageLite.Builder.addAll(
                  values, cls_);
              onChanged();
            } else {
              clsBuilder_.addAllMessages(values);
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder clearCls() {
            if (clsBuilder_ == null) {
              cls_ = java.util.Collections.emptyList();
              bitField0_ = (bitField0_ & ~0x00000001);
              onChanged();
            } else {
              clsBuilder_.clear();
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public Builder removeCls(int index) {
            if (clsBuilder_ == null) {
              ensureClsIsMutable();
              cls_.remove(index);
              onChanged();
            } else {
              clsBuilder_.remove(index);
            }
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder getClsBuilder(
              int index) {
            return getClsFieldBuilder().getBuilder(index);
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder getClsOrBuilder(
              int index) {
            if (clsBuilder_ == null) {
              return cls_.get(index);  } else {
              return clsBuilder_.getMessageOrBuilder(index);
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder> 
               getClsOrBuilderList() {
            if (clsBuilder_ != null) {
              return clsBuilder_.getMessageOrBuilderList();
            } else {
              return java.util.Collections.unmodifiableList(cls_);
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder addClsBuilder() {
            return getClsFieldBuilder().addBuilder(
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.getDefaultInstance());
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder addClsBuilder(
              int index) {
            return getClsFieldBuilder().addBuilder(
                index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.getDefaultInstance());
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: Class cls (repeated)
           *&#64;&#64;
           *&#64;&#64;         The topk classes for this output.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Class cls = 1;</code>
           */
          public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder> 
               getClsBuilderList() {
            return getClsFieldBuilder().getBuilderList();
          }
          private com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.Api.InferResponseHeader.Output.Class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder> 
              getClsFieldBuilder() {
            if (clsBuilder_ == null) {
              clsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                  nvidia.inferenceserver.Api.InferResponseHeader.Output.Class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Class.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassOrBuilder>(
                      cls_,
                      ((bitField0_ & 0x00000001) != 0),
                      getParentForChildren(),
                      isClean());
              cls_ = null;
            }
            return clsBuilder_;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferResponseHeader.Output.Classes)
        }

        // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferResponseHeader.Output.Classes)
        private static final nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes();
        }

        public static nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Classes>
            PARSER = new com.google.protobuf.AbstractParser<Classes>() {
          @java.lang.Override
          public Classes parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Classes(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Classes> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Classes> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int RAW_FIELD_NUMBER = 2;
      private nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw raw_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      public boolean hasRaw() {
        return raw_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getRaw() {
        return raw_ == null ? nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.getDefaultInstance() : raw_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Raw raw
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as raw tensor data.
       *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
       *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
       *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder getRawOrBuilder() {
        return getRaw();
      }

      public static final int BATCH_CLASSES_FIELD_NUMBER = 3;
      private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> batchClasses_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> getBatchClassesList() {
        return batchClasses_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder> 
          getBatchClassesOrBuilderList() {
        return batchClasses_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      public int getBatchClassesCount() {
        return batchClasses_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getBatchClasses(int index) {
        return batchClasses_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
       *&#64;&#64;
       *&#64;&#64;       If specified deliver results for this output as classifications.
       *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
       *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
       *&#64;&#64;       specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder getBatchClassesOrBuilder(
          int index) {
        return batchClasses_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (raw_ != null) {
          output.writeMessage(2, getRaw());
        }
        for (int i = 0; i < batchClasses_.size(); i++) {
          output.writeMessage(3, batchClasses_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        if (raw_ != null) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, getRaw());
        }
        for (int i = 0; i < batchClasses_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, batchClasses_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.Api.InferResponseHeader.Output other = (nvidia.inferenceserver.Api.InferResponseHeader.Output) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (hasRaw() != other.hasRaw()) return false;
        if (hasRaw()) {
          if (!getRaw()
              .equals(other.getRaw())) return false;
        }
        if (!getBatchClassesList()
            .equals(other.getBatchClassesList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (hasRaw()) {
          hash = (37 * hash) + RAW_FIELD_NUMBER;
          hash = (53 * hash) + getRaw().hashCode();
        }
        if (getBatchClassesCount() > 0) {
          hash = (37 * hash) + BATCH_CLASSES_FIELD_NUMBER;
          hash = (53 * hash) + getBatchClassesList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.Api.InferResponseHeader.Output parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.Api.InferResponseHeader.Output prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Output
       *&#64;&#64;
       *&#64;&#64;     Meta-data for an output tensor requested as part of an inferencing
       *&#64;&#64;     request.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader.Output}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferResponseHeader.Output)
          nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.Api.InferResponseHeader.Output.class, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder.class);
        }

        // Construct using nvidia.inferenceserver.Api.InferResponseHeader.Output.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getBatchClassesFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          if (rawBuilder_ == null) {
            raw_ = null;
          } else {
            raw_ = null;
            rawBuilder_ = null;
          }
          if (batchClassesBuilder_ == null) {
            batchClasses_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            batchClassesBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output getDefaultInstanceForType() {
          return nvidia.inferenceserver.Api.InferResponseHeader.Output.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output build() {
          nvidia.inferenceserver.Api.InferResponseHeader.Output result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.Api.InferResponseHeader.Output buildPartial() {
          nvidia.inferenceserver.Api.InferResponseHeader.Output result = new nvidia.inferenceserver.Api.InferResponseHeader.Output(this);
          int from_bitField0_ = bitField0_;
          result.name_ = name_;
          if (rawBuilder_ == null) {
            result.raw_ = raw_;
          } else {
            result.raw_ = rawBuilder_.build();
          }
          if (batchClassesBuilder_ == null) {
            if (((bitField0_ & 0x00000001) != 0)) {
              batchClasses_ = java.util.Collections.unmodifiableList(batchClasses_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.batchClasses_ = batchClasses_;
          } else {
            result.batchClasses_ = batchClassesBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.Api.InferResponseHeader.Output) {
            return mergeFrom((nvidia.inferenceserver.Api.InferResponseHeader.Output)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.Api.InferResponseHeader.Output other) {
          if (other == nvidia.inferenceserver.Api.InferResponseHeader.Output.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (other.hasRaw()) {
            mergeRaw(other.getRaw());
          }
          if (batchClassesBuilder_ == null) {
            if (!other.batchClasses_.isEmpty()) {
              if (batchClasses_.isEmpty()) {
                batchClasses_ = other.batchClasses_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureBatchClassesIsMutable();
                batchClasses_.addAll(other.batchClasses_);
              }
              onChanged();
            }
          } else {
            if (!other.batchClasses_.isEmpty()) {
              if (batchClassesBuilder_.isEmpty()) {
                batchClassesBuilder_.dispose();
                batchClassesBuilder_ = null;
                batchClasses_ = other.batchClasses_;
                bitField0_ = (bitField0_ & ~0x00000001);
                batchClassesBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getBatchClassesFieldBuilder() : null;
              } else {
                batchClassesBuilder_.addAllMessages(other.batchClasses_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.Api.InferResponseHeader.Output parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.Api.InferResponseHeader.Output) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the output tensor.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw raw_;
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw, nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder> rawBuilder_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public boolean hasRaw() {
          return rawBuilder_ != null || raw_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw getRaw() {
          if (rawBuilder_ == null) {
            return raw_ == null ? nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.getDefaultInstance() : raw_;
          } else {
            return rawBuilder_.getMessage();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public Builder setRaw(nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw value) {
          if (rawBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            raw_ = value;
            onChanged();
          } else {
            rawBuilder_.setMessage(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public Builder setRaw(
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder builderForValue) {
          if (rawBuilder_ == null) {
            raw_ = builderForValue.build();
            onChanged();
          } else {
            rawBuilder_.setMessage(builderForValue.build());
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public Builder mergeRaw(nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw value) {
          if (rawBuilder_ == null) {
            if (raw_ != null) {
              raw_ =
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.newBuilder(raw_).mergeFrom(value).buildPartial();
            } else {
              raw_ = value;
            }
            onChanged();
          } else {
            rawBuilder_.mergeFrom(value);
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public Builder clearRaw() {
          if (rawBuilder_ == null) {
            raw_ = null;
            onChanged();
          } else {
            raw_ = null;
            rawBuilder_ = null;
          }

          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder getRawBuilder() {
          
          onChanged();
          return getRawFieldBuilder().getBuilder();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder getRawOrBuilder() {
          if (rawBuilder_ != null) {
            return rawBuilder_.getMessageOrBuilder();
          } else {
            return raw_ == null ?
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.getDefaultInstance() : raw_;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Raw raw
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as raw tensor data.
         *&#64;&#64;       The actual output data is delivered in the HTTP body for an HTTP
         *&#64;&#64;       request, or in the :cpp:var:`InferResponse` message for a gRPC
         *&#64;&#64;       request. Only one of 'raw' and 'batch_classes' may be specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.InferResponseHeader.Output.Raw raw = 2;</code>
         */
        private com.google.protobuf.SingleFieldBuilderV3<
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw, nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder> 
            getRawFieldBuilder() {
          if (rawBuilder_ == null) {
            rawBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw, nvidia.inferenceserver.Api.InferResponseHeader.Output.Raw.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.RawOrBuilder>(
                    getRaw(),
                    getParentForChildren(),
                    isClean());
            raw_ = null;
          }
          return rawBuilder_;
        }

        private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> batchClasses_ =
          java.util.Collections.emptyList();
        private void ensureBatchClassesIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            batchClasses_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes>(batchClasses_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder> batchClassesBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> getBatchClassesList() {
          if (batchClassesBuilder_ == null) {
            return java.util.Collections.unmodifiableList(batchClasses_);
          } else {
            return batchClassesBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public int getBatchClassesCount() {
          if (batchClassesBuilder_ == null) {
            return batchClasses_.size();
          } else {
            return batchClassesBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes getBatchClasses(int index) {
          if (batchClassesBuilder_ == null) {
            return batchClasses_.get(index);
          } else {
            return batchClassesBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder setBatchClasses(
            int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes value) {
          if (batchClassesBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBatchClassesIsMutable();
            batchClasses_.set(index, value);
            onChanged();
          } else {
            batchClassesBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder setBatchClasses(
            int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder builderForValue) {
          if (batchClassesBuilder_ == null) {
            ensureBatchClassesIsMutable();
            batchClasses_.set(index, builderForValue.build());
            onChanged();
          } else {
            batchClassesBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder addBatchClasses(nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes value) {
          if (batchClassesBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBatchClassesIsMutable();
            batchClasses_.add(value);
            onChanged();
          } else {
            batchClassesBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder addBatchClasses(
            int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes value) {
          if (batchClassesBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureBatchClassesIsMutable();
            batchClasses_.add(index, value);
            onChanged();
          } else {
            batchClassesBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder addBatchClasses(
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder builderForValue) {
          if (batchClassesBuilder_ == null) {
            ensureBatchClassesIsMutable();
            batchClasses_.add(builderForValue.build());
            onChanged();
          } else {
            batchClassesBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder addBatchClasses(
            int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder builderForValue) {
          if (batchClassesBuilder_ == null) {
            ensureBatchClassesIsMutable();
            batchClasses_.add(index, builderForValue.build());
            onChanged();
          } else {
            batchClassesBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder addAllBatchClasses(
            java.lang.Iterable<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes> values) {
          if (batchClassesBuilder_ == null) {
            ensureBatchClassesIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, batchClasses_);
            onChanged();
          } else {
            batchClassesBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder clearBatchClasses() {
          if (batchClassesBuilder_ == null) {
            batchClasses_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            batchClassesBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public Builder removeBatchClasses(int index) {
          if (batchClassesBuilder_ == null) {
            ensureBatchClassesIsMutable();
            batchClasses_.remove(index);
            onChanged();
          } else {
            batchClassesBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder getBatchClassesBuilder(
            int index) {
          return getBatchClassesFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder getBatchClassesOrBuilder(
            int index) {
          if (batchClassesBuilder_ == null) {
            return batchClasses_.get(index);  } else {
            return batchClassesBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder> 
             getBatchClassesOrBuilderList() {
          if (batchClassesBuilder_ != null) {
            return batchClassesBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(batchClasses_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder addBatchClassesBuilder() {
          return getBatchClassesFieldBuilder().addBuilder(
              nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder addBatchClassesBuilder(
            int index) {
          return getBatchClassesFieldBuilder().addBuilder(
              index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Classes batch_classes (repeated)
         *&#64;&#64;
         *&#64;&#64;       If specified deliver results for this output as classifications.
         *&#64;&#64;       There is one :cpp:var:`Classes` object for each batch entry in
         *&#64;&#64;       the output. Only one of 'raw' and 'batch_classes' may be
         *&#64;&#64;       specified.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output.Classes batch_classes = 3;</code>
         */
        public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder> 
             getBatchClassesBuilderList() {
          return getBatchClassesFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder> 
            getBatchClassesFieldBuilder() {
          if (batchClassesBuilder_ == null) {
            batchClassesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes, nvidia.inferenceserver.Api.InferResponseHeader.Output.Classes.Builder, nvidia.inferenceserver.Api.InferResponseHeader.Output.ClassesOrBuilder>(
                    batchClasses_,
                    ((bitField0_ & 0x00000001) != 0),
                    getParentForChildren(),
                    isClean());
            batchClasses_ = null;
          }
          return batchClassesBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferResponseHeader.Output)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferResponseHeader.Output)
      private static final nvidia.inferenceserver.Api.InferResponseHeader.Output DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferResponseHeader.Output();
      }

      public static nvidia.inferenceserver.Api.InferResponseHeader.Output getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Output>
          PARSER = new com.google.protobuf.AbstractParser<Output>() {
        @java.lang.Override
        public Output parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Output(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Output> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Output> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferResponseHeader.Output getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int ID_FIELD_NUMBER = 5;
    private long id_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 id
     *&#64;&#64;
     *&#64;&#64;     The ID of the inference response. The response will have the same ID
     *&#64;&#64;     as the ID of its originated request. The request sender can use
     *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 id = 5;</code>
     */
    public long getId() {
      return id_;
    }

    public static final int MODEL_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object modelName_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string model_name
     *&#64;&#64;
     *&#64;&#64;     The name of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>string model_name = 1;</code>
     */
    public java.lang.String getModelName() {
      java.lang.Object ref = modelName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        modelName_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string model_name
     *&#64;&#64;
     *&#64;&#64;     The name of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>string model_name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getModelNameBytes() {
      java.lang.Object ref = modelName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        modelName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MODEL_VERSION_FIELD_NUMBER = 2;
    private long modelVersion_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 model_version
     *&#64;&#64;
     *&#64;&#64;     The version of the model that produced the outputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>int64 model_version = 2;</code>
     */
    public long getModelVersion() {
      return modelVersion_;
    }

    public static final int BATCH_SIZE_FIELD_NUMBER = 3;
    private int batchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the outputs. This will always be equal to the
     *&#64;&#64;     batch size of the inputs. For models that don't support
     *&#64;&#64;     batching the batch_size will be 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 3;</code>
     */
    public int getBatchSize() {
      return batchSize_;
    }

    public static final int OUTPUT_FIELD_NUMBER = 4;
    private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    public nvidia.inferenceserver.Api.InferResponseHeader.Output getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Output output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs, in the same order as they were requested in
     *&#64;&#64;     :cpp:var:`InferRequestHeader`.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
     */
    public nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getModelNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, modelName_);
      }
      if (modelVersion_ != 0L) {
        output.writeInt64(2, modelVersion_);
      }
      if (batchSize_ != 0) {
        output.writeUInt32(3, batchSize_);
      }
      for (int i = 0; i < output_.size(); i++) {
        output.writeMessage(4, output_.get(i));
      }
      if (id_ != 0L) {
        output.writeUInt64(5, id_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getModelNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, modelName_);
      }
      if (modelVersion_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, modelVersion_);
      }
      if (batchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, batchSize_);
      }
      for (int i = 0; i < output_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, output_.get(i));
      }
      if (id_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(5, id_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.Api.InferResponseHeader)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.Api.InferResponseHeader other = (nvidia.inferenceserver.Api.InferResponseHeader) obj;

      if (getId()
          != other.getId()) return false;
      if (!getModelName()
          .equals(other.getModelName())) return false;
      if (getModelVersion()
          != other.getModelVersion()) return false;
      if (getBatchSize()
          != other.getBatchSize()) return false;
      if (!getOutputList()
          .equals(other.getOutputList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ID_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getId());
      hash = (37 * hash) + MODEL_NAME_FIELD_NUMBER;
      hash = (53 * hash) + getModelName().hashCode();
      hash = (37 * hash) + MODEL_VERSION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getModelVersion());
      hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getBatchSize();
      if (getOutputCount() > 0) {
        hash = (37 * hash) + OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getOutputList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.Api.InferResponseHeader parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.Api.InferResponseHeader prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message InferResponseHeader
     *&#64;&#64;
     *&#64;&#64;   Meta-data for the response to an inferencing request. The actual output
     *&#64;&#64;   data is delivered separate from this header, in the HTTP body for an HTTP
     *&#64;&#64;   request, or in the :cpp:var:`InferResponse` message for a gRPC request.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.InferResponseHeader}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.InferResponseHeader)
        nvidia.inferenceserver.Api.InferResponseHeaderOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.Api.InferResponseHeader.class, nvidia.inferenceserver.Api.InferResponseHeader.Builder.class);
      }

      // Construct using nvidia.inferenceserver.Api.InferResponseHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getOutputFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        id_ = 0L;

        modelName_ = "";

        modelVersion_ = 0L;

        batchSize_ = 0;

        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          outputBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.Api.internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferResponseHeader getDefaultInstanceForType() {
        return nvidia.inferenceserver.Api.InferResponseHeader.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferResponseHeader build() {
        nvidia.inferenceserver.Api.InferResponseHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.Api.InferResponseHeader buildPartial() {
        nvidia.inferenceserver.Api.InferResponseHeader result = new nvidia.inferenceserver.Api.InferResponseHeader(this);
        int from_bitField0_ = bitField0_;
        result.id_ = id_;
        result.modelName_ = modelName_;
        result.modelVersion_ = modelVersion_;
        result.batchSize_ = batchSize_;
        if (outputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            output_ = java.util.Collections.unmodifiableList(output_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.output_ = output_;
        } else {
          result.output_ = outputBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.Api.InferResponseHeader) {
          return mergeFrom((nvidia.inferenceserver.Api.InferResponseHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.Api.InferResponseHeader other) {
        if (other == nvidia.inferenceserver.Api.InferResponseHeader.getDefaultInstance()) return this;
        if (other.getId() != 0L) {
          setId(other.getId());
        }
        if (!other.getModelName().isEmpty()) {
          modelName_ = other.modelName_;
          onChanged();
        }
        if (other.getModelVersion() != 0L) {
          setModelVersion(other.getModelVersion());
        }
        if (other.getBatchSize() != 0) {
          setBatchSize(other.getBatchSize());
        }
        if (outputBuilder_ == null) {
          if (!other.output_.isEmpty()) {
            if (output_.isEmpty()) {
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureOutputIsMutable();
              output_.addAll(other.output_);
            }
            onChanged();
          }
        } else {
          if (!other.output_.isEmpty()) {
            if (outputBuilder_.isEmpty()) {
              outputBuilder_.dispose();
              outputBuilder_ = null;
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000001);
              outputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputFieldBuilder() : null;
            } else {
              outputBuilder_.addAllMessages(other.output_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.Api.InferResponseHeader parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.Api.InferResponseHeader) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long id_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference response. The response will have the same ID
       *&#64;&#64;     as the ID of its originated request. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public long getId() {
        return id_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference response. The response will have the same ID
       *&#64;&#64;     as the ID of its originated request. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public Builder setId(long value) {
        
        id_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 id
       *&#64;&#64;
       *&#64;&#64;     The ID of the inference response. The response will have the same ID
       *&#64;&#64;     as the ID of its originated request. The request sender can use
       *&#64;&#64;     the ID to correlate the response to corresponding request if needed.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 id = 5;</code>
       */
      public Builder clearId() {
        
        id_ = 0L;
        onChanged();
        return this;
      }

      private java.lang.Object modelName_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public java.lang.String getModelName() {
        java.lang.Object ref = modelName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          modelName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getModelNameBytes() {
        java.lang.Object ref = modelName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          modelName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public Builder setModelName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        modelName_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public Builder clearModelName() {
        
        modelName_ = getDefaultInstance().getModelName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public Builder setModelNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        modelName_ = value;
        onChanged();
        return this;
      }

      private long modelVersion_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      public long getModelVersion() {
        return modelVersion_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      public Builder setModelVersion(long value) {
        
        modelVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model that produced the outputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      public Builder clearModelVersion() {
        
        modelVersion_ = 0L;
        onChanged();
        return this;
      }

      private int batchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the outputs. This will always be equal to the
       *&#64;&#64;     batch size of the inputs. For models that don't support
       *&#64;&#64;     batching the batch_size will be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 3;</code>
       */
      public int getBatchSize() {
        return batchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the outputs. This will always be equal to the
       *&#64;&#64;     batch size of the inputs. For models that don't support
       *&#64;&#64;     batching the batch_size will be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 3;</code>
       */
      public Builder setBatchSize(int value) {
        
        batchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the outputs. This will always be equal to the
       *&#64;&#64;     batch size of the inputs. For models that don't support
       *&#64;&#64;     batching the batch_size will be 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 3;</code>
       */
      public Builder clearBatchSize() {
        
        batchSize_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output> output_ =
        java.util.Collections.emptyList();
      private void ensureOutputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          output_ = new java.util.ArrayList<nvidia.inferenceserver.Api.InferResponseHeader.Output>(output_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferResponseHeader.Output, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder, nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder> outputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output> getOutputList() {
        if (outputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(output_);
        } else {
          return outputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public int getOutputCount() {
        if (outputBuilder_ == null) {
          return output_.size();
        } else {
          return outputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output getOutput(int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);
        } else {
          return outputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.Api.InferResponseHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.set(index, value);
          onChanged();
        } else {
          outputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder addOutput(nvidia.inferenceserver.Api.InferResponseHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(value);
          onChanged();
        } else {
          outputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.Api.InferResponseHeader.Output value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(index, value);
          onChanged();
        } else {
          outputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder addOutput(
          nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends nvidia.inferenceserver.Api.InferResponseHeader.Output> values) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, output_);
          onChanged();
        } else {
          outputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder clearOutput() {
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          outputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public Builder removeOutput(int index) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.remove(index);
          onChanged();
        } else {
          outputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder getOutputBuilder(
          int index) {
        return getOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder getOutputOrBuilder(
          int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);  } else {
          return outputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder> 
           getOutputOrBuilderList() {
        if (outputBuilder_ != null) {
          return outputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(output_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder addOutputBuilder() {
        return getOutputFieldBuilder().addBuilder(
            nvidia.inferenceserver.Api.InferResponseHeader.Output.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder addOutputBuilder(
          int index) {
        return getOutputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.Api.InferResponseHeader.Output.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Output output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs, in the same order as they were requested in
       *&#64;&#64;     :cpp:var:`InferRequestHeader`.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.InferResponseHeader.Output output = 4;</code>
       */
      public java.util.List<nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder> 
           getOutputBuilderList() {
        return getOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.Api.InferResponseHeader.Output, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder, nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder> 
          getOutputFieldBuilder() {
        if (outputBuilder_ == null) {
          outputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.Api.InferResponseHeader.Output, nvidia.inferenceserver.Api.InferResponseHeader.Output.Builder, nvidia.inferenceserver.Api.InferResponseHeader.OutputOrBuilder>(
                  output_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          output_ = null;
        }
        return outputBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.InferResponseHeader)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferResponseHeader)
    private static final nvidia.inferenceserver.Api.InferResponseHeader DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.Api.InferResponseHeader();
    }

    public static nvidia.inferenceserver.Api.InferResponseHeader getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InferResponseHeader>
        PARSER = new com.google.protobuf.AbstractParser<InferResponseHeader>() {
      @java.lang.Override
      public InferResponseHeader parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new InferResponseHeader(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<InferResponseHeader> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<InferResponseHeader> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.Api.InferResponseHeader getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferSharedMemory_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferRequestHeader_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferRequestHeader_Input_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferRequestHeader_Output_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferResponseHeader_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\tapi.proto\022\026nvidia.inferenceserver\"D\n\021I" +
      "nferSharedMemory\022\014\n\004name\030\001 \001(\t\022\016\n\006offset" +
      "\030\002 \001(\004\022\021\n\tbyte_size\030\003 \001(\004\"\337\004\n\022InferReque" +
      "stHeader\022\n\n\002id\030\005 \001(\004\022\r\n\005flags\030\006 \001(\r\022\026\n\016c" +
      "orrelation_id\030\004 \001(\004\022\022\n\nbatch_size\030\001 \001(\r\022" +
      "?\n\005input\030\002 \003(\01320.nvidia.inferenceserver." +
      "InferRequestHeader.Input\022A\n\006output\030\003 \003(\013" +
      "21.nvidia.inferenceserver.InferRequestHe" +
      "ader.Output\032~\n\005Input\022\014\n\004name\030\001 \001(\t\022\014\n\004di" +
      "ms\030\002 \003(\003\022\027\n\017batch_byte_size\030\003 \001(\004\022@\n\rsha" +
      "red_memory\030\004 \001(\0132).nvidia.inferenceserve" +
      "r.InferSharedMemory\032\266\001\n\006Output\022\014\n\004name\030\001" +
      " \001(\t\022D\n\003cls\030\003 \001(\01327.nvidia.inferenceserv" +
      "er.InferRequestHeader.Output.Class\022@\n\rsh" +
      "ared_memory\030\004 \001(\0132).nvidia.inferenceserv" +
      "er.InferSharedMemory\032\026\n\005Class\022\r\n\005count\030\001" +
      " \001(\r\"E\n\004Flag\022\r\n\tFLAG_NONE\020\000\022\027\n\023FLAG_SEQU" +
      "ENCE_START\020\001\022\025\n\021FLAG_SEQUENCE_END\020\002\"\211\004\n\023" +
      "InferResponseHeader\022\n\n\002id\030\005 \001(\004\022\022\n\nmodel" +
      "_name\030\001 \001(\t\022\025\n\rmodel_version\030\002 \001(\003\022\022\n\nba" +
      "tch_size\030\003 \001(\r\022B\n\006output\030\004 \003(\01322.nvidia." +
      "inferenceserver.InferResponseHeader.Outp" +
      "ut\032\342\002\n\006Output\022\014\n\004name\030\001 \001(\t\022C\n\003raw\030\002 \001(\013" +
      "26.nvidia.inferenceserver.InferResponseH" +
      "eader.Output.Raw\022Q\n\rbatch_classes\030\003 \003(\0132" +
      ":.nvidia.inferenceserver.InferResponseHe" +
      "ader.Output.Classes\032,\n\003Raw\022\014\n\004dims\030\001 \003(\003" +
      "\022\027\n\017batch_byte_size\030\002 \001(\004\0322\n\005Class\022\013\n\003id" +
      "x\030\001 \001(\005\022\r\n\005value\030\002 \001(\002\022\r\n\005label\030\003 \001(\t\032P\n" +
      "\007Classes\022E\n\003cls\030\001 \003(\01328.nvidia.inference" +
      "server.InferResponseHeader.Output.Classb" +
      "\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_nvidia_inferenceserver_InferSharedMemory_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferSharedMemory_descriptor,
        new java.lang.String[] { "Name", "Offset", "ByteSize", });
    internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_nvidia_inferenceserver_InferRequestHeader_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor,
        new java.lang.String[] { "Id", "Flags", "CorrelationId", "BatchSize", "Input", "Output", });
    internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor =
      internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_InferRequestHeader_Input_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferRequestHeader_Input_descriptor,
        new java.lang.String[] { "Name", "Dims", "BatchByteSize", "SharedMemory", });
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor =
      internal_static_nvidia_inferenceserver_InferRequestHeader_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor,
        new java.lang.String[] { "Name", "Cls", "SharedMemory", });
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor =
      internal_static_nvidia_inferenceserver_InferRequestHeader_Output_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferRequestHeader_Output_Class_descriptor,
        new java.lang.String[] { "Count", });
    internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_nvidia_inferenceserver_InferResponseHeader_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor,
        new java.lang.String[] { "Id", "ModelName", "ModelVersion", "BatchSize", "Output", });
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor =
      internal_static_nvidia_inferenceserver_InferResponseHeader_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor,
        new java.lang.String[] { "Name", "Raw", "BatchClasses", });
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor =
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Raw_descriptor,
        new java.lang.String[] { "Dims", "BatchByteSize", });
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor =
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Class_descriptor,
        new java.lang.String[] { "Idx", "Value", "Label", });
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor =
      internal_static_nvidia_inferenceserver_InferResponseHeader_Output_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_InferResponseHeader_Output_Classes_descriptor,
        new java.lang.String[] { "Cls", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
