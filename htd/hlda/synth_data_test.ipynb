{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from synth_data import HldaDataGenerator\n",
    "from hlda.sampler import NCRPNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data test for hierarchical LDA inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['w0' 'w1' 'w2' 'w3' 'w4']\n",
      " ['w5' 'w6' 'w7' 'w8' 'w9']\n",
      " ['w10' 'w11' 'w12' 'w13' 'w14']\n",
      " ['w15' 'w16' 'w17' 'w18' 'w19']\n",
      " ['w20' 'w21' 'w22' 'w23' 'w24']\n",
      " ['w25' 'w26' 'w27' 'w28' 'w29']\n",
      " ['w30' 'w31' 'w32' 'w33' 'w34']\n",
      " ['w35' 'w36' 'w37' 'w38' 'w39']\n",
      " ['w40' 'w41' 'w42' 'w43' 'w44']\n",
      " ['w45' 'w46' 'w47' 'w48' 'w49']\n",
      " ['w50' 'w51' 'w52' 'w53' 'w54']\n",
      " ['w55' 'w56' 'w57' 'w58' 'w59']\n",
      " ['w60' 'w61' 'w62' 'w63' 'w64']\n",
      " ['w65' 'w66' 'w67' 'w68' 'w69']\n",
      " ['w70' 'w71' 'w72' 'w73' 'w74']\n",
      " ['w75' 'w76' 'w77' 'w78' 'w79']\n",
      " ['w80' 'w81' 'w82' 'w83' 'w84']\n",
      " ['w85' 'w86' 'w87' 'w88' 'w89']\n",
      " ['w90' 'w91' 'w92' 'w93' 'w94']\n",
      " ['w95' 'w96' 'w97' 'w98' 'w99']]\n"
     ]
    }
   ],
   "source": [
    "n_rows = 20\n",
    "n_cols = 5\n",
    "vocab_mat = np.zeros((n_rows, n_cols), dtype=np.object)\n",
    "word_count = 0\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        vocab_mat[i, j] = 'w%s' % word_count\n",
    "        word_count += 1\n",
    "        \n",
    "print vocab_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10', 'w11', 'w12', 'w13', 'w14', 'w15', 'w16', 'w17', 'w18', 'w19', 'w20', 'w21', 'w22', 'w23', 'w24', 'w25', 'w26', 'w27', 'w28', 'w29', 'w30', 'w31', 'w32', 'w33', 'w34', 'w35', 'w36', 'w37', 'w38', 'w39', 'w40', 'w41', 'w42', 'w43', 'w44', 'w45', 'w46', 'w47', 'w48', 'w49', 'w50', 'w51', 'w52', 'w53', 'w54', 'w55', 'w56', 'w57', 'w58', 'w59', 'w60', 'w61', 'w62', 'w63', 'w64', 'w65', 'w66', 'w67', 'w68', 'w69', 'w70', 'w71', 'w72', 'w73', 'w74', 'w75', 'w76', 'w77', 'w78', 'w79', 'w80', 'w81', 'w82', 'w83', 'w84', 'w85', 'w86', 'w87', 'w88', 'w89', 'w90', 'w91', 'w92', 'w93', 'w94', 'w95', 'w96', 'w97', 'w98', 'w99']\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab_mat.flatten().tolist()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assign Documents to Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "node 0 (level=0, documents=100): \n",
      "    node 1 (level=1, documents=56): \n",
      "        node 2 (level=2, documents=23): \n",
      "        node 5 (level=2, documents=31): \n",
      "        node 10 (level=2, documents=2): \n",
      "    node 3 (level=1, documents=35): \n",
      "        node 4 (level=2, documents=28): \n",
      "        node 6 (level=2, documents=7): \n",
      "    node 7 (level=1, documents=9): \n",
      "        node 8 (level=2, documents=2): \n",
      "        node 9 (level=2, documents=7): \n"
     ]
    }
   ],
   "source": [
    "NCRPNode.total_nodes = 0\n",
    "NCRPNode.last_node_id = 0\n",
    "num_levels = 3\n",
    "gamma = 1\n",
    "num_docs = 100\n",
    "\n",
    "root_node = NCRPNode(num_levels, vocab)\n",
    "document_path = {}\n",
    "unique_nodes = set()\n",
    "unique_nodes.add(root_node)\n",
    "for d in range(num_docs):\n",
    "\n",
    "    # populate nodes into the path of this document\n",
    "    path = np.zeros(num_levels, dtype=np.object)\n",
    "    path[0] = root_node\n",
    "    root_node.customers += 1 # always add to the root node first\n",
    "    for level in range(1, num_levels):\n",
    "        # at each level, a node is selected by its parent node based on the CRP prior\n",
    "        parent_node = path[level-1]\n",
    "        level_node = parent_node.select(gamma)\n",
    "        level_node.customers += 1\n",
    "        path[level] = level_node\n",
    "        unique_nodes.add(level_node)\n",
    "\n",
    "    # set the leaf node for this document                 \n",
    "    document_path[d] = path\n",
    "    \n",
    "unique_nodes = sorted(unique_nodes, key=lambda x: x.node_id)\n",
    "print len(unique_nodes)\n",
    "    \n",
    "def print_node(node, indent, node_topic):\n",
    "    out = '    ' * indent\n",
    "    out += 'node %d (level=%d, documents=%d): ' % (node.node_id, node.level, node.customers)\n",
    "    if node in node_topic:\n",
    "        probs, words = node_topic[node]\n",
    "        out += ' '.join(words)\n",
    "    print out        \n",
    "    for child in node.children:\n",
    "        print_node(child, indent+1, node_topic)        \n",
    "\n",
    "node_topic = {}\n",
    "print_node(root_node, 0, node_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Assign Each Node Along the Tree to a Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00040692  0.03688672  0.09907082  0.02081315  0.0340949   0.06334867\n",
      "  0.05575553  0.00243414  0.01760897  0.09163744  0.00026633  0.0544931\n",
      "  0.04593761  0.20827443  0.01494573  0.00392205  0.05150066  0.07604774\n",
      "  0.02063409  0.10192099]\n",
      "['w0' 'w5' 'w10' 'w15' 'w20' 'w25' 'w30' 'w35' 'w40' 'w45' 'w50' 'w55'\n",
      " 'w60' 'w65' 'w70' 'w75' 'w80' 'w85' 'w90' 'w95']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def get_words(vocab_mat, eta, pos, dim):\n",
    "\n",
    "    if dim == 'row':\n",
    "        words = vocab_mat[pos]\n",
    "    elif dim == 'col':\n",
    "        words = vocab_mat[:, pos]\n",
    "    \n",
    "    k = len(words)\n",
    "    eta = [eta] * k\n",
    "    probs = np.random.dirichlet(eta)\n",
    "    return probs, words\n",
    "    \n",
    "pos = 0\n",
    "eta = 1\n",
    "probs, words = get_words(vocab_mat, eta, pos, 'col')\n",
    "print probs\n",
    "print words\n",
    "print np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "node_topic = {}\n",
    "node_topic[unique_nodes[0]] = get_words(vocab_mat, eta, 0, 'row') \n",
    "node_topic[unique_nodes[1]] = get_words(vocab_mat, eta, 1, 'row') \n",
    "node_topic[unique_nodes[2]] = get_words(vocab_mat, eta, 2, 'row') \n",
    "node_topic[unique_nodes[3]] = get_words(vocab_mat, eta, 3, 'row') \n",
    "node_topic[unique_nodes[4]] = get_words(vocab_mat, eta, 4, 'row') \n",
    "node_topic[unique_nodes[5]] = get_words(vocab_mat, eta, 5, 'row') \n",
    "node_topic[unique_nodes[6]] = get_words(vocab_mat, eta, 6, 'row') \n",
    "node_topic[unique_nodes[7]] = get_words(vocab_mat, eta, 7, 'row') \n",
    "node_topic[unique_nodes[8]] = get_words(vocab_mat, eta, 8, 'row') \n",
    "node_topic[unique_nodes[9]] = get_words(vocab_mat, eta, 9, 'row') \n",
    "node_topic[unique_nodes[10]] = get_words(vocab_mat, eta, 10, 'row') \n",
    "print len(node_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 0 (level=0, documents=100): w0 w1 w2 w3 w4\n",
      "    node 1 (level=1, documents=56): w5 w6 w7 w8 w9\n",
      "        node 2 (level=2, documents=23): w10 w11 w12 w13 w14\n",
      "        node 5 (level=2, documents=31): w25 w26 w27 w28 w29\n",
      "        node 10 (level=2, documents=2): w50 w51 w52 w53 w54\n",
      "    node 3 (level=1, documents=35): w15 w16 w17 w18 w19\n",
      "        node 4 (level=2, documents=28): w20 w21 w22 w23 w24\n",
      "        node 6 (level=2, documents=7): w30 w31 w32 w33 w34\n",
      "    node 7 (level=1, documents=9): w35 w36 w37 w38 w39\n",
      "        node 8 (level=2, documents=2): w40 w41 w42 w43 w44\n",
      "        node 9 (level=2, documents=7): w45 w46 w47 w48 w49\n"
     ]
    }
   ],
   "source": [
    "print_node(root_node, 0, node_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Words in a Document Based on Its Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_document(topics, theta, doc_len):\n",
    "\n",
    "    # for every word in the vocab for this document\n",
    "    doc = []\n",
    "    for n in range(doc_len):\n",
    "\n",
    "        # sample a new topic index    \n",
    "        k = np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "        # sample a new word from the word distribution of topic k\n",
    "        probs, words = topics[k]\n",
    "        w = np.random.multinomial(1, probs).argmax()\n",
    "        doc_word = words[w]\n",
    "\n",
    "        doc.append(doc_word)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "# alpha = [2.0, 1.0, 0.5]\n",
    "alpha = [1.0, 1.0, 1.0]\n",
    "doc_len = 50\n",
    "for d in range(num_docs):\n",
    "    path = document_path[d]\n",
    "    topics = [node_topic[node] for node in path]\n",
    "    theta = np.random.mtrand.dirichlet(alpha)\n",
    "    doc = generate_document(topics, theta, doc_len)\n",
    "    corpus.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "outdir = '/Users/joewandy/Dropbox/Analysis/hLDA/data/synthetic/'\n",
    "for d in range(len(corpus)):\n",
    "    doc = corpus[d]\n",
    "    file_name = 'doc_%d.txt' % d\n",
    "    file_path = os.path.join(outdir, file_name)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"%s\\n\" % ' '.join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run hLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 50\n"
     ]
    }
   ],
   "source": [
    "print len(vocab), len(corpus), len(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert corpus words into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for doc in corpus:\n",
    "    new_doc = []\n",
    "    for word in doc:\n",
    "        word_idx = vocab.index(word)\n",
    "        new_doc.append(word_idx)\n",
    "    new_corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "['w4', 'w13', 'w7', 'w4', 'w9', 'w4', 'w1', 'w4', 'w7', 'w6', 'w14', 'w13', 'w4', 'w4', 'w7', 'w8', 'w4', 'w3', 'w3', 'w2', 'w6', 'w4', 'w8', 'w2', 'w4', 'w14', 'w4', 'w4', 'w2', 'w4', 'w12', 'w4', 'w7', 'w4', 'w3', 'w2', 'w1', 'w10', 'w1', 'w7', 'w2', 'w12', 'w9', 'w13', 'w9', 'w3', 'w6', 'w2', 'w2', 'w10']\n",
      "[4, 13, 7, 4, 9, 4, 1, 4, 7, 6, 14, 13, 4, 4, 7, 8, 4, 3, 3, 2, 6, 4, 8, 2, 4, 14, 4, 4, 2, 4, 12, 4, 7, 4, 3, 2, 1, 10, 1, 7, 2, 12, 9, 13, 9, 3, 6, 2, 2, 10]\n"
     ]
    }
   ],
   "source": [
    "print len(vocab), len(new_corpus)\n",
    "print corpus[0]\n",
    "print new_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hlda.sampler import HierarchicalLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0] 1 1\n"
     ]
    }
   ],
   "source": [
    "print alpha, gamma, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalLDA sampling\n",
      "\n",
      ".......... 10\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w9, \n",
      "    topic=1 level=1 (documents=36): w4, w2, w3, w19, w17, \n",
      "        topic=2 level=2 (documents=27): w20, w24, w22, w17, w23, \n",
      "        topic=12 level=2 (documents=9): w12, w13, w6, w10, w14, \n",
      "    topic=4 level=1 (documents=64): w29, w7, w8, w6, w25, \n",
      "        topic=5 level=2 (documents=26): w6, w7, w9, w8, w5, \n",
      "        topic=6 level=2 (documents=19): w25, w29, w28, w26, w6, \n",
      "        topic=8 level=2 (documents=10): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=8): w49, w36, w39, w47, w46, \n",
      "        topic=23 level=2 (documents=1): w43, w41, w42, w2, w44, \n",
      "\n",
      ".......... 20\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=38): w50, w52, w6, w7, w17, \n",
      "        topic=2 level=2 (documents=28): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=9): w12, w13, w10, w14, w6, \n",
      "        topic=33 level=2 (documents=1): w43, w41, w42, w4, w44, \n",
      "    topic=4 level=1 (documents=62): w7, w25, w8, w29, w6, \n",
      "        topic=5 level=2 (documents=27): w6, w9, w7, w8, w12, \n",
      "        topic=6 level=2 (documents=18): w29, w25, w28, w26, w6, \n",
      "        topic=8 level=2 (documents=9): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=8): w49, w36, w39, w47, w46, \n",
      "\n",
      ".......... 30\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=36): w50, w6, w52, w7, w9, \n",
      "        topic=2 level=2 (documents=27): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=7): w12, w13, w10, w14, w6, \n",
      "        topic=43 level=2 (documents=2): w43, w41, w42, w44, w40, \n",
      "    topic=4 level=1 (documents=64): w6, w7, w9, w8, w29, \n",
      "        topic=5 level=2 (documents=28): w6, w7, w9, w12, w8, \n",
      "        topic=6 level=2 (documents=18): w25, w29, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=9): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=9): w49, w36, w39, w47, w35, \n",
      "\n",
      ".......... 40\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=35): w6, w9, w7, w8, w29, \n",
      "        topic=2 level=2 (documents=27): w24, w20, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=7): w12, w13, w10, w14, w6, \n",
      "        topic=53 level=2 (documents=1): w43, w41, w42, w3, w40, \n",
      "    topic=4 level=1 (documents=65): w6, w7, w9, w8, w25, \n",
      "        topic=5 level=2 (documents=25): w6, w7, w9, w12, w8, \n",
      "        topic=6 level=2 (documents=20): w29, w25, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=9): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=10): w49, w36, w39, w47, w35, \n",
      "        topic=54 level=2 (documents=1): w99, w36, w26, w27, w28, \n",
      "\n",
      ".......... 50\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w25, \n",
      "    topic=1 level=1 (documents=37): w7, w6, w9, w8, w5, \n",
      "        topic=2 level=2 (documents=27): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=10): w12, w13, w10, w14, w6, \n",
      "    topic=4 level=1 (documents=63): w6, w7, w9, w8, w5, \n",
      "        topic=5 level=2 (documents=20): w6, w9, w7, w12, w13, \n",
      "        topic=6 level=2 (documents=23): w29, w25, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=10): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=8): w49, w36, w39, w47, w35, \n",
      "        topic=61 level=2 (documents=2): w43, w41, w42, w54, w39, \n",
      "\n",
      ".......... 60\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=41): w6, w9, w7, w8, w50, \n",
      "        topic=2 level=2 (documents=28): w24, w20, w22, w17, w23, \n",
      "        topic=12 level=2 (documents=11): w12, w13, w10, w14, w11, \n",
      "        topic=72 level=2 (documents=2): w43, w41, w42, w2, w44, \n",
      "    topic=4 level=1 (documents=59): w6, w7, w9, w8, w5, \n",
      "        topic=5 level=2 (documents=13): w12, w6, w13, w7, w10, \n",
      "        topic=6 level=2 (documents=27): w29, w25, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=9): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=10): w49, w36, w39, w47, w46, \n",
      "\n",
      ".......... 70\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=52): w6, w9, w7, w8, w5, \n",
      "        topic=2 level=2 (documents=29): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=19): w12, w13, w10, w14, w11, \n",
      "        topic=81 level=2 (documents=4): w43, w41, w42, w40, w39, \n",
      "    topic=4 level=1 (documents=48): w6, w7, w9, w8, w5, \n",
      "        topic=6 level=2 (documents=29): w25, w29, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=9): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=8): w49, w36, w39, w47, w35, \n",
      "        topic=75 level=2 (documents=2): w50, w52, w53, w54, w3, \n",
      "\n",
      ".......... 80\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w29, \n",
      "    topic=1 level=1 (documents=52): w6, w7, w9, w8, w5, \n",
      "        topic=2 level=2 (documents=26): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=20): w12, w13, w10, w14, w11, \n",
      "        topic=88 level=2 (documents=5): w50, w52, w53, w6, w54, \n",
      "        topic=89 level=2 (documents=1): w43, w41, w42, w2, w44, \n",
      "    topic=4 level=1 (documents=48): w6, w7, w9, w8, w5, \n",
      "        topic=6 level=2 (documents=29): w25, w29, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=10): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=9): w49, w36, w39, w47, w35, \n",
      "\n",
      ".......... 90\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w9, \n",
      "    topic=1 level=1 (documents=47): w6, w7, w9, w8, w5, \n",
      "        topic=2 level=2 (documents=26): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=18): w12, w13, w10, w14, w11, \n",
      "        topic=88 level=2 (documents=3): w50, w52, w53, w54, w3, \n",
      "    topic=4 level=1 (documents=53): w6, w7, w9, w8, w5, \n",
      "        topic=6 level=2 (documents=30): w29, w25, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=11): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=9): w49, w36, w39, w47, w35, \n",
      "        topic=97 level=2 (documents=3): w43, w41, w42, w44, w39, \n",
      "\n",
      ".......... 100\n",
      "topic=0 level=0 (documents=100): w4, w3, w2, w1, w0, \n",
      "    topic=1 level=1 (documents=48): w6, w7, w9, w8, w12, \n",
      "        topic=2 level=2 (documents=28): w20, w24, w22, w17, w19, \n",
      "        topic=12 level=2 (documents=18): w12, w13, w10, w14, w11, \n",
      "        topic=105 level=2 (documents=2): w50, w52, w53, w54, w3, \n",
      "    topic=4 level=1 (documents=52): w6, w7, w9, w8, w5, \n",
      "        topic=6 level=2 (documents=30): w29, w25, w28, w26, w27, \n",
      "        topic=8 level=2 (documents=11): w17, w16, w19, w33, w15, \n",
      "        topic=9 level=2 (documents=8): w49, w36, w39, w47, w35, \n",
      "        topic=101 level=2 (documents=1): w99, w36, w26, w27, w28, \n",
      "        topic=103 level=2 (documents=2): w43, w41, w42, w2, w4, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "hlda = HierarchicalLDA(new_corpus, vocab, alpha=1, gamma=1.0, eta=1.0, num_levels=3)\n",
    "hlda.estimate(n_samples, display_topics=10, n_words=5, with_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
