{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and HLDA\n",
    "\n",
    "References:<br>\n",
    "https://github.com/lda-project/lda <br>\n",
    "https://datascience.blog.wzb.eu/2016/06/17/creating-a-sparse-document-term-matrix-for-topic-modeling-via-lda/ <br>\n",
    "https://github.com/joewandy/hlda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = stopwords.words('english') + [\"com\", \"www\", \"http\", \"url\", \"org\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "vocab = set()\n",
    "n_nonzero = 0\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "for filename in glob.glob('/home/juan/Documents/new_life/*.txt'):\n",
    "    with open(filename) as f:\n",
    "        try:\n",
    "            doc = f.read().splitlines()\n",
    "            doc = '. '.join(doc).translate(str.maketrans(dict.fromkeys(string.punctuation, \" \"))).translate(str.maketrans(dict.fromkeys('0123456789')))  # strip punctuations            \n",
    "            tokens = word_tokenize(str(doc))\n",
    "            filtered = []\n",
    "            for w in tokens:\n",
    "                w = stemmer.stem(w.lower())  # use Porter's stemmer\n",
    "                if len(w) < 3:  # remove short tokens\n",
    "                    continue\n",
    "                if w in stopset:  # remove stop words\n",
    "                    continue\n",
    "                filtered.append(w)\n",
    "            unique_terms = set(filtered)\n",
    "            n_nonzero += len(unique_terms)\n",
    "            vocab.update(unique_terms)\n",
    "            corpus.append(filtered)            \n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print ('Failed to load', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(list(vocab))  \n",
    "vocab_sorter = np.argsort(vocab)\n",
    "ndocs = len(corpus)\n",
    "nvocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.empty(n_nonzero, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "rows = np.empty(n_nonzero, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "cols = np.empty(n_nonzero, dtype=np.intc)     # column index for kth data item (kth term freq.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0     # current index in the sparse matrix data\n",
    "# go through all documents with their terms\n",
    "i = 0\n",
    "for terms in corpus:\n",
    "    # find indices into  such that, if the corresponding elements in  were\n",
    "    # inserted before the indices, the order of  would be preserved\n",
    "    # -> array of indices of  in \n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ind_end = ind + n_vals  #  to  is the slice that we will fill with data\n",
    "\n",
    "    data[ind:ind_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ind:ind_end] = uniq_indices            # save the column index: index in \n",
    "    rows[ind:ind_end] = np.repeat(i, n_vals)  # save it as repeated value\n",
    "    \n",
    "    i += 1\n",
    "    ind = ind_end  # resume with next document -> add data to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = coo_matrix((data, (rows, cols)), shape=(ndocs, nvocab), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/venv/lib/python3.7/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: dog cat breed align greyhound\n",
      "Topic 1: plant speci flower leav jpg\n",
      "Topic 2: bird speci statu iucn bat\n",
      "Topic 3: last first journal titl genu\n",
      "Topic 4: viru infect diseas cell use\n",
      "Topic 5: speci statu iucn author genu\n",
      "Topic 6: binomi speci beetl cerambycida insect\n",
      "Topic 7: speci author genu speciesbox famili\n",
      "Topic 8: speci marin clade imag famili\n",
      "Topic 9: speci new india imag zealand\n",
      "Topic 10: date state new web first\n",
      "Topic 11: film episod batman seri titl\n",
      "Topic 12: speci genu imag author fungi\n",
      "Topic 13: sfn book jpg file death\n",
      "Topic 14: convert australia abbr flower long\n",
      "Topic 15: nbsp brown white dark spot\n",
      "Topic 16: speci genu plant famili unrank\n",
      "Topic 17: imag moth binomi lepidoptera speci\n",
      "Topic 18: thi femal male also may\n",
      "Topic 19: speci fish statu author genu\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1000, random_state=1)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "model.fit(dtm)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 5\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(vocab))\n",
    "vocab_index = {}\n",
    "for i, w in enumerate(vocab):\n",
    "    vocab_index[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296042\n",
      "850757\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for doc in corpus:\n",
    "    new_doc = []\n",
    "    for word in doc:\n",
    "        word_idx = vocab_index[word]\n",
    "        new_doc.append(word_idx)\n",
    "    new_corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 1000       # no of iterations for the sampler\n",
    "alpha = 10.0          # smoothing over level distributions\n",
    "gamma = 1.0           # CRP smoothing parameter; number of imaginary customers at next, as yet unused table\n",
    "eta = 0.1             # smoothing over topic-word distributions\n",
    "num_levels = 30        # the number of levels in the tree\n",
    "display_topics = 200   # the number of iterations between printing a brief summary of the topics so far\n",
    "n_words = 5           # the number of most probable words to print for each topic after model estimation\n",
    "with_weights = False  # whether to print the words with the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f8a50f1227b9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from hlda import\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f8a50f1227b9>, line 1)",
     "output_type": "error"
    }
   ],
   "source": [
    "from hlda.sampler import HierarchicalLDA\n",
    "hlda = HierarchicalLDA(new_corpus, vocab, alpha=alpha, gamma=gamma, eta=eta, num_levels=num_levels)\n",
    "hlda.estimate(n_samples, display_topics=display_topics, n_words=n_words, with_weights=with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from ipywidgets import widgets\n",
    "from IPython.core.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = {\n",
    "    0: 'blue',\n",
    "    1: 'red',\n",
    "    2: 'green'\n",
    "}\n",
    "\n",
    "def show_doc(d=0):\n",
    "    node = hlda.document_leaves[d]\n",
    "    path = []\n",
    "    while node is not None:\n",
    "        path.append(node)\n",
    "        node = node.parent\n",
    "    path.reverse()\n",
    "\n",
    "    n_words = 10\n",
    "    with_weights = False\n",
    "    for n in range(len(path)):\n",
    "        node = path[n]\n",
    "        colour = colour_map[n]\n",
    "        msg = 'Level %d Topic %d: ' % (node.level, node.node_id)\n",
    "        msg += node.get_top_words(n_words, with_weights)\n",
    "        output = '<h%d><span style=\"color:%s\">%s</span></h3>' % (n + 1, colour, msg)\n",
    "        display(HTML(output))\n",
    "\n",
    "    display(HTML('<hr/><h5>Processed Document</h5>'))\n",
    "\n",
    "    doc = corpus[d]\n",
    "    output = ''\n",
    "    for n in range(len(doc)):\n",
    "        w = doc[n]\n",
    "        l = hlda.levels[d][n]\n",
    "        colour = colour_map[l]\n",
    "        output += '<span style=\"color:%s\">%s</span> ' % (colour, w)\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook locally, you'd be able to flip through the documents in the corpus and see the topic assignments of individual words of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(show_doc, d=(0, len(corpus)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dump the hlda object for further use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/18474791/decreasing-the-size-of-cpickle-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "\n",
    "def save_zipped_pickle(obj, filename, protocol=-1):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        cPickle.dump(obj, f, protocol)\n",
    "\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "save_zipped_pickle(hlda, 'bbc_hlda.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "03d4a94715d647d1ad07cd0817285e4f": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}