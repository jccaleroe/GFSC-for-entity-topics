// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

package nvidia.inferenceserver;

public final class ModelConfigOuterClass {
  private ModelConfigOuterClass() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: DataType
   *&#64;&#64;
   *&#64;&#64;   Data types supported for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code nvidia.inferenceserver.DataType}
   */
  public enum DataType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    TYPE_INVALID(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    TYPE_BOOL(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    TYPE_UINT8(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    TYPE_UINT16(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    TYPE_UINT32(4),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    TYPE_UINT64(5),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    TYPE_INT8(6),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    TYPE_INT16(7),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    TYPE_INT32(8),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    TYPE_INT64(9),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    TYPE_FP16(10),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    TYPE_FP32(11),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    TYPE_FP64(12),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    TYPE_STRING(13),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    public static final int TYPE_INVALID_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    public static final int TYPE_BOOL_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    public static final int TYPE_UINT8_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    public static final int TYPE_UINT16_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    public static final int TYPE_UINT32_VALUE = 4;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    public static final int TYPE_UINT64_VALUE = 5;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    public static final int TYPE_INT8_VALUE = 6;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    public static final int TYPE_INT16_VALUE = 7;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    public static final int TYPE_INT32_VALUE = 8;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    public static final int TYPE_INT64_VALUE = 9;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    public static final int TYPE_FP16_VALUE = 10;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    public static final int TYPE_FP32_VALUE = 11;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    public static final int TYPE_FP64_VALUE = 12;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    public static final int TYPE_STRING_VALUE = 13;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static DataType valueOf(int value) {
      return forNumber(value);
    }

    public static DataType forNumber(int value) {
      switch (value) {
        case 0: return TYPE_INVALID;
        case 1: return TYPE_BOOL;
        case 2: return TYPE_UINT8;
        case 3: return TYPE_UINT16;
        case 4: return TYPE_UINT32;
        case 5: return TYPE_UINT64;
        case 6: return TYPE_INT8;
        case 7: return TYPE_INT16;
        case 8: return TYPE_INT32;
        case 9: return TYPE_INT64;
        case 10: return TYPE_FP16;
        case 11: return TYPE_FP32;
        case 12: return TYPE_FP64;
        case 13: return TYPE_STRING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<DataType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        DataType> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<DataType>() {
            public DataType findValueByNumber(int number) {
              return DataType.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.getDescriptor().getEnumTypes().get(0);
    }

    private static final DataType[] VALUES = values();

    public static DataType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private DataType(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.DataType)
  }

  public interface ModelInstanceGroupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelInstanceGroup)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     */
    int getCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    java.util.List<java.lang.Integer> getGpusList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    int getGpusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    int getGpus(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
   *&#64;&#64;
   *&#64;&#64;   A group of one or more instances of a model and resources made
   *&#64;&#64;   available for those instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelInstanceGroup}
   */
  public  static final class ModelInstanceGroup extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelInstanceGroup)
      ModelInstanceGroupOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInstanceGroup.newBuilder() to construct.
    private ModelInstanceGroup(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInstanceGroup() {
      name_ = "";
      kind_ = 0;
      gpus_ = emptyIntList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelInstanceGroup();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInstanceGroup(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              count_ = input.readInt32();
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                gpus_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              gpus_.addInt(input.readInt32());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                gpus_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                gpus_.addInt(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 32: {
              int rawValue = input.readEnum();

              kind_ = rawValue;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          gpus_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     Kind of this instance group.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelInstanceGroup.Kind}
     */
    public enum Kind
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      KIND_AUTO(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      KIND_GPU(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      KIND_CPU(2),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      KIND_MODEL(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      public static final int KIND_AUTO_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      public static final int KIND_GPU_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      public static final int KIND_CPU_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      public static final int KIND_MODEL_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return KIND_AUTO;
          case 1: return KIND_GPU;
          case 2: return KIND_CPU;
          case 3: return KIND_MODEL;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDescriptor().getEnumTypes().get(0);
      }

      private static final Kind[] VALUES = values();

      public static Kind valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelInstanceGroup.Kind)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int KIND_FIELD_NUMBER = 4;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
    }

    public static final int COUNT_FIELD_NUMBER = 2;
    private int count_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     */
    public int getCount() {
      return count_;
    }

    public static final int GPUS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.IntList gpus_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public java.util.List<java.lang.Integer>
        getGpusList() {
      return gpus_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public int getGpusCount() {
      return gpus_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public int getGpus(int index) {
      return gpus_.getInt(index);
    }
    private int gpusMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (count_ != 0) {
        output.writeInt32(2, count_);
      }
      if (getGpusList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(gpusMemoizedSerializedSize);
      }
      for (int i = 0; i < gpus_.size(); i++) {
        output.writeInt32NoTag(gpus_.getInt(i));
      }
      if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        output.writeEnum(4, kind_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (count_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, count_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < gpus_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(gpus_.getInt(i));
        }
        size += dataSize;
        if (!getGpusList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        gpusMemoizedSerializedSize = dataSize;
      }
      if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(4, kind_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (kind_ != other.kind_) return false;
      if (getCount()
          != other.getCount()) return false;
      if (!getGpusList()
          .equals(other.getGpusList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + KIND_FIELD_NUMBER;
      hash = (53 * hash) + kind_;
      hash = (37 * hash) + COUNT_FIELD_NUMBER;
      hash = (53 * hash) + getCount();
      if (getGpusCount() > 0) {
        hash = (37 * hash) + GPUS_FIELD_NUMBER;
        hash = (53 * hash) + getGpusList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
     *&#64;&#64;
     *&#64;&#64;   A group of one or more instances of a model and resources made
     *&#64;&#64;   available for those instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelInstanceGroup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelInstanceGroup)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        kind_ = 0;

        count_ = 0;

        gpus_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.kind_ = kind_;
        result.count_ = count_;
        if (((bitField0_ & 0x00000001) != 0)) {
          gpus_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.gpus_ = gpus_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.kind_ != 0) {
          setKindValue(other.getKindValue());
        }
        if (other.getCount() != 0) {
          setCount(other.getCount());
        }
        if (!other.gpus_.isEmpty()) {
          if (gpus_.isEmpty()) {
            gpus_ = other.gpus_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureGpusIsMutable();
            gpus_.addAll(other.gpus_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int kind_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder setKindValue(int value) {
        kind_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder setKind(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        kind_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder clearKind() {
        
        kind_ = 0;
        onChanged();
        return this;
      }

      private int count_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public int getCount() {
        return count_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public Builder setCount(int value) {
        
        count_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public Builder clearCount() {
        
        count_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.IntList gpus_ = emptyIntList();
      private void ensureGpusIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          gpus_ = mutableCopy(gpus_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public java.util.List<java.lang.Integer>
          getGpusList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(gpus_) : gpus_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public int getGpusCount() {
        return gpus_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public int getGpus(int index) {
        return gpus_.getInt(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder setGpus(
          int index, int value) {
        ensureGpusIsMutable();
        gpus_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder addGpus(int value) {
        ensureGpusIsMutable();
        gpus_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder addAllGpus(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureGpusIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, gpus_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder clearGpus() {
        gpus_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelInstanceGroup)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInstanceGroup>
        PARSER = new com.google.protobuf.AbstractParser<ModelInstanceGroup>() {
      @java.lang.Override
      public ModelInstanceGroup parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInstanceGroup(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInstanceGroup> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInstanceGroup> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelTensorReshapeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelTensorReshape)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    java.util.List<java.lang.Long> getShapeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    int getShapeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    long getShape(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelTensorReshape
   *&#64;&#64;
   *&#64;&#64;   Reshape specification for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelTensorReshape}
   */
  public  static final class ModelTensorReshape extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelTensorReshape)
      ModelTensorReshapeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelTensorReshape.newBuilder() to construct.
    private ModelTensorReshape(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelTensorReshape() {
      shape_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelTensorReshape();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelTensorReshape(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                shape_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              shape_.addLong(input.readInt64());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                shape_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                shape_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          shape_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
    }

    public static final int SHAPE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.LongList shape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public java.util.List<java.lang.Long>
        getShapeList() {
      return shape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public int getShapeCount() {
      return shape_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public long getShape(int index) {
      return shape_.getLong(index);
    }
    private int shapeMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getShapeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(shapeMemoizedSerializedSize);
      }
      for (int i = 0; i < shape_.size(); i++) {
        output.writeInt64NoTag(shape_.getLong(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < shape_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(shape_.getLong(i));
        }
        size += dataSize;
        if (!getShapeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shapeMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) obj;

      if (!getShapeList()
          .equals(other.getShapeList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getShapeCount() > 0) {
        hash = (37 * hash) + SHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getShapeList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelTensorReshape
     *&#64;&#64;
     *&#64;&#64;   Reshape specification for input and output tensors.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelTensorReshape}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelTensorReshape)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        shape_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          shape_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.shape_ = shape_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance()) return this;
        if (!other.shape_.isEmpty()) {
          if (shape_.isEmpty()) {
            shape_ = other.shape_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureShapeIsMutable();
            shape_.addAll(other.shape_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.LongList shape_ = emptyLongList();
      private void ensureShapeIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          shape_ = mutableCopy(shape_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public java.util.List<java.lang.Long>
          getShapeList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(shape_) : shape_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public int getShapeCount() {
        return shape_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public long getShape(int index) {
        return shape_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder setShape(
          int index, long value) {
        ensureShapeIsMutable();
        shape_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder addShape(long value) {
        ensureShapeIsMutable();
        shape_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder addAllShape(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureShapeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shape_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder clearShape() {
        shape_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelTensorReshape)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelTensorReshape)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelTensorReshape>
        PARSER = new com.google.protobuf.AbstractParser<ModelTensorReshape>() {
      @java.lang.Override
      public ModelTensorReshape parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelTensorReshape(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelTensorReshape> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelTensorReshape> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelInput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    int getFormatValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInput
   *&#64;&#64;
   *&#64;&#64;   An input required by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelInput}
   */
  public  static final class ModelInput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelInput)
      ModelInputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInput.newBuilder() to construct.
    private ModelInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInput() {
      name_ = "";
      dataType_ = 0;
      format_ = 0;
      dims_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelInput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();

              format_ = rawValue;
              break;
            }
            case 32: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              dims_.addLong(input.readInt64());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 42: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Format
     *&#64;&#64;
     *&#64;&#64;     The format for the input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelInput.Format}
     */
    public enum Format
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      FORMAT_NONE(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      FORMAT_NHWC(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      FORMAT_NCHW(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      public static final int FORMAT_NONE_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      public static final int FORMAT_NHWC_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      public static final int FORMAT_NCHW_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Format valueOf(int value) {
        return forNumber(value);
      }

      public static Format forNumber(int value) {
        switch (value) {
          case 0: return FORMAT_NONE;
          case 1: return FORMAT_NHWC;
          case 2: return FORMAT_NCHW;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Format>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Format> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Format>() {
              public Format findValueByNumber(int number) {
                return Format.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDescriptor().getEnumTypes().get(0);
      }

      private static final Format[] VALUES = values();

      public static Format valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Format(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelInput.Format)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int FORMAT_FIELD_NUMBER = 3;
    private int format_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    public int getFormatValue() {
      return format_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (format_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        output.writeEnum(3, format_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.getLong(i));
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      if (format_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, format_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.getLong(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInput)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (dataType_ != other.dataType_) return false;
      if (format_ != other.format_) return false;
      if (!getDimsList()
          .equals(other.getDimsList())) return false;
      if (hasReshape() != other.hasReshape()) return false;
      if (hasReshape()) {
        if (!getReshape()
            .equals(other.getReshape())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      hash = (37 * hash) + FORMAT_FIELD_NUMBER;
      hash = (53 * hash) + format_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInput
     *&#64;&#64;
     *&#64;&#64;   An input required by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelInput)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        format_ = 0;

        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInput(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.dataType_ = dataType_;
        result.format_ = format_;
        if (((bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelInput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (other.format_ != 0) {
          setFormatValue(other.getFormatValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataTypeValue(int value) {
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private int format_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public int getFormatValue() {
        return format_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder setFormatValue(int value) {
        format_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder setFormat(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        format_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder clearFormat() {
        
        format_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          dims_ = mutableCopy(dims_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(dims_) : dims_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder clearDims() {
        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelInput)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelInput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInput();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInput>
        PARSER = new com.google.protobuf.AbstractParser<ModelInput>() {
      @java.lang.Override
      public ModelInput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOutput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    java.lang.String getLabelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    com.google.protobuf.ByteString
        getLabelFilenameBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOutput
   *&#64;&#64;
   *&#64;&#64;   An output produced by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelOutput}
   */
  public  static final class ModelOutput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOutput)
      ModelOutputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOutput.newBuilder() to construct.
    private ModelOutput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOutput() {
      name_ = "";
      dataType_ = 0;
      dims_ = emptyLongList();
      labelFilename_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelOutput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOutput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              dims_.addLong(input.readInt64());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();

              labelFilename_ = s;
              break;
            }
            case 42: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    public static final int LABEL_FILENAME_FIELD_NUMBER = 4;
    private volatile java.lang.Object labelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    public java.lang.String getLabelFilename() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        labelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    public com.google.protobuf.ByteString
        getLabelFilenameBytes() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        labelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.getLong(i));
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, labelFilename_);
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.getLong(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, labelFilename_);
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (dataType_ != other.dataType_) return false;
      if (!getDimsList()
          .equals(other.getDimsList())) return false;
      if (hasReshape() != other.hasReshape()) return false;
      if (hasReshape()) {
        if (!getReshape()
            .equals(other.getReshape())) return false;
      }
      if (!getLabelFilename()
          .equals(other.getLabelFilename())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (37 * hash) + LABEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getLabelFilename().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOutput
     *&#64;&#64;
     *&#64;&#64;   An output produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOutput)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        labelFilename_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.dataType_ = dataType_;
        if (((bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        result.labelFilename_ = labelFilename_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        if (!other.getLabelFilename().isEmpty()) {
          labelFilename_ = other.labelFilename_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataTypeValue(int value) {
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          dims_ = mutableCopy(dims_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(dims_) : dims_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder clearDims() {
        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }

      private java.lang.Object labelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public java.lang.String getLabelFilename() {
        java.lang.Object ref = labelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          labelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public com.google.protobuf.ByteString
          getLabelFilenameBytes() {
        java.lang.Object ref = labelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          labelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder setLabelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        labelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder clearLabelFilename() {
        
        labelFilename_ = getDefaultInstance().getLabelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder setLabelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        labelFilename_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOutput)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOutput>
        PARSER = new com.google.protobuf.AbstractParser<ModelOutput>() {
      @java.lang.Override
      public ModelOutput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOutput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOutput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOutput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelVersionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    boolean hasLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    boolean hasAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    boolean hasSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder();

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.PolicyChoiceCase getPolicyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
   *&#64;&#64;
   *&#64;&#64;   Policy indicating which versions of a model should be made
   *&#64;&#64;   available by the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy}
   */
  public  static final class ModelVersionPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy)
      ModelVersionPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelVersionPolicy.newBuilder() to construct.
    private ModelVersionPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelVersionPolicy() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelVersionPolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelVersionPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder subBuilder = null;
              if (policyChoiceCase_ == 1) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 1;
              break;
            }
            case 18: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder subBuilder = null;
              if (policyChoiceCase_ == 2) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 2;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder subBuilder = null;
              if (policyChoiceCase_ == 3) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 3;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
    }

    public interface LatestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.Latest)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       */
      int getNumVersions();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Latest
     *&#64;&#64;
     *&#64;&#64;     Serve only the latest version(s) of a model. This is
     *&#64;&#64;     the default policy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Latest}
     */
    public  static final class Latest extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.Latest)
        LatestOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Latest.newBuilder() to construct.
      private Latest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Latest() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Latest();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Latest(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                numVersions_ = input.readUInt32();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
      }

      public static final int NUM_VERSIONS_FIELD_NUMBER = 1;
      private int numVersions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       */
      public int getNumVersions() {
        return numVersions_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (numVersions_ != 0) {
          output.writeUInt32(1, numVersions_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (numVersions_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, numVersions_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) obj;

        if (getNumVersions()
            != other.getNumVersions()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NUM_VERSIONS_FIELD_NUMBER;
        hash = (53 * hash) + getNumVersions();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Latest
       *&#64;&#64;
       *&#64;&#64;     Serve only the latest version(s) of a model. This is
       *&#64;&#64;     the default policy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Latest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.Latest)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          numVersions_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest(this);
          result.numVersions_ = numVersions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) return this;
          if (other.getNumVersions() != 0) {
            setNumVersions(other.getNumVersions());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int numVersions_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public int getNumVersions() {
          return numVersions_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public Builder setNumVersions(int value) {
          
          numVersions_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public Builder clearNumVersions() {
          
          numVersions_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Latest>
          PARSER = new com.google.protobuf.AbstractParser<Latest>() {
        @java.lang.Override
        public Latest parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Latest(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Latest> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Latest> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface AllOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.All)
        com.google.protobuf.MessageOrBuilder {
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message All
     *&#64;&#64;
     *&#64;&#64;     Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.All}
     */
    public  static final class All extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.All)
        AllOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use All.newBuilder() to construct.
      private All(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private All() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new All();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private All(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) obj;

        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message All
       *&#64;&#64;
       *&#64;&#64;     Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.All}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.All)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All(this);
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) return this;
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<All>
          PARSER = new com.google.protobuf.AbstractParser<All>() {
        @java.lang.Override
        public All parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new All(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<All> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<All> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface SpecificOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.Specific)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      java.util.List<java.lang.Long> getVersionsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      int getVersionsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      long getVersions(int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Specific
     *&#64;&#64;
     *&#64;&#64;     Serve only specific versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Specific}
     */
    public  static final class Specific extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.Specific)
        SpecificOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Specific.newBuilder() to construct.
      private Specific(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Specific() {
        versions_ = emptyLongList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Specific();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Specific(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  versions_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                versions_.addLong(input.readInt64());
                break;
              }
              case 10: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  versions_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  versions_.addLong(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            versions_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
      }

      public static final int VERSIONS_FIELD_NUMBER = 1;
      private com.google.protobuf.Internal.LongList versions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public java.util.List<java.lang.Long>
          getVersionsList() {
        return versions_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public int getVersionsCount() {
        return versions_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public long getVersions(int index) {
        return versions_.getLong(index);
      }
      private int versionsMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getVersionsList().size() > 0) {
          output.writeUInt32NoTag(10);
          output.writeUInt32NoTag(versionsMemoizedSerializedSize);
        }
        for (int i = 0; i < versions_.size(); i++) {
          output.writeInt64NoTag(versions_.getLong(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          for (int i = 0; i < versions_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(versions_.getLong(i));
          }
          size += dataSize;
          if (!getVersionsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          versionsMemoizedSerializedSize = dataSize;
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) obj;

        if (!getVersionsList()
            .equals(other.getVersionsList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getVersionsCount() > 0) {
          hash = (37 * hash) + VERSIONS_FIELD_NUMBER;
          hash = (53 * hash) + getVersionsList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Specific
       *&#64;&#64;
       *&#64;&#64;     Serve only specific versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Specific}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.Specific)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          versions_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific(this);
          int from_bitField0_ = bitField0_;
          if (((bitField0_ & 0x00000001) != 0)) {
            versions_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.versions_ = versions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) return this;
          if (!other.versions_.isEmpty()) {
            if (versions_.isEmpty()) {
              versions_ = other.versions_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureVersionsIsMutable();
              versions_.addAll(other.versions_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private com.google.protobuf.Internal.LongList versions_ = emptyLongList();
        private void ensureVersionsIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            versions_ = mutableCopy(versions_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public java.util.List<java.lang.Long>
            getVersionsList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(versions_) : versions_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public int getVersionsCount() {
          return versions_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public long getVersions(int index) {
          return versions_.getLong(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder setVersions(
            int index, long value) {
          ensureVersionsIsMutable();
          versions_.setLong(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder addVersions(long value) {
          ensureVersionsIsMutable();
          versions_.addLong(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder addAllVersions(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureVersionsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, versions_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder clearVersions() {
          versions_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Specific>
          PARSER = new com.google.protobuf.AbstractParser<Specific>() {
        @java.lang.Override
        public Specific parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Specific(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Specific> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Specific> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int policyChoiceCase_ = 0;
    private java.lang.Object policyChoice_;
    public enum PolicyChoiceCase
        implements com.google.protobuf.Internal.EnumLite {
      LATEST(1),
      ALL(2),
      SPECIFIC(3),
      POLICYCHOICE_NOT_SET(0);
      private final int value;
      private PolicyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static PolicyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static PolicyChoiceCase forNumber(int value) {
        switch (value) {
          case 1: return LATEST;
          case 2: return ALL;
          case 3: return SPECIFIC;
          case 0: return POLICYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public PolicyChoiceCase
    getPolicyChoiceCase() {
      return PolicyChoiceCase.forNumber(
          policyChoiceCase_);
    }

    public static final int LATEST_FIELD_NUMBER = 1;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public boolean hasLatest() {
      return policyChoiceCase_ == 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
      if (policyChoiceCase_ == 1) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
      if (policyChoiceCase_ == 1) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }

    public static final int ALL_FIELD_NUMBER = 2;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public boolean hasAll() {
      return policyChoiceCase_ == 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
      if (policyChoiceCase_ == 2) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
      if (policyChoiceCase_ == 2) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }

    public static final int SPECIFIC_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public boolean hasSpecific() {
      return policyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
      if (policyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
      if (policyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (policyChoiceCase_ == 1) {
        output.writeMessage(1, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        output.writeMessage(2, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        output.writeMessage(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (policyChoiceCase_ == 1) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) obj;

      if (!getPolicyChoiceCase().equals(other.getPolicyChoiceCase())) return false;
      switch (policyChoiceCase_) {
        case 1:
          if (!getLatest()
              .equals(other.getLatest())) return false;
          break;
        case 2:
          if (!getAll()
              .equals(other.getAll())) return false;
          break;
        case 3:
          if (!getSpecific()
              .equals(other.getSpecific())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (policyChoiceCase_) {
        case 1:
          hash = (37 * hash) + LATEST_FIELD_NUMBER;
          hash = (53 * hash) + getLatest().hashCode();
          break;
        case 2:
          hash = (37 * hash) + ALL_FIELD_NUMBER;
          hash = (53 * hash) + getAll().hashCode();
          break;
        case 3:
          hash = (37 * hash) + SPECIFIC_FIELD_NUMBER;
          hash = (53 * hash) + getSpecific().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
     *&#64;&#64;
     *&#64;&#64;   Policy indicating which versions of a model should be made
     *&#64;&#64;   available by the inference server.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy(this);
        if (policyChoiceCase_ == 1) {
          if (latestBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = latestBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 2) {
          if (allBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = allBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 3) {
          if (specificBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = specificBuilder_.build();
          }
        }
        result.policyChoiceCase_ = policyChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance()) return this;
        switch (other.getPolicyChoiceCase()) {
          case LATEST: {
            mergeLatest(other.getLatest());
            break;
          }
          case ALL: {
            mergeAll(other.getAll());
            break;
          }
          case SPECIFIC: {
            mergeSpecific(other.getSpecific());
            break;
          }
          case POLICYCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int policyChoiceCase_ = 0;
      private java.lang.Object policyChoice_;
      public PolicyChoiceCase
          getPolicyChoiceCase() {
        return PolicyChoiceCase.forNumber(
            policyChoiceCase_);
      }

      public Builder clearPolicyChoice() {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        onChanged();
        return this;
      }


      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> latestBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public boolean hasLatest() {
        return policyChoiceCase_ == 1;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 1) {
            return latestBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder builderForValue) {
        if (latestBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          latestBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder mergeLatest(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 1) {
            latestBuilder_.mergeFrom(value);
          }
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder clearLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          latestBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder getLatestBuilder() {
        return getLatestFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
        if ((policyChoiceCase_ == 1) && (latestBuilder_ != null)) {
          return latestBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 1) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> 
          getLatestFieldBuilder() {
        if (latestBuilder_ == null) {
          if (!(policyChoiceCase_ == 1)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
          }
          latestBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 1;
        onChanged();;
        return latestBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> allBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public boolean hasAll() {
        return policyChoiceCase_ == 2;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 2) {
            return allBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder builderForValue) {
        if (allBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          allBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder mergeAll(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 2) {
            allBuilder_.mergeFrom(value);
          }
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder clearAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          allBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder getAllBuilder() {
        return getAllFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
        if ((policyChoiceCase_ == 2) && (allBuilder_ != null)) {
          return allBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 2) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> 
          getAllFieldBuilder() {
        if (allBuilder_ == null) {
          if (!(policyChoiceCase_ == 2)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
          }
          allBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 2;
        onChanged();;
        return allBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> specificBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public boolean hasSpecific() {
        return policyChoiceCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 3) {
            return specificBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder builderForValue) {
        if (specificBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          specificBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder mergeSpecific(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 3) {
            specificBuilder_.mergeFrom(value);
          }
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder clearSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          specificBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder getSpecificBuilder() {
        return getSpecificFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
        if ((policyChoiceCase_ == 3) && (specificBuilder_ != null)) {
          return specificBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> 
          getSpecificFieldBuilder() {
        if (specificBuilder_ == null) {
          if (!(policyChoiceCase_ == 3)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
          }
          specificBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 3;
        onChanged();;
        return specificBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelVersionPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelVersionPolicy>() {
      @java.lang.Override
      public ModelVersionPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelVersionPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelVersionPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelVersionPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOptimizationPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    boolean hasGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    int getPriorityValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    boolean hasCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
   *&#64;&#64;
   *&#64;&#64;   Optimization settings for a model. These settings control if/how a
   *&#64;&#64;   model is optimized and prioritized by the backend framework when
   *&#64;&#64;   it is loaded.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy}
   */
  public  static final class ModelOptimizationPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy)
      ModelOptimizationPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOptimizationPolicy.newBuilder() to construct.
    private ModelOptimizationPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOptimizationPolicy() {
      priority_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelOptimizationPolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOptimizationPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder subBuilder = null;
              if (graph_ != null) {
                subBuilder = graph_.toBuilder();
              }
              graph_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(graph_);
                graph_ = subBuilder.buildPartial();
              }

              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              priority_ = rawValue;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder subBuilder = null;
              if (cuda_ != null) {
                subBuilder = cuda_.toBuilder();
              }
              cuda_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cuda_);
                cuda_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: ModelPriority
     *&#64;&#64;
     *&#64;&#64;     Model priorities. A model will be given scheduling and execution
     *&#64;&#64;     preference over models at lower priorities. Current model
     *&#64;&#64;     priorities only work for TensorRT models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority}
     */
    public enum ModelPriority
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      PRIORITY_DEFAULT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      PRIORITY_MAX(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      PRIORITY_MIN(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      public static final int PRIORITY_DEFAULT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      public static final int PRIORITY_MAX_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      public static final int PRIORITY_MIN_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static ModelPriority valueOf(int value) {
        return forNumber(value);
      }

      public static ModelPriority forNumber(int value) {
        switch (value) {
          case 0: return PRIORITY_DEFAULT;
          case 1: return PRIORITY_MAX;
          case 2: return PRIORITY_MIN;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ModelPriority>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ModelPriority> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ModelPriority>() {
              public ModelPriority findValueByNumber(int number) {
                return ModelPriority.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDescriptor().getEnumTypes().get(0);
      }

      private static final ModelPriority[] VALUES = values();

      public static ModelPriority valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private ModelPriority(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority)
    }

    public interface GraphOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       */
      int getLevel();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Graph
     *&#64;&#64;
     *&#64;&#64;     Enable generic graph optimization of the model. If not specified
     *&#64;&#64;     the framework's default level of optimization is used. Currently
     *&#64;&#64;     only supported for TensorFlow graphdef and savedmodel models and
     *&#64;&#64;     causes XLA to be enabled/disabled for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Graph}
     */
    public  static final class Graph extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
        GraphOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Graph.newBuilder() to construct.
      private Graph(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Graph() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Graph();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Graph(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                level_ = input.readInt32();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
      }

      public static final int LEVEL_FIELD_NUMBER = 1;
      private int level_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       */
      public int getLevel() {
        return level_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (level_ != 0) {
          output.writeInt32(1, level_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (level_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt32Size(1, level_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) obj;

        if (getLevel()
            != other.getLevel()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getLevel();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Graph
       *&#64;&#64;
       *&#64;&#64;     Enable generic graph optimization of the model. If not specified
       *&#64;&#64;     the framework's default level of optimization is used. Currently
       *&#64;&#64;     only supported for TensorFlow graphdef and savedmodel models and
       *&#64;&#64;     causes XLA to be enabled/disabled for the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Graph}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          level_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph(this);
          result.level_ = level_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance()) return this;
          if (other.getLevel() != 0) {
            setLevel(other.getLevel());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int level_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public int getLevel() {
          return level_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public Builder setLevel(int value) {
          
          level_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public Builder clearLevel() {
          
          level_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Graph>
          PARSER = new com.google.protobuf.AbstractParser<Graph>() {
        @java.lang.Override
        public Graph parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Graph(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Graph> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Graph> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface CudaOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
       *&#64;&#64;       backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       */
      boolean getGraphs();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Cuda}
     */
    public  static final class Cuda extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
        CudaOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Cuda.newBuilder() to construct.
      private Cuda(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Cuda() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Cuda();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Cuda(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                graphs_ = input.readBool();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
      }

      public static final int GRAPHS_FIELD_NUMBER = 1;
      private boolean graphs_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
       *&#64;&#64;       backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       */
      public boolean getGraphs() {
        return graphs_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (graphs_ != false) {
          output.writeBool(1, graphs_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (graphs_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(1, graphs_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) obj;

        if (getGraphs()
            != other.getGraphs()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + GRAPHS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getGraphs());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Cuda}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          graphs_ = false;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda(this);
          result.graphs_ = graphs_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance()) return this;
          if (other.getGraphs() != false) {
            setGraphs(other.getGraphs());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private boolean graphs_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public boolean getGraphs() {
          return graphs_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public Builder setGraphs(boolean value) {
          
          graphs_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public Builder clearGraphs() {
          
          graphs_ = false;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Cuda>
          PARSER = new com.google.protobuf.AbstractParser<Cuda>() {
        @java.lang.Override
        public Cuda parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Cuda(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Cuda> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Cuda> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int GRAPH_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public boolean hasGraph() {
      return graph_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
      return graph_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
      return getGraph();
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    public int getPriorityValue() {
      return priority_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
    }

    public static final int CUDA_FIELD_NUMBER = 3;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public boolean hasCuda() {
      return cuda_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
      return cuda_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
      return getCuda();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (graph_ != null) {
        output.writeMessage(1, getGraph());
      }
      if (priority_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        output.writeEnum(2, priority_);
      }
      if (cuda_ != null) {
        output.writeMessage(3, getCuda());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (graph_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getGraph());
      }
      if (priority_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, priority_);
      }
      if (cuda_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getCuda());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) obj;

      if (hasGraph() != other.hasGraph()) return false;
      if (hasGraph()) {
        if (!getGraph()
            .equals(other.getGraph())) return false;
      }
      if (priority_ != other.priority_) return false;
      if (hasCuda() != other.hasCuda()) return false;
      if (hasCuda()) {
        if (!getCuda()
            .equals(other.getCuda())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasGraph()) {
        hash = (37 * hash) + GRAPH_FIELD_NUMBER;
        hash = (53 * hash) + getGraph().hashCode();
      }
      hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
      hash = (53 * hash) + priority_;
      if (hasCuda()) {
        hash = (37 * hash) + CUDA_FIELD_NUMBER;
        hash = (53 * hash) + getCuda().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
     *&#64;&#64;
     *&#64;&#64;   Optimization settings for a model. These settings control if/how a
     *&#64;&#64;   model is optimized and prioritized by the backend framework when
     *&#64;&#64;   it is loaded.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (graphBuilder_ == null) {
          graph_ = null;
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }
        priority_ = 0;

        if (cudaBuilder_ == null) {
          cuda_ = null;
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy(this);
        if (graphBuilder_ == null) {
          result.graph_ = graph_;
        } else {
          result.graph_ = graphBuilder_.build();
        }
        result.priority_ = priority_;
        if (cudaBuilder_ == null) {
          result.cuda_ = cuda_;
        } else {
          result.cuda_ = cudaBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance()) return this;
        if (other.hasGraph()) {
          mergeGraph(other.getGraph());
        }
        if (other.priority_ != 0) {
          setPriorityValue(other.getPriorityValue());
        }
        if (other.hasCuda()) {
          mergeCuda(other.getCuda());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> graphBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public boolean hasGraph() {
        return graphBuilder_ != null || graph_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
        if (graphBuilder_ == null) {
          return graph_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        } else {
          return graphBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          graph_ = value;
          onChanged();
        } else {
          graphBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder builderForValue) {
        if (graphBuilder_ == null) {
          graph_ = builderForValue.build();
          onChanged();
        } else {
          graphBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder mergeGraph(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (graph_ != null) {
            graph_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder(graph_).mergeFrom(value).buildPartial();
          } else {
            graph_ = value;
          }
          onChanged();
        } else {
          graphBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder clearGraph() {
        if (graphBuilder_ == null) {
          graph_ = null;
          onChanged();
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder getGraphBuilder() {
        
        onChanged();
        return getGraphFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
        if (graphBuilder_ != null) {
          return graphBuilder_.getMessageOrBuilder();
        } else {
          return graph_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> 
          getGraphFieldBuilder() {
        if (graphBuilder_ == null) {
          graphBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder>(
                  getGraph(),
                  getParentForChildren(),
                  isClean());
          graph_ = null;
        }
        return graphBuilder_;
      }

      private int priority_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public int getPriorityValue() {
        return priority_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder setPriorityValue(int value) {
        priority_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder setPriority(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        priority_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder clearPriority() {
        
        priority_ = 0;
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> cudaBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public boolean hasCuda() {
        return cudaBuilder_ != null || cuda_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
        if (cudaBuilder_ == null) {
          return cuda_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        } else {
          return cudaBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cuda_ = value;
          onChanged();
        } else {
          cudaBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder builderForValue) {
        if (cudaBuilder_ == null) {
          cuda_ = builderForValue.build();
          onChanged();
        } else {
          cudaBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder mergeCuda(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (cuda_ != null) {
            cuda_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder(cuda_).mergeFrom(value).buildPartial();
          } else {
            cuda_ = value;
          }
          onChanged();
        } else {
          cudaBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder clearCuda() {
        if (cudaBuilder_ == null) {
          cuda_ = null;
          onChanged();
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder getCudaBuilder() {
        
        onChanged();
        return getCudaFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
        if (cudaBuilder_ != null) {
          return cudaBuilder_.getMessageOrBuilder();
        } else {
          return cuda_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> 
          getCudaFieldBuilder() {
        if (cudaBuilder_ == null) {
          cudaBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder>(
                  getCuda(),
                  getParentForChildren(),
                  isClean());
          cuda_ = null;
        }
        return cudaBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOptimizationPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelOptimizationPolicy>() {
      @java.lang.Override
      public ModelOptimizationPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOptimizationPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOptimizationPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOptimizationPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelDynamicBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelDynamicBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    java.util.List<java.lang.Integer> getPreferredBatchSizeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    int getPreferredBatchSizeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    int getPreferredBatchSize(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     */
    long getMaxQueueDelayMicroseconds();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
   *&#64;&#64;
   *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelDynamicBatching}
   */
  public  static final class ModelDynamicBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelDynamicBatching)
      ModelDynamicBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelDynamicBatching.newBuilder() to construct.
    private ModelDynamicBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelDynamicBatching() {
      preferredBatchSize_ = emptyIntList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelDynamicBatching();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelDynamicBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                preferredBatchSize_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              preferredBatchSize_.addInt(input.readInt32());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                preferredBatchSize_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                preferredBatchSize_.addInt(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 16: {

              maxQueueDelayMicroseconds_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
    }

    public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.IntList preferredBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public java.util.List<java.lang.Integer>
        getPreferredBatchSizeList() {
      return preferredBatchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public int getPreferredBatchSizeCount() {
      return preferredBatchSize_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public int getPreferredBatchSize(int index) {
      return preferredBatchSize_.getInt(index);
    }
    private int preferredBatchSizeMemoizedSerializedSize = -1;

    public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 2;
    private long maxQueueDelayMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     */
    public long getMaxQueueDelayMicroseconds() {
      return maxQueueDelayMicroseconds_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getPreferredBatchSizeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(preferredBatchSizeMemoizedSerializedSize);
      }
      for (int i = 0; i < preferredBatchSize_.size(); i++) {
        output.writeInt32NoTag(preferredBatchSize_.getInt(i));
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        output.writeUInt64(2, maxQueueDelayMicroseconds_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < preferredBatchSize_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(preferredBatchSize_.getInt(i));
        }
        size += dataSize;
        if (!getPreferredBatchSizeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        preferredBatchSizeMemoizedSerializedSize = dataSize;
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, maxQueueDelayMicroseconds_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) obj;

      if (!getPreferredBatchSizeList()
          .equals(other.getPreferredBatchSizeList())) return false;
      if (getMaxQueueDelayMicroseconds()
          != other.getMaxQueueDelayMicroseconds()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getPreferredBatchSizeCount() > 0) {
        hash = (37 * hash) + PREFERRED_BATCH_SIZE_FIELD_NUMBER;
        hash = (53 * hash) + getPreferredBatchSizeList().hashCode();
      }
      hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxQueueDelayMicroseconds());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
     *&#64;&#64;
     *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelDynamicBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelDynamicBatching)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        preferredBatchSize_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        maxQueueDelayMicroseconds_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.preferredBatchSize_ = preferredBatchSize_;
        result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) return this;
        if (!other.preferredBatchSize_.isEmpty()) {
          if (preferredBatchSize_.isEmpty()) {
            preferredBatchSize_ = other.preferredBatchSize_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensurePreferredBatchSizeIsMutable();
            preferredBatchSize_.addAll(other.preferredBatchSize_);
          }
          onChanged();
        }
        if (other.getMaxQueueDelayMicroseconds() != 0L) {
          setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.IntList preferredBatchSize_ = emptyIntList();
      private void ensurePreferredBatchSizeIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_ = mutableCopy(preferredBatchSize_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(preferredBatchSize_) : preferredBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.getInt(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder setPreferredBatchSize(
          int index, int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder addPreferredBatchSize(int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder addAllPreferredBatchSize(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensurePreferredBatchSizeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, preferredBatchSize_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder clearPreferredBatchSize() {
        preferredBatchSize_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private long maxQueueDelayMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public Builder setMaxQueueDelayMicroseconds(long value) {
        
        maxQueueDelayMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public Builder clearMaxQueueDelayMicroseconds() {
        
        maxQueueDelayMicroseconds_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelDynamicBatching)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelDynamicBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelDynamicBatching>() {
      @java.lang.Override
      public ModelDynamicBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelDynamicBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelDynamicBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelDynamicBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelSequenceBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the batch slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     */
    long getMaxSequenceIdleMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> 
        getControlInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    int getControlInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
   *&#64;&#64;
   *&#64;&#64;   Sequence batching configuration. These settings control how sequence
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching}
   */
  public  static final class ModelSequenceBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching)
      ModelSequenceBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelSequenceBatching.newBuilder() to construct.
    private ModelSequenceBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelSequenceBatching() {
      controlInput_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelSequenceBatching();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelSequenceBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              maxSequenceIdleMicroseconds_ = input.readUInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                controlInput_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>();
                mutable_bitField0_ |= 0x00000001;
              }
              controlInput_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
    }

    public interface ControlOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.Control)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      int getKindValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      java.util.List<java.lang.Integer> getInt32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      int getInt32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      int getInt32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      java.util.List<java.lang.Float> getFp32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      int getFp32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      float getFp32FalseTrue(int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Control
     *&#64;&#64;
     *&#64;&#64;     A control is a binary signal to a backend.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.Control}
     */
    public  static final class Control extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.Control)
        ControlOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Control.newBuilder() to construct.
      private Control(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Control() {
        kind_ = 0;
        int32FalseTrue_ = emptyIntList();
        fp32FalseTrue_ = emptyFloatList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Control();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Control(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                int rawValue = input.readEnum();

                kind_ = rawValue;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  int32FalseTrue_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                int32FalseTrue_.addInt(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_.addInt(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              case 29: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  fp32FalseTrue_ = newFloatList();
                  mutable_bitField0_ |= 0x00000002;
                }
                fp32FalseTrue_.addFloat(input.readFloat());
                break;
              }
              case 26: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_ = newFloatList();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_.addFloat(input.readFloat());
                }
                input.popLimit(limit);
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_.makeImmutable(); // C
          }
          if (((mutable_bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:enum:: Kind
       *&#64;&#64;
       *&#64;&#64;       The kind of the control.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf enum {@code nvidia.inferenceserver.ModelSequenceBatching.Control.Kind}
       */
      public enum Kind
          implements com.google.protobuf.ProtocolMessageEnum {
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        CONTROL_SEQUENCE_START(0),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        CONTROL_SEQUENCE_READY(1),
        UNRECOGNIZED(-1),
        ;

        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        public static final int CONTROL_SEQUENCE_START_VALUE = 0;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        public static final int CONTROL_SEQUENCE_READY_VALUE = 1;


        public final int getNumber() {
          if (this == UNRECOGNIZED) {
            throw new java.lang.IllegalArgumentException(
                "Can't get the number of an unknown enum value.");
          }
          return value;
        }

        /**
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static Kind valueOf(int value) {
          return forNumber(value);
        }

        public static Kind forNumber(int value) {
          switch (value) {
            case 0: return CONTROL_SEQUENCE_START;
            case 1: return CONTROL_SEQUENCE_READY;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Kind>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static final com.google.protobuf.Internal.EnumLiteMap<
            Kind> internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
                public Kind findValueByNumber(int number) {
                  return Kind.forNumber(number);
                }
              };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
          return getDescriptor().getValues().get(ordinal());
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDescriptor().getEnumTypes().get(0);
        }

        private static final Kind[] VALUES = values();

        public static Kind valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
          if (desc.getType() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "EnumValueDescriptor is not for this type.");
          }
          if (desc.getIndex() == -1) {
            return UNRECOGNIZED;
          }
          return VALUES[desc.getIndex()];
        }

        private final int value;

        private Kind(int value) {
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelSequenceBatching.Control.Kind)
      }

      public static final int KIND_FIELD_NUMBER = 1;
      private int kind_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
      }

      public static final int INT32_FALSE_TRUE_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList int32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public java.util.List<java.lang.Integer>
          getInt32FalseTrueList() {
        return int32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public int getInt32FalseTrueCount() {
        return int32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public int getInt32FalseTrue(int index) {
        return int32FalseTrue_.getInt(index);
      }
      private int int32FalseTrueMemoizedSerializedSize = -1;

      public static final int FP32_FALSE_TRUE_FIELD_NUMBER = 3;
      private com.google.protobuf.Internal.FloatList fp32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public java.util.List<java.lang.Float>
          getFp32FalseTrueList() {
        return fp32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public int getFp32FalseTrueCount() {
        return fp32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public float getFp32FalseTrue(int index) {
        return fp32FalseTrue_.getFloat(index);
      }
      private int fp32FalseTrueMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          output.writeEnum(1, kind_);
        }
        if (getInt32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(int32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < int32FalseTrue_.size(); i++) {
          output.writeInt32NoTag(int32FalseTrue_.getInt(i));
        }
        if (getFp32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(26);
          output.writeUInt32NoTag(fp32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < fp32FalseTrue_.size(); i++) {
          output.writeFloatNoTag(fp32FalseTrue_.getFloat(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, kind_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < int32FalseTrue_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(int32FalseTrue_.getInt(i));
          }
          size += dataSize;
          if (!getInt32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          int32FalseTrueMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          dataSize = 4 * getFp32FalseTrueList().size();
          size += dataSize;
          if (!getFp32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          fp32FalseTrueMemoizedSerializedSize = dataSize;
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) obj;

        if (kind_ != other.kind_) return false;
        if (!getInt32FalseTrueList()
            .equals(other.getInt32FalseTrueList())) return false;
        if (!getFp32FalseTrueList()
            .equals(other.getFp32FalseTrueList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + KIND_FIELD_NUMBER;
        hash = (53 * hash) + kind_;
        if (getInt32FalseTrueCount() > 0) {
          hash = (37 * hash) + INT32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getInt32FalseTrueList().hashCode();
        }
        if (getFp32FalseTrueCount() > 0) {
          hash = (37 * hash) + FP32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getFp32FalseTrueList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Control
       *&#64;&#64;
       *&#64;&#64;     A control is a binary signal to a backend.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.Control}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.Control)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          kind_ = 0;

          int32FalseTrue_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          fp32FalseTrue_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control(this);
          int from_bitField0_ = bitField0_;
          result.kind_ = kind_;
          if (((bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.int32FalseTrue_ = int32FalseTrue_;
          if (((bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.fp32FalseTrue_ = fp32FalseTrue_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance()) return this;
          if (other.kind_ != 0) {
            setKindValue(other.getKindValue());
          }
          if (!other.int32FalseTrue_.isEmpty()) {
            if (int32FalseTrue_.isEmpty()) {
              int32FalseTrue_ = other.int32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureInt32FalseTrueIsMutable();
              int32FalseTrue_.addAll(other.int32FalseTrue_);
            }
            onChanged();
          }
          if (!other.fp32FalseTrue_.isEmpty()) {
            if (fp32FalseTrue_.isEmpty()) {
              fp32FalseTrue_ = other.fp32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFp32FalseTrueIsMutable();
              fp32FalseTrue_.addAll(other.fp32FalseTrue_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private int kind_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public int getKindValue() {
          return kind_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder setKindValue(int value) {
          kind_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
          @SuppressWarnings("deprecation")
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
          return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder setKind(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          kind_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder clearKind() {
          
          kind_ = 0;
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList int32FalseTrue_ = emptyIntList();
        private void ensureInt32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_ = mutableCopy(int32FalseTrue_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public java.util.List<java.lang.Integer>
            getInt32FalseTrueList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(int32FalseTrue_) : int32FalseTrue_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public int getInt32FalseTrueCount() {
          return int32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public int getInt32FalseTrue(int index) {
          return int32FalseTrue_.getInt(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder setInt32FalseTrue(
            int index, int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.setInt(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder addInt32FalseTrue(int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.addInt(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder addAllInt32FalseTrue(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureInt32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, int32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder clearInt32FalseTrue() {
          int32FalseTrue_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.FloatList fp32FalseTrue_ = emptyFloatList();
        private void ensureFp32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_ = mutableCopy(fp32FalseTrue_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public java.util.List<java.lang.Float>
            getFp32FalseTrueList() {
          return ((bitField0_ & 0x00000002) != 0) ?
                   java.util.Collections.unmodifiableList(fp32FalseTrue_) : fp32FalseTrue_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public int getFp32FalseTrueCount() {
          return fp32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public float getFp32FalseTrue(int index) {
          return fp32FalseTrue_.getFloat(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder setFp32FalseTrue(
            int index, float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.setFloat(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder addFp32FalseTrue(float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.addFloat(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder addAllFp32FalseTrue(
            java.lang.Iterable<? extends java.lang.Float> values) {
          ensureFp32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, fp32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder clearFp32FalseTrue() {
          fp32FalseTrue_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Control>
          PARSER = new com.google.protobuf.AbstractParser<Control>() {
        @java.lang.Override
        public Control parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Control(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Control> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Control> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface ControlInputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> 
          getControlList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      int getControlCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message ControlInput
     *&#64;&#64;
     *&#64;&#64;     The sequence control values to communicate by a model input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.ControlInput}
     */
    public  static final class ControlInput extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
        ControlInputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use ControlInput.newBuilder() to construct.
      private ControlInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private ControlInput() {
        name_ = "";
        control_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new ControlInput();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private ControlInput(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 18: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  control_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control>();
                  mutable_bitField0_ |= 0x00000001;
                }
                control_.add(
                    input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            control_ = java.util.Collections.unmodifiableList(control_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int CONTROL_FIELD_NUMBER = 2;
      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> control_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public int getControlCount() {
        return control_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
        return control_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index) {
        return control_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          output.writeMessage(2, control_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, control_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (!getControlList()
            .equals(other.getControlList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (getControlCount() > 0) {
          hash = (37 * hash) + CONTROL_FIELD_NUMBER;
          hash = (53 * hash) + getControlList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message ControlInput
       *&#64;&#64;
       *&#64;&#64;     The sequence control values to communicate by a model input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.ControlInput}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getControlFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            controlBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput(this);
          int from_bitField0_ = bitField0_;
          result.name_ = name_;
          if (controlBuilder_ == null) {
            if (((bitField0_ & 0x00000001) != 0)) {
              control_ = java.util.Collections.unmodifiableList(control_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.control_ = control_;
          } else {
            result.control_ = controlBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (controlBuilder_ == null) {
            if (!other.control_.isEmpty()) {
              if (control_.isEmpty()) {
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureControlIsMutable();
                control_.addAll(other.control_);
              }
              onChanged();
            }
          } else {
            if (!other.control_.isEmpty()) {
              if (controlBuilder_.isEmpty()) {
                controlBuilder_.dispose();
                controlBuilder_ = null;
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000001);
                controlBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getControlFieldBuilder() : null;
              } else {
                controlBuilder_.addAllMessages(other.control_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> control_ =
          java.util.Collections.emptyList();
        private void ensureControlIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            control_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control>(control_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> controlBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
          if (controlBuilder_ == null) {
            return java.util.Collections.unmodifiableList(control_);
          } else {
            return controlBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public int getControlCount() {
          if (controlBuilder_ == null) {
            return control_.size();
          } else {
            return controlBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);
          } else {
            return controlBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.set(index, value);
            onChanged();
          } else {
            controlBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.set(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(value);
            onChanged();
          } else {
            controlBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(index, value);
            onChanged();
          } else {
            controlBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addAllControl(
            java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> values) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, control_);
            onChanged();
          } else {
            controlBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder clearControl() {
          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            controlBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder removeControl(int index) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.remove(index);
            onChanged();
          } else {
            controlBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder getControlBuilder(
            int index) {
          return getControlFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
            int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);  } else {
            return controlBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
             getControlOrBuilderList() {
          if (controlBuilder_ != null) {
            return controlBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(control_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder() {
          return getControlFieldBuilder().addBuilder(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder(
            int index) {
          return getControlFieldBuilder().addBuilder(
              index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder> 
             getControlBuilderList() {
          return getControlFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
            getControlFieldBuilder() {
          if (controlBuilder_ == null) {
            controlBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder>(
                    control_,
                    ((bitField0_ & 0x00000001) != 0),
                    getParentForChildren(),
                    isClean());
            control_ = null;
          }
          return controlBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<ControlInput>
          PARSER = new com.google.protobuf.AbstractParser<ControlInput>() {
        @java.lang.Override
        public ControlInput parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new ControlInput(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<ControlInput> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<ControlInput> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER = 1;
    private long maxSequenceIdleMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the batch slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     */
    public long getMaxSequenceIdleMicroseconds() {
      return maxSequenceIdleMicroseconds_;
    }

    public static final int CONTROL_INPUT_FIELD_NUMBER = 2;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public int getControlInputCount() {
      return controlInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
      return controlInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index) {
      return controlInput_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (maxSequenceIdleMicroseconds_ != 0L) {
        output.writeUInt64(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        output.writeMessage(2, controlInput_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (maxSequenceIdleMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, controlInput_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) obj;

      if (getMaxSequenceIdleMicroseconds()
          != other.getMaxSequenceIdleMicroseconds()) return false;
      if (!getControlInputList()
          .equals(other.getControlInputList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxSequenceIdleMicroseconds());
      if (getControlInputCount() > 0) {
        hash = (37 * hash) + CONTROL_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getControlInputList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
     *&#64;&#64;
     *&#64;&#64;   Sequence batching configuration. These settings control how sequence
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getControlInputFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        maxSequenceIdleMicroseconds_ = 0L;

        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          controlInputBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching(this);
        int from_bitField0_ = bitField0_;
        result.maxSequenceIdleMicroseconds_ = maxSequenceIdleMicroseconds_;
        if (controlInputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.controlInput_ = controlInput_;
        } else {
          result.controlInput_ = controlInputBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) return this;
        if (other.getMaxSequenceIdleMicroseconds() != 0L) {
          setMaxSequenceIdleMicroseconds(other.getMaxSequenceIdleMicroseconds());
        }
        if (controlInputBuilder_ == null) {
          if (!other.controlInput_.isEmpty()) {
            if (controlInput_.isEmpty()) {
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureControlInputIsMutable();
              controlInput_.addAll(other.controlInput_);
            }
            onChanged();
          }
        } else {
          if (!other.controlInput_.isEmpty()) {
            if (controlInputBuilder_.isEmpty()) {
              controlInputBuilder_.dispose();
              controlInputBuilder_ = null;
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000001);
              controlInputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getControlInputFieldBuilder() : null;
            } else {
              controlInputBuilder_.addAllMessages(other.controlInput_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long maxSequenceIdleMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the batch slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public long getMaxSequenceIdleMicroseconds() {
        return maxSequenceIdleMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the batch slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public Builder setMaxSequenceIdleMicroseconds(long value) {
        
        maxSequenceIdleMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the batch slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public Builder clearMaxSequenceIdleMicroseconds() {
        
        maxSequenceIdleMicroseconds_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_ =
        java.util.Collections.emptyList();
      private void ensureControlInputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          controlInput_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>(controlInput_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> controlInputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
        if (controlInputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(controlInput_);
        } else {
          return controlInputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public int getControlInputCount() {
        if (controlInputBuilder_ == null) {
          return controlInput_.size();
        } else {
          return controlInputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);
        } else {
          return controlInputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.set(index, value);
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.set(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(index, value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addAllControlInput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> values) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, controlInput_);
          onChanged();
        } else {
          controlInputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder clearControlInput() {
        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          controlInputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder removeControlInput(int index) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.remove(index);
          onChanged();
        } else {
          controlInputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder getControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
          int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);  } else {
          return controlInputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
           getControlInputOrBuilderList() {
        if (controlInputBuilder_ != null) {
          return controlInputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(controlInput_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder() {
        return getControlInputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder> 
           getControlInputBuilderList() {
        return getControlInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
          getControlInputFieldBuilder() {
        if (controlInputBuilder_ == null) {
          controlInputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder>(
                  controlInput_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          controlInput_ = null;
        }
        return controlInputBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelSequenceBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelSequenceBatching>() {
      @java.lang.Override
      public ModelSequenceBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelSequenceBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelSequenceBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelSequenceBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelEnsemblingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelEnsembling)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> 
        getStepList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    int getStepCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelEnsembling
   *&#64;&#64;
   *&#64;&#64;   Model ensembling configuration. These settings specify the models that
   *&#64;&#64;   compose the ensemble and how data flows between the models.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling}
   */
  public  static final class ModelEnsembling extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelEnsembling)
      ModelEnsemblingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelEnsembling.newBuilder() to construct.
    private ModelEnsembling(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelEnsembling() {
      step_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelEnsembling();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelEnsembling(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                step_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step>();
                mutable_bitField0_ |= 0x00000001;
              }
              step_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          step_ = java.util.Collections.unmodifiableList(step_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder.class);
    }

    public interface StepOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelEnsembling.Step)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      java.lang.String getModelName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      com.google.protobuf.ByteString
          getModelNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      long getModelVersion();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      int getInputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      boolean containsInputMap(
          java.lang.String key);
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getInputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getInputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrThrow(
          java.lang.String key);

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      int getOutputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      boolean containsOutputMap(
          java.lang.String key);
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrThrow(
          java.lang.String key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Step
     *&#64;&#64;
     *&#64;&#64;     Each step specifies a model included in the ensemble,
     *&#64;&#64;     maps ensemble tensor names to the model input tensors,
     *&#64;&#64;     and maps model output tensors to ensemble tensor names
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling.Step}
     */
    public  static final class Step extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelEnsembling.Step)
        StepOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Step.newBuilder() to construct.
      private Step(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Step() {
        modelName_ = "";
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Step();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Step(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                modelName_ = s;
                break;
              }
              case 16: {

                modelVersion_ = input.readInt64();
                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  inputMap_ = com.google.protobuf.MapField.newMapField(
                      InputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000001;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                inputMap__ = input.readMessage(
                    InputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                inputMap_.getMutableMap().put(
                    inputMap__.getKey(), inputMap__.getValue());
                break;
              }
              case 34: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  outputMap_ = com.google.protobuf.MapField.newMapField(
                      OutputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000002;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                outputMap__ = input.readMessage(
                    OutputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                outputMap_.getMutableMap().put(
                    outputMap__.getKey(), outputMap__.getValue());
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      @java.lang.Override
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetInputMap();
          case 4:
            return internalGetOutputMap();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
      }

      public static final int MODEL_NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object modelName_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public java.lang.String getModelName() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          modelName_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getModelNameBytes() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          modelName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int MODEL_VERSION_FIELD_NUMBER = 2;
      private long modelVersion_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      public long getModelVersion() {
        return modelVersion_;
      }

      public static final int INPUT_MAP_FIELD_NUMBER = 3;
      private static final class InputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> inputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetInputMap() {
        if (inputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InputMapDefaultEntryHolder.defaultEntry);
        }
        return inputMap_;
      }

      public int getInputMapCount() {
        return internalGetInputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public boolean containsInputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetInputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
        return getInputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
        return internalGetInputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.lang.String getInputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public static final int OUTPUT_MAP_FIELD_NUMBER = 4;
      private static final class OutputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> outputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetOutputMap() {
        if (outputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              OutputMapDefaultEntryHolder.defaultEntry);
        }
        return outputMap_;
      }

      public int getOutputMapCount() {
        return internalGetOutputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public boolean containsOutputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetOutputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
        return getOutputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
        return internalGetOutputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.lang.String getOutputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getModelNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, modelName_);
        }
        if (modelVersion_ != 0L) {
          output.writeInt64(2, modelVersion_);
        }
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetInputMap(),
            InputMapDefaultEntryHolder.defaultEntry,
            3);
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetOutputMap(),
            OutputMapDefaultEntryHolder.defaultEntry,
            4);
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getModelNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, modelName_);
        }
        if (modelVersion_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, modelVersion_);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetInputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          inputMap__ = InputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(3, inputMap__);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetOutputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          outputMap__ = OutputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(4, outputMap__);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) obj;

        if (!getModelName()
            .equals(other.getModelName())) return false;
        if (getModelVersion()
            != other.getModelVersion()) return false;
        if (!internalGetInputMap().equals(
            other.internalGetInputMap())) return false;
        if (!internalGetOutputMap().equals(
            other.internalGetOutputMap())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MODEL_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getModelName().hashCode();
        hash = (37 * hash) + MODEL_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getModelVersion());
        if (!internalGetInputMap().getMap().isEmpty()) {
          hash = (37 * hash) + INPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetInputMap().hashCode();
        }
        if (!internalGetOutputMap().getMap().isEmpty()) {
          hash = (37 * hash) + OUTPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetOutputMap().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Step
       *&#64;&#64;
       *&#64;&#64;     Each step specifies a model included in the ensemble,
       *&#64;&#64;     maps ensemble tensor names to the model input tensors,
       *&#64;&#64;     and maps model output tensors to ensemble tensor names
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling.Step}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelEnsembling.Step)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetInputMap();
            case 4:
              return internalGetOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMutableMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetMutableInputMap();
            case 4:
              return internalGetMutableOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          modelName_ = "";

          modelVersion_ = 0L;

          internalGetMutableInputMap().clear();
          internalGetMutableOutputMap().clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step(this);
          int from_bitField0_ = bitField0_;
          result.modelName_ = modelName_;
          result.modelVersion_ = modelVersion_;
          result.inputMap_ = internalGetInputMap();
          result.inputMap_.makeImmutable();
          result.outputMap_ = internalGetOutputMap();
          result.outputMap_.makeImmutable();
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance()) return this;
          if (!other.getModelName().isEmpty()) {
            modelName_ = other.modelName_;
            onChanged();
          }
          if (other.getModelVersion() != 0L) {
            setModelVersion(other.getModelVersion());
          }
          internalGetMutableInputMap().mergeFrom(
              other.internalGetInputMap());
          internalGetMutableOutputMap().mergeFrom(
              other.internalGetOutputMap());
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object modelName_ = "";
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public java.lang.String getModelName() {
          java.lang.Object ref = modelName_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            modelName_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getModelNameBytes() {
          java.lang.Object ref = modelName_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            modelName_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder setModelName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          modelName_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder clearModelName() {
          
          modelName_ = getDefaultInstance().getModelName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder setModelNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          modelName_ = value;
          onChanged();
          return this;
        }

        private long modelVersion_ ;
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public long getModelVersion() {
          return modelVersion_;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public Builder setModelVersion(long value) {
          
          modelVersion_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public Builder clearModelVersion() {
          
          modelVersion_ = 0L;
          onChanged();
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> inputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetInputMap() {
          if (inputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          return inputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableInputMap() {
          onChanged();;
          if (inputMap_ == null) {
            inputMap_ = com.google.protobuf.MapField.newMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          if (!inputMap_.isMutable()) {
            inputMap_ = inputMap_.copy();
          }
          return inputMap_;
        }

        public int getInputMapCount() {
          return internalGetInputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public boolean containsInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetInputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getInputMapMap()} instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
          return getInputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
          return internalGetInputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.lang.String getInputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.lang.String getInputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearInputMap() {
          internalGetMutableInputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder removeInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableInputMap() {
          return internalGetMutableInputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        public Builder putInputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder putAllInputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableInputMap().getMutableMap()
              .putAll(values);
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> outputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetOutputMap() {
          if (outputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          return outputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableOutputMap() {
          onChanged();;
          if (outputMap_ == null) {
            outputMap_ = com.google.protobuf.MapField.newMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          if (!outputMap_.isMutable()) {
            outputMap_ = outputMap_.copy();
          }
          return outputMap_;
        }

        public int getOutputMapCount() {
          return internalGetOutputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public boolean containsOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetOutputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getOutputMapMap()} instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
          return getOutputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
          return internalGetOutputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.lang.String getOutputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.lang.String getOutputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearOutputMap() {
          internalGetMutableOutputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder removeOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableOutputMap() {
          return internalGetMutableOutputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        public Builder putOutputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder putAllOutputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableOutputMap().getMutableMap()
              .putAll(values);
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelEnsembling.Step)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Step>
          PARSER = new com.google.protobuf.AbstractParser<Step>() {
        @java.lang.Override
        public Step parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Step(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Step> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Step> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int STEP_FIELD_NUMBER = 1;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> step_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public int getStepCount() {
      return step_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
      return step_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index) {
      return step_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < step_.size(); i++) {
        output.writeMessage(1, step_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < step_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, step_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) obj;

      if (!getStepList()
          .equals(other.getStepList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getStepCount() > 0) {
        hash = (37 * hash) + STEP_FIELD_NUMBER;
        hash = (53 * hash) + getStepList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelEnsembling
     *&#64;&#64;
     *&#64;&#64;   Model ensembling configuration. These settings specify the models that
     *&#64;&#64;   compose the ensemble and how data flows between the models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelEnsembling)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStepFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          stepBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling(this);
        int from_bitField0_ = bitField0_;
        if (stepBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            step_ = java.util.Collections.unmodifiableList(step_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.step_ = step_;
        } else {
          result.step_ = stepBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) return this;
        if (stepBuilder_ == null) {
          if (!other.step_.isEmpty()) {
            if (step_.isEmpty()) {
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStepIsMutable();
              step_.addAll(other.step_);
            }
            onChanged();
          }
        } else {
          if (!other.step_.isEmpty()) {
            if (stepBuilder_.isEmpty()) {
              stepBuilder_.dispose();
              stepBuilder_ = null;
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
              stepBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStepFieldBuilder() : null;
            } else {
              stepBuilder_.addAllMessages(other.step_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> step_ =
        java.util.Collections.emptyList();
      private void ensureStepIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          step_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step>(step_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> stepBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
        if (stepBuilder_ == null) {
          return java.util.Collections.unmodifiableList(step_);
        } else {
          return stepBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public int getStepCount() {
        if (stepBuilder_ == null) {
          return step_.size();
        } else {
          return stepBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);
        } else {
          return stepBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.set(index, value);
          onChanged();
        } else {
          stepBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.set(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(value);
          onChanged();
        } else {
          stepBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(index, value);
          onChanged();
        } else {
          stepBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addAllStep(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> values) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, step_);
          onChanged();
        } else {
          stepBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder clearStep() {
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          stepBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder removeStep(int index) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.remove(index);
          onChanged();
        } else {
          stepBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder getStepBuilder(
          int index) {
        return getStepFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
          int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);  } else {
          return stepBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
           getStepOrBuilderList() {
        if (stepBuilder_ != null) {
          return stepBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(step_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder() {
        return getStepFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder(
          int index) {
        return getStepFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder> 
           getStepBuilderList() {
        return getStepFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
          getStepFieldBuilder() {
        if (stepBuilder_ == null) {
          stepBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder>(
                  step_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          step_ = null;
        }
        return stepBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelEnsembling)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelEnsembling>
        PARSER = new com.google.protobuf.AbstractParser<ModelEnsembling>() {
      @java.lang.Override
      public ModelEnsembling parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelEnsembling(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelEnsembling> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelEnsembling> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelParameterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelParameter)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    java.lang.String getStringValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    com.google.protobuf.ByteString
        getStringValueBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelParameter
   *&#64;&#64;
   *&#64;&#64;   A model parameter.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelParameter}
   */
  public  static final class ModelParameter extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelParameter)
      ModelParameterOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelParameter.newBuilder() to construct.
    private ModelParameter(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelParameter() {
      stringValue_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelParameter();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelParameter(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              stringValue_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.Builder.class);
    }

    public static final int STRING_VALUE_FIELD_NUMBER = 1;
    private volatile java.lang.Object stringValue_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    public java.lang.String getStringValue() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        stringValue_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    public com.google.protobuf.ByteString
        getStringValueBytes() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stringValue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getStringValueBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, stringValue_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getStringValueBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, stringValue_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) obj;

      if (!getStringValue()
          .equals(other.getStringValue())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STRING_VALUE_FIELD_NUMBER;
      hash = (53 * hash) + getStringValue().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelParameter
     *&#64;&#64;
     *&#64;&#64;   A model parameter.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelParameter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelParameter)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameterOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stringValue_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter(this);
        result.stringValue_ = stringValue_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance()) return this;
        if (!other.getStringValue().isEmpty()) {
          stringValue_ = other.stringValue_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object stringValue_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public java.lang.String getStringValue() {
        java.lang.Object ref = stringValue_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          stringValue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public com.google.protobuf.ByteString
          getStringValueBytes() {
        java.lang.Object ref = stringValue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stringValue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder setStringValue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        stringValue_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder clearStringValue() {
        
        stringValue_ = getDefaultInstance().getStringValue();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder setStringValueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        stringValue_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelParameter)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelParameter)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelParameter>
        PARSER = new com.google.protobuf.AbstractParser<ModelParameter>() {
      @java.lang.Override
      public ModelParameter parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelParameter(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelParameter> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelParameter> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelConfigOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelConfig)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    java.lang.String getPlatform();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    com.google.protobuf.ByteString
        getPlatformBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    boolean hasVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     */
    int getMaxBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> 
        getInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    int getInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    int getOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    boolean hasOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    boolean hasDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    boolean hasSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    boolean hasEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> 
        getInstanceGroupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    int getInstanceGroupCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    java.lang.String getDefaultModelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    com.google.protobuf.ByteString
        getDefaultModelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    int getCcModelFilenamesCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    boolean containsCcModelFilenames(
        java.lang.String key);
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenames();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenamesMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    int getMetricTagsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    boolean containsMetricTags(
        java.lang.String key);
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTags();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTagsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    int getParametersCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    boolean containsParameters(
        java.lang.String key);
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    getParameters();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    getParametersMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key);

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.SchedulingChoiceCase getSchedulingChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelConfig
   *&#64;&#64;
   *&#64;&#64;   A model configuration.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelConfig}
   */
  public  static final class ModelConfig extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelConfig)
      ModelConfigOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelConfig.newBuilder() to construct.
    private ModelConfig(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelConfig() {
      name_ = "";
      platform_ = "";
      input_ = java.util.Collections.emptyList();
      output_ = java.util.Collections.emptyList();
      instanceGroup_ = java.util.Collections.emptyList();
      defaultModelFilename_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelConfig();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelConfig(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();

              platform_ = s;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder subBuilder = null;
              if (versionPolicy_ != null) {
                subBuilder = versionPolicy_.toBuilder();
              }
              versionPolicy_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(versionPolicy_);
                versionPolicy_ = subBuilder.buildPartial();
              }

              break;
            }
            case 32: {

              maxBatchSize_ = input.readInt32();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                input_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput>();
                mutable_bitField0_ |= 0x00000001;
              }
              input_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.parser(), extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                output_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput>();
                mutable_bitField0_ |= 0x00000002;
              }
              output_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.parser(), extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                instanceGroup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup>();
                mutable_bitField0_ |= 0x00000004;
              }
              instanceGroup_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.parser(), extensionRegistry));
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();

              defaultModelFilename_ = s;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
                    CcModelFilenamesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000008;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              ccModelFilenames__ = input.readMessage(
                  CcModelFilenamesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              ccModelFilenames_.getMutableMap().put(
                  ccModelFilenames__.getKey(), ccModelFilenames__.getValue());
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                metricTags_ = com.google.protobuf.MapField.newMapField(
                    MetricTagsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000010;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              metricTags__ = input.readMessage(
                  MetricTagsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metricTags_.getMutableMap().put(
                  metricTags__.getKey(), metricTags__.getValue());
              break;
            }
            case 90: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 11) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 11;
              break;
            }
            case 98: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder subBuilder = null;
              if (optimization_ != null) {
                subBuilder = optimization_.toBuilder();
              }
              optimization_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(optimization_);
                optimization_ = subBuilder.buildPartial();
              }

              break;
            }
            case 106: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 13) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 13;
              break;
            }
            case 114: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                parameters_ = com.google.protobuf.MapField.newMapField(
                    ParametersDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000020;
              }
              com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
              parameters__ = input.readMessage(
                  ParametersDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              parameters_.getMutableMap().put(
                  parameters__.getKey(), parameters__.getValue());
              break;
            }
            case 122: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 15) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 15;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          input_ = java.util.Collections.unmodifiableList(input_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          output_ = java.util.Collections.unmodifiableList(output_);
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 9:
          return internalGetCcModelFilenames();
        case 10:
          return internalGetMetricTags();
        case 14:
          return internalGetParameters();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder.class);
    }

    private int schedulingChoiceCase_ = 0;
    private java.lang.Object schedulingChoice_;
    public enum SchedulingChoiceCase
        implements com.google.protobuf.Internal.EnumLite {
      DYNAMIC_BATCHING(11),
      SEQUENCE_BATCHING(13),
      ENSEMBLE_SCHEDULING(15),
      SCHEDULINGCHOICE_NOT_SET(0);
      private final int value;
      private SchedulingChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static SchedulingChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static SchedulingChoiceCase forNumber(int value) {
        switch (value) {
          case 11: return DYNAMIC_BATCHING;
          case 13: return SEQUENCE_BATCHING;
          case 15: return ENSEMBLE_SCHEDULING;
          case 0: return SCHEDULINGCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public SchedulingChoiceCase
    getSchedulingChoiceCase() {
      return SchedulingChoiceCase.forNumber(
          schedulingChoiceCase_);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PLATFORM_FIELD_NUMBER = 2;
    private volatile java.lang.Object platform_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    public java.lang.String getPlatform() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        platform_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    public com.google.protobuf.ByteString
        getPlatformBytes() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        platform_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERSION_POLICY_FIELD_NUMBER = 3;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public boolean hasVersionPolicy() {
      return versionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
      return versionPolicy_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
      return getVersionPolicy();
    }

    public static final int MAX_BATCH_SIZE_FIELD_NUMBER = 4;
    private int maxBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     */
    public int getMaxBatchSize() {
      return maxBatchSize_;
    }

    public static final int INPUT_FIELD_NUMBER = 5;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> input_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> getInputList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public int getInputCount() {
      return input_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index) {
      return input_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index) {
      return input_.get(index);
    }

    public static final int OUTPUT_FIELD_NUMBER = 6;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }

    public static final int OPTIMIZATION_FIELD_NUMBER = 12;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public boolean hasOptimization() {
      return optimization_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
      return optimization_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
      return getOptimization();
    }

    public static final int DYNAMIC_BATCHING_FIELD_NUMBER = 11;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public boolean hasDynamicBatching() {
      return schedulingChoiceCase_ == 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
      if (schedulingChoiceCase_ == 11) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 11) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }

    public static final int SEQUENCE_BATCHING_FIELD_NUMBER = 13;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public boolean hasSequenceBatching() {
      return schedulingChoiceCase_ == 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
      if (schedulingChoiceCase_ == 13) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 13) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }

    public static final int ENSEMBLE_SCHEDULING_FIELD_NUMBER = 15;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public boolean hasEnsembleScheduling() {
      return schedulingChoiceCase_ == 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
      if (schedulingChoiceCase_ == 15) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
      if (schedulingChoiceCase_ == 15) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }

    public static final int INSTANCE_GROUP_FIELD_NUMBER = 7;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public int getInstanceGroupCount() {
      return instanceGroup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
      return instanceGroup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index) {
      return instanceGroup_.get(index);
    }

    public static final int DEFAULT_MODEL_FILENAME_FIELD_NUMBER = 8;
    private volatile java.lang.Object defaultModelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    public java.lang.String getDefaultModelFilename() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        defaultModelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    public com.google.protobuf.ByteString
        getDefaultModelFilenameBytes() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        defaultModelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CC_MODEL_FILENAMES_FIELD_NUMBER = 9;
    private static final class CcModelFilenamesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> ccModelFilenames_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetCcModelFilenames() {
      if (ccModelFilenames_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            CcModelFilenamesDefaultEntryHolder.defaultEntry);
      }
      return ccModelFilenames_;
    }

    public int getCcModelFilenamesCount() {
      return internalGetCcModelFilenames().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public boolean containsCcModelFilenames(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetCcModelFilenames().getMap().containsKey(key);
    }
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
      return getCcModelFilenamesMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
      return internalGetCcModelFilenames().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int METRIC_TAGS_FIELD_NUMBER = 10;
    private static final class MetricTagsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> metricTags_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetMetricTags() {
      if (metricTags_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricTagsDefaultEntryHolder.defaultEntry);
      }
      return metricTags_;
    }

    public int getMetricTagsCount() {
      return internalGetMetricTags().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public boolean containsMetricTags(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetricTags().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
      return getMetricTagsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
      return internalGetMetricTags().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.lang.String getMetricTagsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int PARAMETERS_FIELD_NUMBER = 14;
    private static final class ParametersDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> parameters_;
    private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    internalGetParameters() {
      if (parameters_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ParametersDefaultEntryHolder.defaultEntry);
      }
      return parameters_;
    }

    public int getParametersCount() {
      return internalGetParameters().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public boolean containsParameters(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetParameters().getMap().containsKey(key);
    }
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParameters() {
      return getParametersMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParametersMap() {
      return internalGetParameters().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, platform_);
      }
      if (versionPolicy_ != null) {
        output.writeMessage(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        output.writeInt32(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        output.writeMessage(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        output.writeMessage(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        output.writeMessage(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, defaultModelFilename_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetCcModelFilenames(),
          CcModelFilenamesDefaultEntryHolder.defaultEntry,
          9);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetricTags(),
          MetricTagsDefaultEntryHolder.defaultEntry,
          10);
      if (schedulingChoiceCase_ == 11) {
        output.writeMessage(11, (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        output.writeMessage(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        output.writeMessage(13, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetParameters(),
          ParametersDefaultEntryHolder.defaultEntry,
          14);
      if (schedulingChoiceCase_ == 15) {
        output.writeMessage(15, (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, platform_);
      }
      if (versionPolicy_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, defaultModelFilename_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetCcModelFilenames().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        ccModelFilenames__ = CcModelFilenamesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(9, ccModelFilenames__);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetMetricTags().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        metricTags__ = MetricTagsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(10, metricTags__);
      }
      if (schedulingChoiceCase_ == 11) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      for (java.util.Map.Entry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> entry
           : internalGetParameters().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
        parameters__ = ParametersDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(14, parameters__);
      }
      if (schedulingChoiceCase_ == 15) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (!getPlatform()
          .equals(other.getPlatform())) return false;
      if (hasVersionPolicy() != other.hasVersionPolicy()) return false;
      if (hasVersionPolicy()) {
        if (!getVersionPolicy()
            .equals(other.getVersionPolicy())) return false;
      }
      if (getMaxBatchSize()
          != other.getMaxBatchSize()) return false;
      if (!getInputList()
          .equals(other.getInputList())) return false;
      if (!getOutputList()
          .equals(other.getOutputList())) return false;
      if (hasOptimization() != other.hasOptimization()) return false;
      if (hasOptimization()) {
        if (!getOptimization()
            .equals(other.getOptimization())) return false;
      }
      if (!getInstanceGroupList()
          .equals(other.getInstanceGroupList())) return false;
      if (!getDefaultModelFilename()
          .equals(other.getDefaultModelFilename())) return false;
      if (!internalGetCcModelFilenames().equals(
          other.internalGetCcModelFilenames())) return false;
      if (!internalGetMetricTags().equals(
          other.internalGetMetricTags())) return false;
      if (!internalGetParameters().equals(
          other.internalGetParameters())) return false;
      if (!getSchedulingChoiceCase().equals(other.getSchedulingChoiceCase())) return false;
      switch (schedulingChoiceCase_) {
        case 11:
          if (!getDynamicBatching()
              .equals(other.getDynamicBatching())) return false;
          break;
        case 13:
          if (!getSequenceBatching()
              .equals(other.getSequenceBatching())) return false;
          break;
        case 15:
          if (!getEnsembleScheduling()
              .equals(other.getEnsembleScheduling())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + PLATFORM_FIELD_NUMBER;
      hash = (53 * hash) + getPlatform().hashCode();
      if (hasVersionPolicy()) {
        hash = (37 * hash) + VERSION_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getVersionPolicy().hashCode();
      }
      hash = (37 * hash) + MAX_BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getMaxBatchSize();
      if (getInputCount() > 0) {
        hash = (37 * hash) + INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getInputList().hashCode();
      }
      if (getOutputCount() > 0) {
        hash = (37 * hash) + OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getOutputList().hashCode();
      }
      if (hasOptimization()) {
        hash = (37 * hash) + OPTIMIZATION_FIELD_NUMBER;
        hash = (53 * hash) + getOptimization().hashCode();
      }
      if (getInstanceGroupCount() > 0) {
        hash = (37 * hash) + INSTANCE_GROUP_FIELD_NUMBER;
        hash = (53 * hash) + getInstanceGroupList().hashCode();
      }
      hash = (37 * hash) + DEFAULT_MODEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getDefaultModelFilename().hashCode();
      if (!internalGetCcModelFilenames().getMap().isEmpty()) {
        hash = (37 * hash) + CC_MODEL_FILENAMES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetCcModelFilenames().hashCode();
      }
      if (!internalGetMetricTags().getMap().isEmpty()) {
        hash = (37 * hash) + METRIC_TAGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetricTags().hashCode();
      }
      if (!internalGetParameters().getMap().isEmpty()) {
        hash = (37 * hash) + PARAMETERS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetParameters().hashCode();
      }
      switch (schedulingChoiceCase_) {
        case 11:
          hash = (37 * hash) + DYNAMIC_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getDynamicBatching().hashCode();
          break;
        case 13:
          hash = (37 * hash) + SEQUENCE_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getSequenceBatching().hashCode();
          break;
        case 15:
          hash = (37 * hash) + ENSEMBLE_SCHEDULING_FIELD_NUMBER;
          hash = (53 * hash) + getEnsembleScheduling().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelConfig
     *&#64;&#64;
     *&#64;&#64;   A model configuration.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelConfig}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelConfig)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetCcModelFilenames();
          case 10:
            return internalGetMetricTags();
          case 14:
            return internalGetParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetMutableCcModelFilenames();
          case 10:
            return internalGetMutableMetricTags();
          case 14:
            return internalGetMutableParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getInputFieldBuilder();
          getOutputFieldBuilder();
          getInstanceGroupFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        platform_ = "";

        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }
        maxBatchSize_ = 0;

        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          inputBuilder_.clear();
        }
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          outputBuilder_.clear();
        }
        if (optimizationBuilder_ == null) {
          optimization_ = null;
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          instanceGroupBuilder_.clear();
        }
        defaultModelFilename_ = "";

        internalGetMutableCcModelFilenames().clear();
        internalGetMutableMetricTags().clear();
        internalGetMutableParameters().clear();
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.platform_ = platform_;
        if (versionPolicyBuilder_ == null) {
          result.versionPolicy_ = versionPolicy_;
        } else {
          result.versionPolicy_ = versionPolicyBuilder_.build();
        }
        result.maxBatchSize_ = maxBatchSize_;
        if (inputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            input_ = java.util.Collections.unmodifiableList(input_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.input_ = input_;
        } else {
          result.input_ = inputBuilder_.build();
        }
        if (outputBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            output_ = java.util.Collections.unmodifiableList(output_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.output_ = output_;
        } else {
          result.output_ = outputBuilder_.build();
        }
        if (optimizationBuilder_ == null) {
          result.optimization_ = optimization_;
        } else {
          result.optimization_ = optimizationBuilder_.build();
        }
        if (schedulingChoiceCase_ == 11) {
          if (dynamicBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = dynamicBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 13) {
          if (sequenceBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = sequenceBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 15) {
          if (ensembleSchedulingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = ensembleSchedulingBuilder_.build();
          }
        }
        if (instanceGroupBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.instanceGroup_ = instanceGroup_;
        } else {
          result.instanceGroup_ = instanceGroupBuilder_.build();
        }
        result.defaultModelFilename_ = defaultModelFilename_;
        result.ccModelFilenames_ = internalGetCcModelFilenames();
        result.ccModelFilenames_.makeImmutable();
        result.metricTags_ = internalGetMetricTags();
        result.metricTags_.makeImmutable();
        result.parameters_ = internalGetParameters();
        result.parameters_.makeImmutable();
        result.schedulingChoiceCase_ = schedulingChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (!other.getPlatform().isEmpty()) {
          platform_ = other.platform_;
          onChanged();
        }
        if (other.hasVersionPolicy()) {
          mergeVersionPolicy(other.getVersionPolicy());
        }
        if (other.getMaxBatchSize() != 0) {
          setMaxBatchSize(other.getMaxBatchSize());
        }
        if (inputBuilder_ == null) {
          if (!other.input_.isEmpty()) {
            if (input_.isEmpty()) {
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureInputIsMutable();
              input_.addAll(other.input_);
            }
            onChanged();
          }
        } else {
          if (!other.input_.isEmpty()) {
            if (inputBuilder_.isEmpty()) {
              inputBuilder_.dispose();
              inputBuilder_ = null;
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
              inputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInputFieldBuilder() : null;
            } else {
              inputBuilder_.addAllMessages(other.input_);
            }
          }
        }
        if (outputBuilder_ == null) {
          if (!other.output_.isEmpty()) {
            if (output_.isEmpty()) {
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureOutputIsMutable();
              output_.addAll(other.output_);
            }
            onChanged();
          }
        } else {
          if (!other.output_.isEmpty()) {
            if (outputBuilder_.isEmpty()) {
              outputBuilder_.dispose();
              outputBuilder_ = null;
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
              outputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputFieldBuilder() : null;
            } else {
              outputBuilder_.addAllMessages(other.output_);
            }
          }
        }
        if (other.hasOptimization()) {
          mergeOptimization(other.getOptimization());
        }
        if (instanceGroupBuilder_ == null) {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroup_.isEmpty()) {
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureInstanceGroupIsMutable();
              instanceGroup_.addAll(other.instanceGroup_);
            }
            onChanged();
          }
        } else {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroupBuilder_.isEmpty()) {
              instanceGroupBuilder_.dispose();
              instanceGroupBuilder_ = null;
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000004);
              instanceGroupBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInstanceGroupFieldBuilder() : null;
            } else {
              instanceGroupBuilder_.addAllMessages(other.instanceGroup_);
            }
          }
        }
        if (!other.getDefaultModelFilename().isEmpty()) {
          defaultModelFilename_ = other.defaultModelFilename_;
          onChanged();
        }
        internalGetMutableCcModelFilenames().mergeFrom(
            other.internalGetCcModelFilenames());
        internalGetMutableMetricTags().mergeFrom(
            other.internalGetMetricTags());
        internalGetMutableParameters().mergeFrom(
            other.internalGetParameters());
        switch (other.getSchedulingChoiceCase()) {
          case DYNAMIC_BATCHING: {
            mergeDynamicBatching(other.getDynamicBatching());
            break;
          }
          case SEQUENCE_BATCHING: {
            mergeSequenceBatching(other.getSequenceBatching());
            break;
          }
          case ENSEMBLE_SCHEDULING: {
            mergeEnsembleScheduling(other.getEnsembleScheduling());
            break;
          }
          case SCHEDULINGCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int schedulingChoiceCase_ = 0;
      private java.lang.Object schedulingChoice_;
      public SchedulingChoiceCase
          getSchedulingChoiceCase() {
        return SchedulingChoiceCase.forNumber(
            schedulingChoiceCase_);
      }

      public Builder clearSchedulingChoice() {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object platform_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public java.lang.String getPlatform() {
        java.lang.Object ref = platform_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          platform_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public com.google.protobuf.ByteString
          getPlatformBytes() {
        java.lang.Object ref = platform_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          platform_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder setPlatform(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        platform_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder clearPlatform() {
        
        platform_ = getDefaultInstance().getPlatform();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder setPlatformBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        platform_ = value;
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> versionPolicyBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public boolean hasVersionPolicy() {
        return versionPolicyBuilder_ != null || versionPolicy_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          return versionPolicy_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        } else {
          return versionPolicyBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          versionPolicy_ = value;
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder builderForValue) {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = builderForValue.build();
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder mergeVersionPolicy(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (versionPolicy_ != null) {
            versionPolicy_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.newBuilder(versionPolicy_).mergeFrom(value).buildPartial();
          } else {
            versionPolicy_ = value;
          }
          onChanged();
        } else {
          versionPolicyBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder clearVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
          onChanged();
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder getVersionPolicyBuilder() {
        
        onChanged();
        return getVersionPolicyFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
        if (versionPolicyBuilder_ != null) {
          return versionPolicyBuilder_.getMessageOrBuilder();
        } else {
          return versionPolicy_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> 
          getVersionPolicyFieldBuilder() {
        if (versionPolicyBuilder_ == null) {
          versionPolicyBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder>(
                  getVersionPolicy(),
                  getParentForChildren(),
                  isClean());
          versionPolicy_ = null;
        }
        return versionPolicyBuilder_;
      }

      private int maxBatchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public int getMaxBatchSize() {
        return maxBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public Builder setMaxBatchSize(int value) {
        
        maxBatchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public Builder clearMaxBatchSize() {
        
        maxBatchSize_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> input_ =
        java.util.Collections.emptyList();
      private void ensureInputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          input_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput>(input_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> inputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> getInputList() {
        if (inputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(input_);
        } else {
          return inputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public int getInputCount() {
        if (inputBuilder_ == null) {
          return input_.size();
        } else {
          return inputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);
        } else {
          return inputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.set(index, value);
          onChanged();
        } else {
          inputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.set(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(value);
          onChanged();
        } else {
          inputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(index, value);
          onChanged();
        } else {
          inputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addAllInput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> values) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, input_);
          onChanged();
        } else {
          inputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder clearInput() {
        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          inputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder removeInput(int index) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.remove(index);
          onChanged();
        } else {
          inputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder getInputBuilder(
          int index) {
        return getInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
          int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);  } else {
          return inputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
           getInputOrBuilderList() {
        if (inputBuilder_ != null) {
          return inputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(input_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder addInputBuilder() {
        return getInputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder addInputBuilder(
          int index) {
        return getInputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder> 
           getInputBuilderList() {
        return getInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
          getInputFieldBuilder() {
        if (inputBuilder_ == null) {
          inputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder>(
                  input_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          input_ = null;
        }
        return inputBuilder_;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> output_ =
        java.util.Collections.emptyList();
      private void ensureOutputIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          output_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput>(output_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> outputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> getOutputList() {
        if (outputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(output_);
        } else {
          return outputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public int getOutputCount() {
        if (outputBuilder_ == null) {
          return output_.size();
        } else {
          return outputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);
        } else {
          return outputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.set(index, value);
          onChanged();
        } else {
          outputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(value);
          onChanged();
        } else {
          outputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(index, value);
          onChanged();
        } else {
          outputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> values) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, output_);
          onChanged();
        } else {
          outputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder clearOutput() {
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          outputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder removeOutput(int index) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.remove(index);
          onChanged();
        } else {
          outputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder getOutputBuilder(
          int index) {
        return getOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
          int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);  } else {
          return outputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
           getOutputOrBuilderList() {
        if (outputBuilder_ != null) {
          return outputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(output_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder() {
        return getOutputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder(
          int index) {
        return getOutputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder> 
           getOutputBuilderList() {
        return getOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
          getOutputFieldBuilder() {
        if (outputBuilder_ == null) {
          outputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder>(
                  output_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          output_ = null;
        }
        return outputBuilder_;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> optimizationBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public boolean hasOptimization() {
        return optimizationBuilder_ != null || optimization_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
        if (optimizationBuilder_ == null) {
          return optimization_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        } else {
          return optimizationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          optimization_ = value;
          onChanged();
        } else {
          optimizationBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder builderForValue) {
        if (optimizationBuilder_ == null) {
          optimization_ = builderForValue.build();
          onChanged();
        } else {
          optimizationBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder mergeOptimization(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (optimization_ != null) {
            optimization_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder(optimization_).mergeFrom(value).buildPartial();
          } else {
            optimization_ = value;
          }
          onChanged();
        } else {
          optimizationBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder clearOptimization() {
        if (optimizationBuilder_ == null) {
          optimization_ = null;
          onChanged();
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder getOptimizationBuilder() {
        
        onChanged();
        return getOptimizationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
        if (optimizationBuilder_ != null) {
          return optimizationBuilder_.getMessageOrBuilder();
        } else {
          return optimization_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> 
          getOptimizationFieldBuilder() {
        if (optimizationBuilder_ == null) {
          optimizationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder>(
                  getOptimization(),
                  getParentForChildren(),
                  isClean());
          optimization_ = null;
        }
        return optimizationBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> dynamicBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public boolean hasDynamicBatching() {
        return schedulingChoiceCase_ == 11;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return dynamicBatchingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder builderForValue) {
        if (dynamicBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder mergeDynamicBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 11) {
            dynamicBatchingBuilder_.mergeFrom(value);
          }
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder clearDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          dynamicBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder getDynamicBatchingBuilder() {
        return getDynamicBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 11) && (dynamicBatchingBuilder_ != null)) {
          return dynamicBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> 
          getDynamicBatchingFieldBuilder() {
        if (dynamicBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 11)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
          }
          dynamicBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 11;
        onChanged();;
        return dynamicBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> sequenceBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public boolean hasSequenceBatching() {
        return schedulingChoiceCase_ == 13;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return sequenceBatchingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder builderForValue) {
        if (sequenceBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder mergeSequenceBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 13) {
            sequenceBatchingBuilder_.mergeFrom(value);
          }
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder clearSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          sequenceBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder getSequenceBatchingBuilder() {
        return getSequenceBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 13) && (sequenceBatchingBuilder_ != null)) {
          return sequenceBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> 
          getSequenceBatchingFieldBuilder() {
        if (sequenceBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 13)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
          }
          sequenceBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 13;
        onChanged();;
        return sequenceBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder> ensembleSchedulingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public boolean hasEnsembleScheduling() {
        return schedulingChoiceCase_ == 15;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return ensembleSchedulingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder builderForValue) {
        if (ensembleSchedulingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder mergeEnsembleScheduling(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 15) {
            ensembleSchedulingBuilder_.mergeFrom(value);
          }
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder clearEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          ensembleSchedulingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder getEnsembleSchedulingBuilder() {
        return getEnsembleSchedulingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
        if ((schedulingChoiceCase_ == 15) && (ensembleSchedulingBuilder_ != null)) {
          return ensembleSchedulingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder> 
          getEnsembleSchedulingFieldBuilder() {
        if (ensembleSchedulingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 15)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
          }
          ensembleSchedulingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 15;
        onChanged();;
        return ensembleSchedulingBuilder_;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_ =
        java.util.Collections.emptyList();
      private void ensureInstanceGroupIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          instanceGroup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup>(instanceGroup_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> instanceGroupBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
        if (instanceGroupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        } else {
          return instanceGroupBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public int getInstanceGroupCount() {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.size();
        } else {
          return instanceGroupBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);
        } else {
          return instanceGroupBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addAllInstanceGroup(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> values) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, instanceGroup_);
          onChanged();
        } else {
          instanceGroupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder clearInstanceGroup() {
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          instanceGroupBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder removeInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.remove(index);
          onChanged();
        } else {
          instanceGroupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder getInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
          int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);  } else {
          return instanceGroupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
           getInstanceGroupOrBuilderList() {
        if (instanceGroupBuilder_ != null) {
          return instanceGroupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder() {
        return getInstanceGroupFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder> 
           getInstanceGroupBuilderList() {
        return getInstanceGroupFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
          getInstanceGroupFieldBuilder() {
        if (instanceGroupBuilder_ == null) {
          instanceGroupBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder>(
                  instanceGroup_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          instanceGroup_ = null;
        }
        return instanceGroupBuilder_;
      }

      private java.lang.Object defaultModelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public java.lang.String getDefaultModelFilename() {
        java.lang.Object ref = defaultModelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          defaultModelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public com.google.protobuf.ByteString
          getDefaultModelFilenameBytes() {
        java.lang.Object ref = defaultModelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          defaultModelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder setDefaultModelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder clearDefaultModelFilename() {
        
        defaultModelFilename_ = getDefaultInstance().getDefaultModelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_names`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder setDefaultModelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> ccModelFilenames_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetCcModelFilenames() {
        if (ccModelFilenames_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        return ccModelFilenames_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableCcModelFilenames() {
        onChanged();;
        if (ccModelFilenames_ == null) {
          ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        if (!ccModelFilenames_.isMutable()) {
          ccModelFilenames_ = ccModelFilenames_.copy();
        }
        return ccModelFilenames_;
      }

      public int getCcModelFilenamesCount() {
        return internalGetCcModelFilenames().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public boolean containsCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetCcModelFilenames().getMap().containsKey(key);
      }
      /**
       * Use {@link #getCcModelFilenamesMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
        return getCcModelFilenamesMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
        return internalGetCcModelFilenames().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.lang.String getCcModelFilenamesOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.lang.String getCcModelFilenamesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearCcModelFilenames() {
        internalGetMutableCcModelFilenames().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder removeCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableCcModelFilenames() {
        return internalGetMutableCcModelFilenames().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      public Builder putCcModelFilenames(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder putAllCcModelFilenames(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableCcModelFilenames().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> metricTags_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMetricTags() {
        if (metricTags_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        return metricTags_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableMetricTags() {
        onChanged();;
        if (metricTags_ == null) {
          metricTags_ = com.google.protobuf.MapField.newMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        if (!metricTags_.isMutable()) {
          metricTags_ = metricTags_.copy();
        }
        return metricTags_;
      }

      public int getMetricTagsCount() {
        return internalGetMetricTags().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public boolean containsMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetricTags().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricTagsMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
        return getMetricTagsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
        return internalGetMetricTags().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.lang.String getMetricTagsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.lang.String getMetricTagsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetricTags() {
        internalGetMutableMetricTags().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder removeMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableMetricTags() {
        return internalGetMutableMetricTags().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      public Builder putMetricTags(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder putAllMetricTags(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableMetricTags().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> parameters_;
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      internalGetParameters() {
        if (parameters_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        return parameters_;
      }
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      internalGetMutableParameters() {
        onChanged();;
        if (parameters_ == null) {
          parameters_ = com.google.protobuf.MapField.newMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        if (!parameters_.isMutable()) {
          parameters_ = parameters_.copy();
        }
        return parameters_;
      }

      public int getParametersCount() {
        return internalGetParameters().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public boolean containsParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetParameters().getMap().containsKey(key);
      }
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParameters() {
        return getParametersMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParametersMap() {
        return internalGetParameters().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearParameters() {
        internalGetMutableParameters().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder removeParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      getMutableParameters() {
        return internalGetMutableParameters().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */
      public Builder putParameters(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder putAllParameters(
          java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> values) {
        internalGetMutableParameters().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelConfig)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelConfig>
        PARSER = new com.google.protobuf.AbstractParser<ModelConfig>() {
      @java.lang.Override
      public ModelConfig parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelConfig(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelConfig> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelConfig> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\022model_config.proto\022\026nvidia.inferencese" +
      "rver\"\301\001\n\022ModelInstanceGroup\022\014\n\004name\030\001 \001(" +
      "\t\022=\n\004kind\030\004 \001(\0162/.nvidia.inferenceserver" +
      ".ModelInstanceGroup.Kind\022\r\n\005count\030\002 \001(\005\022" +
      "\014\n\004gpus\030\003 \003(\005\"A\n\004Kind\022\r\n\tKIND_AUTO\020\000\022\014\n\010" +
      "KIND_GPU\020\001\022\014\n\010KIND_CPU\020\002\022\016\n\nKIND_MODEL\020\003" +
      "\"#\n\022ModelTensorReshape\022\r\n\005shape\030\001 \003(\003\"\222\002" +
      "\n\nModelInput\022\014\n\004name\030\001 \001(\t\0223\n\tdata_type\030" +
      "\002 \001(\0162 .nvidia.inferenceserver.DataType\022" +
      "9\n\006format\030\003 \001(\0162).nvidia.inferenceserver" +
      ".ModelInput.Format\022\014\n\004dims\030\004 \003(\003\022;\n\007resh" +
      "ape\030\005 \001(\0132*.nvidia.inferenceserver.Model" +
      "TensorReshape\";\n\006Format\022\017\n\013FORMAT_NONE\020\000" +
      "\022\017\n\013FORMAT_NHWC\020\001\022\017\n\013FORMAT_NCHW\020\002\"\263\001\n\013M" +
      "odelOutput\022\014\n\004name\030\001 \001(\t\0223\n\tdata_type\030\002 " +
      "\001(\0162 .nvidia.inferenceserver.DataType\022\014\n" +
      "\004dims\030\003 \003(\003\022;\n\007reshape\030\005 \001(\0132*.nvidia.in" +
      "ferenceserver.ModelTensorReshape\022\026\n\016labe" +
      "l_filename\030\004 \001(\t\"\267\002\n\022ModelVersionPolicy\022" +
      "C\n\006latest\030\001 \001(\01321.nvidia.inferenceserver" +
      ".ModelVersionPolicy.LatestH\000\022=\n\003all\030\002 \001(" +
      "\0132..nvidia.inferenceserver.ModelVersionP" +
      "olicy.AllH\000\022G\n\010specific\030\003 \001(\01323.nvidia.i" +
      "nferenceserver.ModelVersionPolicy.Specif" +
      "icH\000\032\036\n\006Latest\022\024\n\014num_versions\030\001 \001(\r\032\005\n\003" +
      "All\032\034\n\010Specific\022\020\n\010versions\030\001 \003(\003B\017\n\rpol" +
      "icy_choice\"\357\002\n\027ModelOptimizationPolicy\022D" +
      "\n\005graph\030\001 \001(\01325.nvidia.inferenceserver.M" +
      "odelOptimizationPolicy.Graph\022O\n\010priority" +
      "\030\002 \001(\0162=.nvidia.inferenceserver.ModelOpt" +
      "imizationPolicy.ModelPriority\022B\n\004cuda\030\003 " +
      "\001(\01324.nvidia.inferenceserver.ModelOptimi" +
      "zationPolicy.Cuda\032\026\n\005Graph\022\r\n\005level\030\001 \001(" +
      "\005\032\026\n\004Cuda\022\016\n\006graphs\030\001 \001(\010\"I\n\rModelPriori" +
      "ty\022\024\n\020PRIORITY_DEFAULT\020\000\022\020\n\014PRIORITY_MAX" +
      "\020\001\022\020\n\014PRIORITY_MIN\020\002\"Z\n\024ModelDynamicBatc" +
      "hing\022\034\n\024preferred_batch_size\030\001 \003(\005\022$\n\034ma" +
      "x_queue_delay_microseconds\030\002 \001(\004\"\301\003\n\025Mod" +
      "elSequenceBatching\022&\n\036max_sequence_idle_" +
      "microseconds\030\001 \001(\004\022Q\n\rcontrol_input\030\002 \003(" +
      "\0132:.nvidia.inferenceserver.ModelSequence" +
      "Batching.ControlInput\032\306\001\n\007Control\022H\n\004kin" +
      "d\030\001 \001(\0162:.nvidia.inferenceserver.ModelSe" +
      "quenceBatching.Control.Kind\022\030\n\020int32_fal" +
      "se_true\030\002 \003(\005\022\027\n\017fp32_false_true\030\003 \003(\002\">" +
      "\n\004Kind\022\032\n\026CONTROL_SEQUENCE_START\020\000\022\032\n\026CO" +
      "NTROL_SEQUENCE_READY\020\001\032d\n\014ControlInput\022\014" +
      "\n\004name\030\001 \001(\t\022F\n\007control\030\002 \003(\01325.nvidia.i" +
      "nferenceserver.ModelSequenceBatching.Con" +
      "trol\"\204\003\n\017ModelEnsembling\022:\n\004step\030\001 \003(\0132," +
      ".nvidia.inferenceserver.ModelEnsembling." +
      "Step\032\264\002\n\004Step\022\022\n\nmodel_name\030\001 \001(\t\022\025\n\rmod" +
      "el_version\030\002 \001(\003\022M\n\tinput_map\030\003 \003(\0132:.nv" +
      "idia.inferenceserver.ModelEnsembling.Ste" +
      "p.InputMapEntry\022O\n\noutput_map\030\004 \003(\0132;.nv" +
      "idia.inferenceserver.ModelEnsembling.Ste" +
      "p.OutputMapEntry\032/\n\rInputMapEntry\022\013\n\003key" +
      "\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0320\n\016OutputMapEn" +
      "try\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\"&\n\016M" +
      "odelParameter\022\024\n\014string_value\030\001 \001(\t\"\300\010\n\013" +
      "ModelConfig\022\014\n\004name\030\001 \001(\t\022\020\n\010platform\030\002 " +
      "\001(\t\022B\n\016version_policy\030\003 \001(\0132*.nvidia.inf" +
      "erenceserver.ModelVersionPolicy\022\026\n\016max_b" +
      "atch_size\030\004 \001(\005\0221\n\005input\030\005 \003(\0132\".nvidia." +
      "inferenceserver.ModelInput\0223\n\006output\030\006 \003" +
      "(\0132#.nvidia.inferenceserver.ModelOutput\022" +
      "E\n\014optimization\030\014 \001(\0132/.nvidia.inference" +
      "server.ModelOptimizationPolicy\022H\n\020dynami" +
      "c_batching\030\013 \001(\0132,.nvidia.inferenceserve" +
      "r.ModelDynamicBatchingH\000\022J\n\021sequence_bat" +
      "ching\030\r \001(\0132-.nvidia.inferenceserver.Mod" +
      "elSequenceBatchingH\000\022F\n\023ensemble_schedul" +
      "ing\030\017 \001(\0132\'.nvidia.inferenceserver.Model" +
      "EnsemblingH\000\022B\n\016instance_group\030\007 \003(\0132*.n" +
      "vidia.inferenceserver.ModelInstanceGroup" +
      "\022\036\n\026default_model_filename\030\010 \001(\t\022U\n\022cc_m" +
      "odel_filenames\030\t \003(\01329.nvidia.inferences" +
      "erver.ModelConfig.CcModelFilenamesEntry\022" +
      "H\n\013metric_tags\030\n \003(\01323.nvidia.inferences" +
      "erver.ModelConfig.MetricTagsEntry\022G\n\npar" +
      "ameters\030\016 \003(\01323.nvidia.inferenceserver.M" +
      "odelConfig.ParametersEntry\0327\n\025CcModelFil" +
      "enamesEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:" +
      "\0028\001\0321\n\017MetricTagsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005v" +
      "alue\030\002 \001(\t:\0028\001\032Y\n\017ParametersEntry\022\013\n\003key" +
      "\030\001 \001(\t\0225\n\005value\030\002 \001(\0132&.nvidia.inference" +
      "server.ModelParameter:\0028\001B\023\n\021scheduling_" +
      "choice*\353\001\n\010DataType\022\020\n\014TYPE_INVALID\020\000\022\r\n" +
      "\tTYPE_BOOL\020\001\022\016\n\nTYPE_UINT8\020\002\022\017\n\013TYPE_UIN" +
      "T16\020\003\022\017\n\013TYPE_UINT32\020\004\022\017\n\013TYPE_UINT64\020\005\022" +
      "\r\n\tTYPE_INT8\020\006\022\016\n\nTYPE_INT16\020\007\022\016\n\nTYPE_I" +
      "NT32\020\010\022\016\n\nTYPE_INT64\020\t\022\r\n\tTYPE_FP16\020\n\022\r\n" +
      "\tTYPE_FP32\020\013\022\r\n\tTYPE_FP64\020\014\022\017\n\013TYPE_STRI" +
      "NG\020\rb\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor,
        new java.lang.String[] { "Name", "Kind", "Count", "Gpus", });
    internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor,
        new java.lang.String[] { "Shape", });
    internal_static_nvidia_inferenceserver_ModelInput_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelInput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Format", "Dims", "Reshape", });
    internal_static_nvidia_inferenceserver_ModelOutput_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOutput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Dims", "Reshape", "LabelFilename", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor,
        new java.lang.String[] { "Latest", "All", "Specific", "PolicyChoice", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor,
        new java.lang.String[] { "NumVersions", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor,
        new java.lang.String[] { });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor,
        new java.lang.String[] { "Versions", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor,
        new java.lang.String[] { "Graph", "Priority", "Cuda", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor,
        new java.lang.String[] { "Level", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor,
        new java.lang.String[] { "Graphs", });
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor,
        new java.lang.String[] { "PreferredBatchSize", "MaxQueueDelayMicroseconds", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor,
        new java.lang.String[] { "MaxSequenceIdleMicroseconds", "ControlInput", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor,
        new java.lang.String[] { "Kind", "Int32FalseTrue", "Fp32FalseTrue", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor,
        new java.lang.String[] { "Name", "Control", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor,
        new java.lang.String[] { "Step", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor,
        new java.lang.String[] { "ModelName", "ModelVersion", "InputMap", "OutputMap", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelParameter_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelParameter_descriptor,
        new java.lang.String[] { "StringValue", });
    internal_static_nvidia_inferenceserver_ModelConfig_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_descriptor,
        new java.lang.String[] { "Name", "Platform", "VersionPolicy", "MaxBatchSize", "Input", "Output", "Optimization", "DynamicBatching", "SequenceBatching", "EnsembleScheduling", "InstanceGroup", "DefaultModelFilename", "CcModelFilenames", "MetricTags", "Parameters", "SchedulingChoice", });
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
